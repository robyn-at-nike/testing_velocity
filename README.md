# testing_velocity
CanM AB Measurement


## Overview 

Comms in an integral part of Nike’s continued engagement with our consumers. From a business perspective we utilize email and push notifications on app/web as tactics for consumers to continue engaging with the brand, continue their shopping journey and pushCa visitors through the purchasing funnel. Today, journey communications are orchestrated via Adobe’s Journey Orchestration tool. 

The Marketing Science (MarSci) teams have created opportunities to experiment with journeys, and comms content, so that we are better at understanding and encouraging consumer experiences with Nike. Today, reporting on comms A/B testing conducted by MarSci team is a relatively manual process that takes ~15 hours per test to measure and analyze. Over the past year over 150 comms tests were conducted which leads to an estimated 2250 hours (56.25 weeks) of ‘human in the middle’ effort to measure tests. In addition to the manual creation of test measurement reporting, there are 14 different templates used for reporting that leads to ad hoc, manual and inconsistencies based on who is running the analysis – i.e. analysts will change logic based on comfort and data accessibility.  

Secondly, it is not possible to see how many members are assigned to Test and Control groups in an experiment directly within AJO, where tests are orchestrated. Audience sizing requires teams to manually query within an Adobe UI and extract data into a spreadsheet to apply sampling population logic to determine audience populations. This is a critical step to measure testing efficacy. 

Furthermore, there are inconsistencies in the statistical inferencing than what/how we measure across the broader experimentation space at Nike. Although not a hinderance to measuring comms experiments is opportunity to enhance and unify measurement capabilities across experimentation types.  

####  $7.5M costs savings. 
The Next% is dependent upon our ability to market and build relationships effectively with specific audiences. This requires the ability to test, analyze and optimize the experiences and communications our consumers have with Nike. The proposal to modernize these data for email and push will set a foundation and patterns in place to integrate and scale even more marketing touchpoints. Investing in an agile data foundation now, as opposed to being locked in with a vendor (Adobe) will allow us to be more nimble in the future, for a fraction of the cost. 

Adobe contract renewals are imminent, and we don’t want to get locked in for another 3 years. Additionally, we can improve the speed of testing by 87%1; a significant cost savings for Nike and opportunity to increase testing prowess. 

For Nike to increase the scale of testing in the comms space we need to support comms experimentation measurement through automation and relieve the manual dependencies on MarSci. Additionally, we need to bring best practices from our broader experimentation strategy for statistical inferencing in testing through alignment/standardization across the experimentation center of excellence. The Experimentation platform team will support Comms testing by enabling more automation, enhance/improve rigor in statistical testing, and align with the broader experimentation platform strategy.  

### There are 2 bodies of work against this initiative:  

* **Modernizing Journey Audience Sizing capabilities** through data migration from CJA to NGAP/SOLE (SOLE being the ideal state) and automating dashboarding so that marketers can effectively identify and understand audience sizing and segmentation requirements for effective testing/experimentation purposes. This informs users on what population criteria and testing timelines would look like based on the audience criteria selected for testing. For example, if they want to run a test on people who have favorited women’s leggings in the past 3 months, the sizing tool can help segment the specific audience they’re looking for to test and determine if the sizing is feasible. **This work has been completed**.

* **AB Measurement improvements through automation and data migration** to DB Sole significantly reduce the time it takes MarSci to run and analyze results of AB tests – a reduction from ~15 hours per test to ~2hours. Automation of measurement reporting will allow our teams to focus on insights and determining scale of winning tests vs. executing what is today a very manual dependent process. 
