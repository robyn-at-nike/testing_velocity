{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source\n",
    "\n",
    "https://nike.ent.box.com/file/1355345336690?s=ppbfr7j7rgcftqpjjcbmav9o09opj38e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %run /Users/Shrey.Bavisi@nike.com/DMA_SharedRepository/AudienceSizing_EO/#eo_functions\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC Target Audience: Email Contactable\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "aud_df = spark.sql('''\n",
    "\n",
    "SELECT DISTINCT mh.source_id AS upm_id\n",
    "FROM\n",
    "  (SELECT source_id, member_id\n",
    "   FROM member.member_hub\n",
    "   WHERE 1=1\n",
    "     AND lower(dpa_status) IN ('activate','reactivate')\n",
    "     AND ctry_2_cd IN ('US')\n",
    "     AND lower(valid_email_format_flag) = 'true') mh\n",
    "\n",
    "INNER JOIN comms_atc.combined_starting_universe su ON su.upm_id = mh.source_id\n",
    "\n",
    "WHERE 1=1\n",
    "  AND su.email_contactable = 1\n",
    "\n",
    "''')\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# estimated count for the audience size\n",
    "aud_df.count()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "aud_eo_df = spark.sql('''\n",
    "\n",
    "SELECT DISTINCT mh.source_id AS upm_id\n",
    "FROM\n",
    "  (SELECT source_id, member_id\n",
    "   FROM member.member_hub\n",
    "   WHERE 1=1\n",
    "     AND lower(dpa_status) IN ('activate','reactivate')\n",
    "     AND ctry_2_cd IN ('US')\n",
    "     AND lower(valid_email_format_flag) = 'true') mh\n",
    "\n",
    "INNER JOIN comms_atc.combined_starting_universe su ON su.upm_id = mh.source_id\n",
    "\n",
    "WHERE 1=1\n",
    "  AND su.email_contactable = 1\n",
    "''')\n",
    "\n",
    "eo_metric_df = spark.sql('''\n",
    "  SELECT DISTINCT gck.upm_id\n",
    "  , CASE WHEN dem.7_day_demand > 0 THEN 1 else 0 END as 7_day_buyer_flag\n",
    "  , dem.7_day_demand\n",
    "  , dem.7_day_units\n",
    "  , dem.7_day_orders\n",
    "  FROM gck_common.agg_user_first_and_last_touchpoint gck\n",
    "\n",
    "  INNER JOIN member.member_hub mh\n",
    "  ON gck.upm_id = mh.source_id\n",
    "\n",
    "  LEFT JOIN\n",
    "    (SELECT DISTINCT upm_id\n",
    "    , SUM(CASE WHEN order_dt >= date_sub(to_date(current_date()), 7) THEN grd_amt_excl_tax_usd END) as 7_day_demand\n",
    "    , SUM(CASE WHEN order_dt >= date_sub(to_date(current_date()), 7) THEN origl_ordered_qty END) as 7_day_units\n",
    "    , COUNT(DISTINCT CASE WHEN order_dt >= date_sub(to_date(current_date()), 7) THEN order_hdr_key END) as 7_day_orders\n",
    "   FROM aud_select_workspace.digital_order_line_snapshot\n",
    "   WHERE order_dt >= date_sub(to_date(current_date()), 7)\n",
    "    AND rec_excl_ind = 0\n",
    "    AND upm_id IS NOT NULL \n",
    "   GROUP BY upm_id\n",
    "     ) dem \n",
    "  ON gck.upm_id = dem.upm_id\n",
    "  ''')\n",
    "\n",
    "aud_eo_metric_df = aud_eo_df.join(eo_metric_df, on = 'upm_id', how = 'left')\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "audience_size = aud_eo_metric_df.cache().count()\n",
    "\n",
    "nbr_buyers = aud_eo_metric_df.filter(f.col(\"7_day_buyer_flag\") == 1).count()\n",
    "\n",
    "conversion_rate = nbr_buyers / audience_size\n",
    "\n",
    "total_demand = aud_eo_metric_df.agg(f.sum(\"7_day_demand\")).collect()[0][0]\n",
    "\n",
    "nbr_orders = aud_eo_metric_df.agg(f.sum(\"7_day_orders\")).collect()[0][0]\n",
    "\n",
    "aov = total_demand / nbr_orders\n",
    "\n",
    "avg_demand = aud_eo_metric_df.agg(f.mean(\"7_day_demand\")).collect()[0][0]\n",
    "std_demand = aud_eo_metric_df.agg(f.stddev(\"7_day_demand\")).collect()[0][0]\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "# store the notebook name to include later on in the graph\n",
    "notebook_name = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get().split('/')[-1]\n",
    "\n",
    "# assumed scaling frequency. This will be used to compute the graph. \n",
    "assumed_scaling_freq = 4\n",
    "\n",
    "# presumed open rate of email notification - current baseline is 30% open rate\n",
    "open_rate = 0.30\n",
    "\n",
    "# list of values representing possible lift percentages. These are just common values we have seen. Add more/less depending on the use case.\n",
    "lift_percentage = [0.0, 0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04, 0.045, 0.05] \n",
    "\n",
    "# list of scaling frequencies. These are common scaling multipliers, yearly, twice yearly, quarterly, ever other month, etc. \n",
    "scaling_freq = [1, 2, 4, 6, 8, 10, 12, 17, 26, 52]\n",
    "\n",
    "# calculation to compute the EO values. Computes for each combination of lift percentage and scaling frequency. \n",
    "eo_vals = [round(audience_size * open_rate * conversion_rate * aov * perc * freq,0) for perc, freq in product(lift_percentage,scaling_freq)]\n",
    "\n",
    "#\n",
    "combi_vals = list(product(lift_percentage,scaling_freq))\n",
    "# a pandas dataframe for all the combinations of lift percentage and scaling frequency\n",
    "eo_pd_df = pd.DataFrame(combi_vals, columns =['lift_perc', 'scaling_freq'])\n",
    "# add a column for the EO values\n",
    "eo_pd_df['EO'] = eo_vals\n",
    "\n",
    "print(f'''\n",
    "Test Name: {notebook_name}\n",
    "Audience Size: {audience_size}\n",
    "Number of Buyers: {nbr_buyers}\n",
    "Conversion Rate: {conversion_rate:.1%}\n",
    "Total Demand: $ {total_demand:.0f}\n",
    "Number of Orders: {nbr_orders}\n",
    "AOV: $ {aov:.0f}\n",
    "Scaling Frequency: {assumed_scaling_freq}\n",
    "avg_demand: {avg_demand:.2f}\n",
    "std_demand: {std_demand:.2f}\n",
    "''')\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "plt.style.use('fivethirtyeight')   \n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "palette = sns.color_palette(\"bright\", 4)\n",
    "g = sns.lineplot(data=eo_pd_df[eo_pd_df['scaling_freq'] == assumed_scaling_freq], x=\"lift_perc\", y=\"EO\")\n",
    "\n",
    "font_color = '#525252'\n",
    "csfont = {'fontname':'Georgia'}\n",
    "hfont = {'fontname':'Georgia'}\n",
    "\n",
    "# title of the notebook that the numbers were calculated in\n",
    "title = f'{notebook_name}'\n",
    "# add the title to the plot\n",
    "fig.suptitle(title, y=.97, fontsize=22, color=font_color, **csfont)\n",
    "# subtitle with some notes about when the numbers were calculated \n",
    "subtitle = f'Note: Audience criteria as of {today} using a 7 day measurement window'\n",
    "# add the subtitle to the plot\n",
    "plt.title(subtitle, fontsize=10, pad=10, color=font_color, **hfont)\n",
    "plt.subplots_adjust(top=0.85)\n",
    "\n",
    "# legend text that includes the values in our calculations\n",
    "legend_txt = f'''\n",
    "Audience Size: {human_format(audience_size)}\n",
    "Baseline Conversion: {conversion_rate:.1%}\n",
    "AOV: ${human_format(aov)}\n",
    "Scaling Frequency: {assumed_scaling_freq}\n",
    "'''\n",
    "# formats the legend box\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.4)\n",
    "# creates the legend box\n",
    "ax.text(0.05, 0.98, legend_txt, transform=ax.transAxes, fontsize=10,\n",
    "        verticalalignment='top', bbox=props)\n",
    "# rename the x and y axises \n",
    "ax.set(xlabel='Lift Assumption', ylabel='Expected Outcome')\n",
    "# reformat the y axis to be displayed in dollars\n",
    "ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos:  \"$\" + human_format(x)))\n",
    "# reformat the x axis to be displayed as a percentage \n",
    "ax.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.1%}'.format(x)))\n",
    "display(g)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Pivoted dataframe displaying lift percentages and scaling frequencies. Tabular view. \n",
    "eo_pd_df.pivot(index='lift_perc', columns='scaling_freq', values='EO')\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from statsmodels.stats.power import tt_ind_solve_power\n",
    "# ratio between test and control groups\n",
    "ratio = 1\n",
    "# probability that the test correctly rejects the Null Hypothesis if the Alternative Hypothesis is true\n",
    "power = 0.8\n",
    "# significance level\n",
    "alpha = 0.1\n",
    "#two-sided or one-sided test\n",
    "alt = 'two-sided'\n",
    "# iterate through various lift percentages to calculate how much total sample is required for each lift percentage\n",
    "eo_sample_size_list = [sample_solver(avg_demand, std_demand, lift, alpha, power, ratio, alt) for lift in lift_percentage[1:]]\n",
    "# create a pandas dataframe to store the values\n",
    "eo_sample_size_pd = pd.DataFrame(lift_percentage[1:], columns = ['lift_perc'])\n",
    "# create a new column with our sample sizes\n",
    "eo_sample_size_pd['sample_size'] = eo_sample_size_list\n",
    "\n",
    "eo_sample_size_pd\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC Pod 1 (Demand) Example comment\n",
    "# MAGIC\n",
    "# MAGIC - Estimated audience size is 6.6M consumers\n",
    "# MAGIC - Baselines conversion: 1.7%\n",
    "# MAGIC - Expected Lift: 0.5%\n",
    "# MAGIC - Expected Treatment Viewing Rate: 5%\n",
    "# MAGIC - Scaled: Monthly (12 sends)\n",
    "# MAGIC - EO: $995K\n",
    "\n",
    "# COMMAND ----------\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
