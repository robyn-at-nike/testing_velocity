{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VUQAHxgWF3H-"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*****************************************\n",
        "# Project Plan Draft V1\n",
        "\n",
        "Goal:\n",
        "\n",
        "Save time by automating more of the current test measurement process.\n",
        "\n",
        "Means:\n",
        "\n",
        "Remove need for Excel. By automating statistical testing and rebalancing steps.\n",
        "\n",
        "The new templates primarily accomplish two things:\n",
        "\n",
        "In Notebook Rebalancing\n",
        "* Former: A 3 step process utilizing Databricks and Excel.\n",
        "* New: Entire process performed completely in Databricks.\n",
        "* Time saved - 30 mins to 1 hour.\n",
        "\n",
        "In Notebook Statistical Testing (scorecard creation)\n",
        "* Former: A 2-step process, utilizing Databricks and Excel\n",
        "* New:  Entire process performed completely in Databricks.\n",
        "* Time saved - 30 mins\n",
        "\n",
        "Bonus:\n",
        "* Ad-hoc analysis is many times faster as it no longer requires waiting for very long SQL code blocks with several cte’s to run.\n",
        "* Once SQL data at `upm_id` level is loaded into a Python (pandas) data frame, ad-hoc calculations take 30 seconds or fewer.\n",
        "\n",
        "Outline of the process:\n",
        "\n",
        "COE group to determine best statistical tests for the Demand and Conversion data.\n",
        "\n",
        "Currently, the templates apply:\n",
        "* Mann Whitney U test for Demand (based on my analysis showing Demand data does not follow a normal distribution)\n",
        "* Chi Square Test for Conversion. (Based on the data point for conversion  - ‘buyer’ - being a binary value)\n",
        "\n",
        "Once there is alignment from COE on the correct test, I can then create iterations of the new template for 4 types of tests (AJO ANBD, NCP Incremental Demand, etc).\n",
        "\n",
        "Currently there are 14 different templates in use.\n",
        "\n",
        "Then, the code for these notebooks must be reviewed by Nike Data Science and Analytics.\n",
        "\n",
        "The next step is training for the team on Python and how to use the templates:\n",
        "\n",
        "Python Crash Course\n",
        "\n",
        "* [Python for Everybody](https://www.py4e.com/lessons) (special attention to: Ch. 5 - Functions, Ch. 6 - Loops & Iterations, Ch. 9 - 11 Lists, Dictionaries and Tuples).\n",
        "* [Google’s Python Crash Course](https://developers.google.com/edu/python)\n",
        "OR [also available on Coursera](https://www.coursera.org/learn/python-crash-course?) - plenty of example problems here\n",
        "\n",
        "Introduction to Pandas (the Python Library for Data Analysis)\n",
        "\n",
        "  * [Kaggle Learn Pandas](https://www.kaggle.com/learn/pandas)\n",
        "  * [A Gentle Introduction to Pandas Data Analysis](https://youtu.be/_Eb0utIRdkw?) - Kaggle on Youtube\n",
        "  * [Google's Pandas Dataframe Ultraquick Tutorial](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)\n",
        "  * [Python Pandas for your Grandpa.](https://youtube.com/playlist?list=PL9oKUrtC4VP7ry0um1QOUUfJBXKnkf-dA&feature=shared) “So easy your grandpa could learn it.”\n",
        "\n",
        "Optional: Introduction to Numpy (the Python library for doing advanced math on large datasets)\n",
        "\n",
        "  * [Python Numpy for your Grandma.](https://www.youtube.com/playlist?list=PL9oKUrtC4VP6gDp1Vq3BzfViO0TWgR0vR) “So easy, your grandma could learn it!\"\n",
        "  * [Google's Ultra Quick Intro to Numpy](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/numpy_ultraquick_tutorial.ipynb?)\n",
        "\n",
        "\n",
        "We should allow a 6-week period where both the new and old templates are run for each test.\n",
        "\n",
        "Things we should compare:\n",
        "\n",
        "Outcomes.\n",
        "* Is there a difference between which tests reach stat sig.\n",
        "Ease of use.\n",
        "*  Is this adding complexity from the perspective of the team.\n",
        "Velocity.\n",
        "* Does this shrink the overall timing duration of test measurement from start to finish.\n",
        "\n",
        "Kickoff week 1 with a walkthrough of a new test, on the new template.\n",
        "\n",
        "Then, throughout this 6 week period, I will make myself available during the Peer Review standup for troubleshooting.\n",
        "\n",
        "At the conclusion of the 6-week period, we should asses if these new templates are adding value, saving time, etc\n",
        "\n",
        "It is possible in the end, we use some portion of the template, and not others.\n",
        "\n"
      ],
      "metadata": {
        "id": "4CcDpNbs2Jg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANBD Error code\n",
        "\n",
        "AnalysisException: Reference 'aud.member_id' is ambiguous, could be: aud.member_id, aud.member_id.; line 27 pos 59"
      ],
      "metadata": {
        "id": "cfRRTRZyfQ2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "\n",
        "------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "-- DEFINE CTEs\n",
        "------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "WITH\n",
        "--- ACTIVE NON BUYING FREQUENCY *EXCLUDING* THE DAYS OF SEND\n",
        "active_non_buying_excl_send_day AS (\n",
        "    select anbd.member_id\n",
        "    , aud.test_control\n",
        "        ,count(distinct activity_dt) as active_nonbuying\n",
        "    from(\n",
        "          select member_agd.member_id,\n",
        "                  activity_dt,\n",
        "                  sum(logged_in_purchase_order_count) as purch\n",
        "          -- from member_agd\n",
        "          from aud_select_workspace.member_agg_member_growth_daily AS member_agd\n",
        "          WHERE activity_dt BETWEEN ${hivevar:msmt_start_dt} AND DATE_ADD(${hivevar:msmt_end_dt},14) -- ANBD KPIs for Comms Pod are calculated over a 14 day period\n",
        "            AND activity_dt NOT IN (${hivevar:msmt_start_dt}\n",
        "--                                   , ${hivevar:msmt_second_send_dt}\n",
        "--                                   , ${hivevar:msmt_third_send_dt}\n",
        "--                                   , ${hivevar:msmt_end_dt}\n",
        "                                   )\n",
        "            AND preferred_retail_geo='NA'\n",
        "            AND experience_name = 'nike_app'\n",
        "            and member_agd.member_id is not null\n",
        "          group by 1,2\n",
        "    ) anbd\n",
        "    INNER JOIN base_audience_table aud ON anbd.member_id = aud.member_id\n",
        "    WHERE purch = 0\n",
        "    GROUP BY 1,2\n",
        "),\n",
        "\n",
        "--Additional ANBD CTE to address the inherent bias in activity caused by a Test vs. Holdout setup, where the Holdout group receives no communication.\n",
        "member_activity_with_dayOfSend as (\n",
        "  SELECT\n",
        "    anbde.member_id\n",
        "    , anbde.test_control\n",
        "    , AVG(PV_CNT) as num_views\n",
        "    , CASE WHEN AVG(PV_CNT) > 9 THEN (max(active_nonbuying) + 1) ELSE max(active_nonbuying)\n",
        "      END AS active_nonbuying\n",
        "FROM active_non_buying_excl_send_day anbde\n",
        "INNER JOIN digital_exp.FACT_CLICKSTREAM_SESSION_NIKEAPP cs\n",
        "  on anbde.member_id = cs.member_id\n",
        "WHERE SESSION_START_DT IN (${hivevar:msmt_start_dt}\n",
        "--                           , date_add(${hivevar:msmt_start_dt}, 5) -- Example: second send was 5 days after the first send\n",
        "                         )\n",
        "and lower(COUNTRY_SKEY) = 'usa'\n",
        "and SESSION_SKEY IS NOT NULL\n",
        "and SESSION_END_GMT_EPOCH-SESSION_START_GMT_EPOCH > 0\n",
        "and SESSION_END_GMT_EPOCH-SESSION_START_GMT_EPOCH <= 3600\n",
        "and PURCHASE_CNT =0\n",
        "GROUP BY 1,2\n",
        "),\n",
        "\n",
        "active_non_buying as (\n",
        "SELECT\n",
        "  member_id\n",
        "  , test_control\n",
        "  , active_nonbuying\n",
        "FROM member_activity_with_dayOfSend\n",
        "),\n",
        "\n",
        "\n",
        "test_control_table AS (\n",
        "\tSELECT\n",
        "\t\t aud.upm_id as upm_id\n",
        "        ,anbd.member_id as member_id\n",
        "        ,nuid\n",
        "        ,aud.test_control\n",
        "\t\t,MAX(CASE WHEN aud.test_control = 'test' THEN 1 ELSE 0 END) AS test_ind\n",
        "\t\t,MAX(CASE WHEN aud.test_control = 'control'  THEN 1 ELSE 0 END) AS control_ind\n",
        "    ,COALESCE(SUM(active_nonbuying),0) as active_non_buying\n",
        "\tFROM base_audience_table aud\n",
        "    LEFT JOIN active_non_buying anbd ON aud.member_id = anbd.member_id\n",
        "    ANTI JOIN gwan13.bot_master_nike_com bot ON aud.upm_id = bot.upm_id\n",
        "    ANTI JOIN aud_select_workspace.na_resellers slr ON aud.upm_id = slr.upm_id\n",
        "\tGROUP BY 1,2,3,4\n",
        "),\n",
        "\n",
        " view_dol AS (\n",
        "  SELECT dol.upm_id\n",
        "  , order_nbr\n",
        "  , order_dt\n",
        "  , origl_ordered_qty\n",
        "  , grd_amt_excl_tax_usd\n",
        "  , line_of_business_desc\n",
        "  , univ_div_desc\n",
        "  FROM awight.na_digital_order_line_snapshot dol\n",
        "  INNER JOIN test_control_table tct ON dol.upm_id = tct.upm_id\n",
        "  WHERE rec_excl_ind = 0\n",
        "    AND ttl_demand_ind = 1\n",
        "    AND univ_cat_desc <> 'Converse'\n",
        "    AND dol.upm_id IS NOT NULL\n",
        "    AND CAST(order_dt AS DATE) BETWEEN DATE_SUB(${hivevar:msmt_start_dt},730) AND DATE_ADD(${hivevar:msmt_end_dt},7)\n",
        "    AND (rtn_qty = 0 or rtn_qty IS NULL)\n",
        "),\n",
        "\n",
        "member_activities as (\n",
        "  SELECT\n",
        "    member_id\n",
        "    ,SUM(logged_in_visits_count) AS visits\n",
        "    ,SUM(PDP_FAVORITE_COUNT) AS PDP_FAVORITE_COUNT\n",
        "    ,SUM(ADD_TO_CART_COUNT) AS ADD_TO_CART_COUNT\n",
        "    ,SUM(physical_activity_count) AS physical_activity\n",
        "    ,MAX(CASE WHEN new_user_experience_flag = 'Y' THEN 1 ELSE 0 END) AS new_user_flag\n",
        "    ,SUM(eod_chat_count) AS NEOD_Session\n",
        "  FROM member_agd\n",
        "  WHERE activity_dt BETWEEN ${hivevar:msmt_start_dt} AND DATE_ADD(${hivevar:msmt_end_dt},7)\n",
        "    AND UPPER(preferred_retail_geo) = 'NA'\n",
        "    AND UPPER(experience_name) <> 'AGNOSTIC'\n",
        "  GROUP BY 1\n",
        "),\n",
        "\n",
        "\n",
        "-- ACTIVE NON BUYING BUCKET FOR REBALANCING ON ACTIVE NON BUYING DAYS\n",
        "active_non_buying_bucket as (\n",
        " SELECT\n",
        "    upm_id\n",
        "   , z.member_id\n",
        "    , z.test_control\n",
        "    , active_nonbuying\n",
        "    , CASE WHEN active_nonbuying = 0 OR active_nonbuying is NULL THEN 'non-active'\n",
        "         WHEN active_nonbuying < 6 THEN '000-005'\n",
        "         WHEN active_nonbuying < 11 THEN '006-010'\n",
        "         WHEN active_nonbuying < 16 THEN '011-015'\n",
        "         WHEN active_nonbuying < 21 THEN '016-020'\n",
        "         WHEN active_nonbuying < 26 THEN '021-025'\n",
        "         WHEN active_nonbuying <=31 THEN '026-031'\n",
        "    END AS anb_bucket\n",
        "    , percent_rank() over (\n",
        "    partition by\n",
        "    z.test_control\n",
        "    ,CASE WHEN active_nonbuying = 0 OR active_nonbuying is NULL THEN 'non-active'\n",
        "         WHEN active_nonbuying < 6 THEN '000-005'\n",
        "         WHEN active_nonbuying < 11 THEN '006-010'\n",
        "         WHEN active_nonbuying < 16 THEN '011-015'\n",
        "         WHEN active_nonbuying < 21 THEN '016-020'\n",
        "         WHEN active_nonbuying < 26 THEN '021-025'\n",
        "         WHEN active_nonbuying <=31 THEN '026-031'\n",
        "         END order by RAND(0)) as anb_percent_rank\n",
        "  from test_control_table z\n",
        "  left join active_non_buying anb\n",
        "    on z.member_id = anb.member_id\n",
        "),\n",
        "\n",
        "balanced_aud as (\n",
        "  SELECT DISTINCT a.upm_id\n",
        "  FROM active_non_buying_bucket a\n",
        "  -- remove outliers\n",
        "  WHERE\n",
        "  -- rebalance on active non-buying days\n",
        "     ((a.test_control = 'control' AND (active_nonbuying > 0 AND active_nonbuying <= 5)   AND anb_percent_rank <= 1)\n",
        "      OR (a.test_control = 'control' AND (active_nonbuying > 5 AND active_nonbuying <= 10)  AND anb_percent_rank <= 1)\n",
        "      OR (a.test_control = 'control' AND (active_nonbuying > 10 AND active_nonbuying <= 15) AND anb_percent_rank <= 1)\n",
        "      OR (a.test_control = 'control' AND (active_nonbuying > 15 AND active_nonbuying <= 20) AND anb_percent_rank <= 1)\n",
        "      OR (a.test_control = 'control' AND (active_nonbuying > 20 AND active_nonbuying <= 25) AND anb_percent_rank <= 1)\n",
        "      OR (a.test_control = 'control' AND (active_nonbuying >                            25) AND anb_percent_rank <= 1)\n",
        "      OR (a.test_control = 'control' AND (active_nonbuying = 0 OR active_nonbuying IS null) AND anb_percent_rank <= 1)\n",
        "\n",
        "      OR (a.test_control = 'test' AND (active_nonbuying > 0 AND active_nonbuying <= 5)   AND anb_percent_rank <= 1)\n",
        "      OR (a.test_control = 'test' AND (active_nonbuying > 5 AND active_nonbuying <= 10)  AND anb_percent_rank <= 1)\n",
        "      OR (a.test_control = 'test' AND (active_nonbuying > 10 AND active_nonbuying <= 15) AND anb_percent_rank <= 1)\n",
        "      OR (a.test_control = 'test' AND (active_nonbuying > 15 AND active_nonbuying <= 20) AND anb_percent_rank <=  1)\n",
        "      OR (a.test_control = 'test' AND (active_nonbuying > 20 AND active_nonbuying <= 25) AND anb_percent_rank <= 1)\n",
        "      OR (a.test_control = 'test' AND (active_nonbuying >                            25) AND anb_percent_rank <=  1)\n",
        "      OR (a.test_control = 'test' AND (active_nonbuying = 0 OR active_nonbuying IS null) AND anb_percent_rank <= 1))\n",
        "),\n",
        "\n",
        "\n",
        "ab_purchase as (\n",
        "  SELECT\n",
        "    upm_id as upm_id_ab_buyer\n",
        "    , order_dt as order_ts\n",
        "    , order_nbr\n",
        "    ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "    ,SUM(CASE WHEN UPPER(DOL.LINE_OF_BUSINESS_DESC) = 'BASE INLINE' AND Univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_FW_demand\n",
        "    ,SUM(CASE WHEN UPPER(DOL.LINE_OF_BUSINESS_DESC) = 'BASE INLINE' AND Univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_AP_demand\n",
        "    ,SUM(CASE WHEN UPPER(DOL.LINE_OF_BUSINESS_DESC) = 'BASE INLINE' AND (Univ_div_desc = 'Equipment' OR Univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_EQ_demand\n",
        "    ,SUM(CASE WHEN UPPER(DOL.LINE_OF_BUSINESS_DESC) = 'CLEARANCE' AND Univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_FW_demand\n",
        "    ,SUM(CASE WHEN UPPER(DOL.LINE_OF_BUSINESS_DESC) = 'CLEARANCE' AND Univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_AP_demand\n",
        "    ,SUM(CASE WHEN UPPER(DOL.LINE_OF_BUSINESS_DESC) = 'CLEARANCE' AND (Univ_div_desc = 'Equipment' OR Univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_EQ_demand\n",
        "    ,SUM(CASE WHEN UPPER(DOL.LINE_OF_BUSINESS_DESC) = 'LAUNCH' AND Univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_FW_demand\n",
        "    ,SUM(CASE WHEN UPPER(DOL.LINE_OF_BUSINESS_DESC) = 'LAUNCH' AND Univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_AP_demand\n",
        "    ,SUM(CASE WHEN UPPER(DOL.LINE_OF_BUSINESS_DESC) = 'LAUNCH' AND (Univ_div_desc = 'Equipment' OR Univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_EQ_demand\n",
        "    ,SUM(CASE WHEN UPPER(DOL.LINE_OF_BUSINESS_DESC) NOT IN ('BASE INLINE', 'CLEARANCE', 'LAUNCH') THEN grd_amt_excl_tax_usd ELSE 0 END) AS rest_demand\n",
        "    ,COUNT(DISTINCT order_nbr) AS orders\n",
        "    ,SUM(origl_ordered_qty) AS units\n",
        "  FROM view_dol dol\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN ${hivevar:msmt_start_dt} AND date_add(${hivevar:msmt_end_dt}, 7)  -- 7 DAY MEASUREMENT FOR DEMAND AND CONVERSION\n",
        "  GROUP BY 1,2,3\n",
        ")\n",
        "\n",
        "--First Usage CTE\n",
        "/*\n",
        ", platform_usage as (\n",
        "  SELECT DISTINCT flt.upm_id\n",
        "  , CASE WHEN CAST(first_${hivevar:usage_platform}_activity_dt AS DATE) BETWEEN ${hivevar:msmt_start_dt} AND DATE_ADD(${hivevar:msmt_end_dt},7) THEN 1\n",
        "         ELSE 0\n",
        "    END AS platform_user\n",
        "  FROM aud_select_workspace.gck_first_and_last_touchpoint flt\n",
        "  INNER JOIN test_control_table tc ON flt.upm_id = tc.upm_id\n",
        ")\n",
        "*/\n",
        "\n",
        "\n",
        "------------------------------------------------------------------------------------------------\n",
        "----------------------------------------- Main Query -------------------------------------------\n",
        "------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "SELECT\n",
        "  test_control\n",
        "  ,COUNT(DISTINCT z.upm_id) AS total_members\n",
        "  -- DEMAND AND CONVERSION KPIs\n",
        "  ,COUNT(DISTINCT abp.upm_id_ab_buyer) AS buying_members\n",
        "  ,COUNT(DISTINCT abp.upm_id_ab_buyer) / COUNT(DISTINCT z.upm_id) AS conversion_rate\n",
        "  ,AVG(demand) AS avg_demand\n",
        "  ,stddev(demand) AS std_demand\n",
        "  -- secondary metrics ON order double click\n",
        "  ,SUM(demand) / SUM(orders) AS AOV\n",
        "  ,SUM(demand) / SUM(units) AS AUR\n",
        "  ,SUM(units) / SUM(orders) AS UPT\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  -- secondary metrics ON site/app engagement\n",
        "  ,AVG(visits) AS avg_site_app_visits\n",
        "  ,AVG(PDP_FAVORITE_COUNT) AS avg_PDP_FAVORITE_COUNT\n",
        "  ,AVG(ADD_TO_CART_COUNT) AS avg_ADD_TO_CART_COUNT\n",
        "  ,AVG(physical_activity) AS avg_physical_activity\n",
        "\n",
        "\n",
        "  -- secondar metrics ON margin\n",
        "  ,(AVG(base_FW_demand) * 0.38 + AVG(base_AP_demand) * 0.28 + AVG(base_EQ_demand) * 0.32\n",
        "    + AVG(clr_FW_demand) * 0.23 + AVG(base_AP_demand) * 0.17 + AVG(clr_EQ_demand) * 0.12\n",
        "    + AVG(launch_FW_demand) * 0.25 + AVG(launch_AP_demand) * 0.08 + AVG(launch_EQ_demand) * 0.19\n",
        "    + AVG(rest_demand) * 0.3\n",
        "  ) AS avg_margin\n",
        "  ,AVG(base_FW_demand) AS base_FW_demand\n",
        "  ,AVG(base_AP_demand) AS base_AP_demand\n",
        "  ,AVG(base_EQ_demand) AS base_EQ_demand\n",
        "  ,AVG(clr_FW_demand) AS clr_FW_demand\n",
        "  ,AVG(clr_AP_demand) AS clr_AP_demand\n",
        "  ,AVG(clr_EQ_demand) AS clr_EQ_demand\n",
        "  ,AVG(launch_FW_demand) AS launch_FW_demand\n",
        "  ,AVG(launch_AP_demand) AS launch_AP_demand\n",
        "  ,AVG(launch_EQ_demand) AS launch_EQ_demand\n",
        "  ,AVG(rest_demand) AS rest_demand\n",
        "\n",
        "\n",
        "  -- ACTIVE NON BUYING KPIs\n",
        "  ,SUM(active_non_buying) AS active_non_buying\n",
        "  ,AVG(active_non_buying) AS avg_active_non_buying\n",
        "  ,STD(active_non_buying) AS std_active_non_buying\n",
        "\n",
        "--------------------------------------- ADD AS NECESSARY ---------------------------------------\n",
        " /*\n",
        "  --Optional: FIRST PLATFORM USERS KPI\n",
        "  ,SUM(platform_user) AS first_usage\n",
        "  ,SUM(platform_user)/COUNT(DISTINCT z.upm_id) AS first_usage_pct\n",
        "*/\n",
        "/*  -- Optional KPI's  --\n",
        "  ,SUM(NEOD_Session) AS NEOD_Session\n",
        "*/\n",
        "\n",
        "\n",
        "FROM test_control_table z\n",
        "\n",
        "INNER JOIN balanced_aud b\n",
        "ON z.upm_id = b.upm_id\n",
        "\n",
        "LEFT JOIN member_activities a\n",
        "ON z.member_id = a.member_id\n",
        "\n",
        "LEFT JOIN ab_purchase abp\n",
        "ON z.upm_id = abp.upm_id_ab_buyer\n",
        "\n",
        "\n",
        "\n",
        "--------------------------------------- First Usage---------------------------------------\n",
        "--LEFT JOIN platform_usage plt ON z.upm_id = plt.upm_id\n",
        "\n",
        "WHERE (demand < 600 OR demand IS NULL)\n",
        "\n",
        "GROUP BY 1\n",
        "ORDER BY 1 DESC;"
      ],
      "metadata": {
        "id": "d81veeXYfNuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Original Code with User Defined Stats Functions"
      ],
      "metadata": {
        "id": "XfBCx1eGPjxX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBFuvbsdC8Uw"
      },
      "outputs": [],
      "source": [
        "%python\n",
        "\n",
        "# Execute SQL Query\n",
        "ab_data = \"\"\"\n",
        "WITH\n",
        " aa_purchase AS (\n",
        "  SELECT dol.upm_id\n",
        "    ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "    ,COUNT(DISTINCT order_nbr) AS orders\n",
        "    ,SUM(origl_ordered_qty) AS units\n",
        "  FROM view_dol dol\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,31) AND DATE_SUB(first_send_dt,1)\n",
        "  GROUP BY 1\n",
        "\n",
        "  -- AA period purchase. dynamically based on when the member receives their first communication.\n",
        ")\n",
        "\n",
        ", demand_bucket AS (\n",
        "  SELECT tct.upm_id\n",
        "  , tct.test_control\n",
        "  , aa.upm_id as upm_id_aa_buyer\n",
        "  , aa.demand\n",
        "  , aa.orders\n",
        "  , aa.units\n",
        "  , CASE\n",
        "    WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "    WHEN aa.demand < 50 THEN '000-050'\n",
        "    WHEN aa.demand < 100 THEN '050-100'\n",
        "    WHEN aa.demand < 200 THEN '100-200'\n",
        "    WHEN aa.demand < 500 THEN '200-500'\n",
        "    WHEN aa.demand < 1000 THEN '500-1000'\n",
        "    WHEN aa.demand >=1000 THEN '1000+'\n",
        "    END AS demand_per_buyer_bucket\n",
        "  , PERCENT_RANK() OVER (\n",
        "    PARTITION BY test_control\n",
        "    , CASE\n",
        "      WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "      WHEN aa.demand < 50 THEN '000-050'\n",
        "      WHEN aa.demand < 100 THEN '050-100'\n",
        "      WHEN aa.demand < 200 THEN '100-200'\n",
        "      WHEN aa.demand < 500 THEN '200-500'\n",
        "      WHEN aa.demand < 1000 THEN '500-1000'\n",
        "      WHEN aa.demand >=1000 THEN '1000+'\n",
        "      END ORDER BY RANDOM(0)\n",
        "    ) AS percent_rank\n",
        "  FROM test_control_table tct\n",
        "  LEFT JOIN aa_purchase aa ON tct.upm_id = aa.upm_id\n",
        ")\n",
        "\n",
        ", balanced_aud_aa as (\n",
        "  SELECT DISTINCT upm_id\n",
        "  FROM demand_bucket db\n",
        "  -- remove outliers\n",
        "  WHERE (demand < 2400 OR demand IS NULL)\n",
        "-- To rebalance, paste text block from template in place of defaults\n",
        "AND ((test_control = 'control' and (demand > 0 and demand <= 50) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand = 0 or demand is null) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand >0 and demand <= 50) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand = 0 or demand is null) and PERCENT_RANK <= 1)\n",
        "\t)\n",
        ")\n",
        "\n",
        ", mhub as (\n",
        " SELECT source_id as upm_id\n",
        " , member_id\n",
        " , preferred_gender\n",
        " FROM MEMBER.MEMBER_HUB\n",
        " WHERE ctry_2_cd = 'US'\n",
        " AND lower(dpa_status) IN ('activate','reactivate')\n",
        ")\n",
        "\n",
        "-- calculate a dynamic member activity window for each communication received by a member\n",
        "-- activity is calculated on a 7 day window, unless the member receives a communication within that timeframe\n",
        "-- When multiple messages are received, activity is only calculated up to the next message_send_dt to avoid double counting\n",
        ", member_activities as (\n",
        "  SELECT mgd.member_id\n",
        "  ,SUM(logged_in_visits_count) AS visits\n",
        "  ,SUM(pdp_favorite_count) AS pdp_favorite_count\n",
        "  ,SUM(add_to_cart_count) AS add_to_cart_count\n",
        "  ,SUM(physical_activity_count) AS physical_activity\n",
        "  ,MAX(CASE WHEN new_user_experience_flag = 'Y' THEN 1 ELSE 0 END) AS new_user_flag\n",
        "  FROM test_control_table tct\n",
        "  INNER JOIN mhub mh ON tct.upm_id = mh.upm_id\n",
        "  LEFT JOIN aud_select_workspace.member_agg_member_growth_daily mgd ON mh.member_id = mgd.member_id\n",
        "  WHERE mgd.activity_dt >= tct.campaign_send_dt\n",
        "    AND mgd.activity_dt < DATE_ADD(campaign_send_dt, INT(LEAST(7,COALESCE(difference_next_send_days,7))))\n",
        "    AND mgd.preferred_retail_geo = 'NA'\n",
        "    AND mgd.experience_name != 'agnostic'\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        "-- for each user, aggregate each order placed within our measurement window. create labels to categorize the type of demand\n",
        ", ab_purchase as (\n",
        "  SELECT dol.upm_id as upm_id_ab_buyer\n",
        "  , order_dt as order_ts\n",
        "  , order_nbr\n",
        "  ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc NOT IN ('Base Inline', 'Clearance', 'Launch') THEN grd_amt_excl_tax_usd ELSE 0 END) AS rest_demand\n",
        "  ,COUNT(DISTINCT order_nbr) AS orders\n",
        "  ,SUM(origl_ordered_qty) AS units\n",
        "  FROM view_dol dol\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN first_send_dt AND DATE_ADD(first_send_dt,7)\n",
        "  GROUP BY 1,2,3\n",
        ")\n",
        "\n",
        "-- For each member, rank each order by how near it was to the last campaign send\n",
        "-- purchases must be after the campaign send and within 7 days of the send to be attributed\n",
        ", aud_order_join as (\n",
        "SELECT a.*\n",
        ", b.*\n",
        ",(bigint(to_timestamp(b.order_ts))-bigint(to_timestamp(a.campaign_send_ts)))/60 as order_send_diff_minutes\n",
        ",RANK() OVER(PARTITION BY a.upm_id, b.order_ts ORDER BY (bigint(to_timestamp(b.order_ts))-bigint(to_timestamp(a.campaign_send_ts)))/60 ASC) as nearest_order\n",
        "FROM test_control_table a\n",
        "LEFT JOIN ab_purchase b ON (a.upm_id = b.upm_id_ab_buyer AND b.order_ts >= a.campaign_send_ts AND b.order_ts <= DATE_ADD(a.campaign_send_ts,7))\n",
        "ORDER BY a.upm_id,a.campaign_send_dt, nearest_order asc\n",
        ")\n",
        "\n",
        "-- Aggregate each distinct send, attributing only the nearest order.\n",
        "-- This prevent double counting of orders when a members receives multiple sends within the measurement window.\n",
        ", aud_order_agg as (\n",
        "SELECT upm_id AS upm_id_ab_buyer\n",
        "  ,SUM(demand) AS demand\n",
        "  ,SUM(base_FW_demand) AS base_FW_demand\n",
        "  ,SUM(base_AP_demand) AS base_AP_demand\n",
        "  ,SUM(base_EQ_demand) AS base_EQ_demand\n",
        "  ,SUM(clr_FW_demand) AS clr_FW_demand\n",
        "  ,SUM(clr_AP_demand) AS clr_AP_demand\n",
        "  ,SUM(clr_EQ_demand) AS clr_EQ_demand\n",
        "  ,SUM(launch_FW_demand) AS launch_FW_demand\n",
        "  ,SUM(launch_AP_demand) AS launch_AP_demand\n",
        "  ,SUM(launch_EQ_demand) AS launch_EQ_demand\n",
        "  ,SUM(rest_demand) AS rest_demand\n",
        "  ,SUM(orders) AS orders\n",
        "  ,SUM(units) AS units\n",
        "FROM aud_order_join a\n",
        "WHERE nearest_order = 1\n",
        "AND demand IS NOT NULL -- additional filter for extraneous records\n",
        "GROUP BY 1\n",
        "ORDER BY 1\n",
        ")\n",
        "\n",
        ", historical_purchase AS (\n",
        "  SELECT aoa.upm_id_ab_buyer as upm_id\n",
        "  , COUNT(DISTINCT order_nbr) as histOrders\n",
        "  FROM aud_order_agg aoa  -- eligible audience\n",
        "  INNER JOIN view_dol dol on aoa.upm_id_ab_buyer = dol.upm_id\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,730) AND DATE_SUB(first_send_dt,1)\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        "-- due to how messages are set up on Bluecore, the campaign_cd is not unique to a particular day. We concat with the message_sent_ts\n",
        "-- only using the date ignores cases where multiple sends occur on the same day, potentially undercounting\n",
        ",email_sent AS (\n",
        "  SELECT fed.upm_id\n",
        "  , COUNT(DISTINCT concat(campaign_cd, LEFT(message_sent_ts,21))) as email_circ\n",
        "  FROM COMMS.FACT_EMAIL_DELIVERY fed\n",
        "  WHERE (campaign_cd LIKE ${hivevar:camp_cd_tst_str} OR campaign_cd LIKE ${hivevar:camp_cd_ctl_str})\n",
        "  AND CAST(message_sent_ts AS DATE) between ${hivevar:msmt_start_dt} and ${hivevar:msmt_end_dt}\n",
        "  AND retail_geo_name = 'NA'\n",
        "  AND fed.upm_id IS NOT NULL\n",
        "  GROUP BY upm_id\n",
        ")\n",
        "\n",
        "-- returns all the instances of the campaign codes for each member along with the response types and revenue if applicable\n",
        ", email_response as (\n",
        "  SELECT DISTINCT fer.upm_id\n",
        "  , LEFT(response_ts,10) AS response_ts\n",
        "  , response_type\n",
        "  , revenue_dollar\n",
        "  FROM comms.fact_email_response fer\n",
        "  INNER JOIN test_control_table tct ON fer.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        "  WHERE (campaign_cd LIKE ${hivevar:camp_cd_tst_str} OR campaign_cd LIKE ${hivevar:camp_cd_ctl_str})\n",
        "  AND CAST(LEFT(response_ts,10) AS DATE) between first_send_dt and DATE_ADD(last_send_dt,7)\n",
        "  AND retail_geo_name = 'NA'\n",
        "  AND fer.upm_id IS NOT NULL\n",
        " )\n",
        "\n",
        "-- calculates the the total number of opens, clicks, purchases and last touch attributed demand for each member for the campaign\n",
        "-- responses are only considered if they occur between when the message was sent and 7 days later.\n",
        ", email_calc AS (\n",
        "  SELECT fer.upm_id\n",
        "  , COUNT(CASE WHEN response_type= 'email open' THEN fer.upm_id ELSE NULL END) AS email_opens\n",
        "  , COUNT(CASE WHEN response_type='link click' THEN fer.upm_id ELSE NULL END) AS email_clicks\n",
        "  , COUNT(CASE WHEN response_type='ecommerce-purchase' THEN fer.upm_id ELSE NULL END) AS email_purch\n",
        "  , SUM(CASE WHEN response_type='ecommerce-purchase' THEN revenue_dollar ELSE 0 END) AS email_demand\n",
        "  FROM email_response fer\n",
        "  INNER JOIN test_control_table tct ON fer.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        "  WHERE CAST(fer.response_ts as DATE) BETWEEN first_send_dt AND DATE_ADD(last_send_dt,7)\n",
        "  GROUP BY 1\n",
        ")\n",
        "-- total number of total messages send for each member during the campaign.\n",
        "-- For a singular campaign, this should be one (1), for triggers it could be more depending on how the trigger is set up\n",
        ", email_delivery_agg as (\n",
        "  SELECT upm_id\n",
        "  ,SUM(email_circ) AS email_circ\n",
        "  FROM email_sent\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", email_response_agg as (\n",
        "  SELECT upm_id\n",
        "  ,SUM(email_opens) AS email_opens\n",
        "  ,SUM(email_clicks) AS email_clicks\n",
        "  ,SUM(email_purch) AS email_purch\n",
        "  ,SUM(email_demand) AS email_demand\n",
        "  FROM email_calc\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", platform_usage as (\n",
        "  SELECT tct.upm_id\n",
        "  , CASE WHEN CAST(first_${hivevar:usage_platform}_activity_dt AS DATE) BETWEEN first_send_dt AND DATE_ADD(last_send_dt,7) THEN 1\n",
        "         ELSE 0\n",
        "    END AS platform_user\n",
        "  FROM aud_select_workspace.gck_first_and_last_touchpoint flt\n",
        "  INNER JOIN test_control_table tct ON flt.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        ")\n",
        "\n",
        "---------------------\n",
        "-- MAIN QUERY\n",
        "----------------------\n",
        "SELECT z.test_control\n",
        "  -- primary metrics ON incremental revenue: conversion rate, average demand per purchaser\n",
        "  ,COUNT(DISTINCT z.upm_id) AS total_members\n",
        "  ,COUNT(DISTINCT abp.upm_id_ab_buyer) AS buying_members\n",
        "  ,COUNT(DISTINCT abp.upm_id_ab_buyer) / COUNT(DISTINCT z.upm_id) AS conversion_rate\n",
        "  ,AVG(demand) AS avg_demand\n",
        "  ,stddev(demand) AS std_demand\n",
        "  -- secondary metrics ON order double click\n",
        "  ,SUM(demand) / SUM(orders) AS AOV\n",
        "  ,SUM(demand) / SUM(units) AS AUR\n",
        "  ,SUM(units) / SUM(orders) AS UPT\n",
        "\n",
        "  -- secondary metrics ON emails\n",
        "  ,SUM(email_circ) AS email_sent\n",
        "  ,SUM(email_opens) AS email_opens\n",
        "  ,SUM(email_clicks) AS email_clicks\n",
        "  ,SUM(email_purch) AS email_purch\n",
        "  ,SUM(email_demand) AS email_demand\n",
        "  ,SUM(email_opens) / SUM(email_circ) AS email_open_rate\n",
        "  ,SUM(email_clicks) / SUM(email_circ) AS email_click_rate --against sends\n",
        "  --,SUM(email_clicks) / SUM(email_opens) AS email_click_rate --against clicks\n",
        "  ,SUM(email_purch) / SUM(email_circ) AS email_conversion_rate\n",
        "\n",
        "  -- secondary metrics ON site/app engagement\n",
        "  ,AVG(visits) AS avg_site_app_visits\n",
        "  ,AVG(PDP_FAVORITE_COUNT) AS avg_PDP_FAVORITE_COUNT\n",
        "  ,AVG(ADD_TO_CART_COUNT) AS avg_ADD_TO_CART_COUNT\n",
        "  ,AVG(physical_activity) AS avg_physical_activity\n",
        "\n",
        "  -- secondary metrics ON margin\n",
        "  ,(AVG(base_FW_demand) * 0.38 + AVG(base_AP_demand) * 0.28 + AVG(base_EQ_demand) * 0.32\n",
        "    + AVG(clr_FW_demand) * 0.23 + AVG(base_AP_demand) * 0.17 + AVG(clr_EQ_demand) * 0.12\n",
        "    + AVG(launch_FW_demand) * 0.25 + AVG(launch_AP_demand) * 0.08 + AVG(launch_EQ_demand) * 0.19\n",
        "    + AVG(rest_demand) * 0.3\n",
        "  ) AS avg_margin\n",
        "  ,AVG(base_FW_demand) AS base_FW_demand\n",
        "  ,AVG(base_AP_demand) AS base_AP_demand\n",
        "  ,AVG(base_EQ_demand) AS base_EQ_demand\n",
        "  ,AVG(clr_FW_demand) AS clr_FW_demand\n",
        "  ,AVG(clr_AP_demand) AS clr_AP_demand\n",
        "  ,AVG(clr_EQ_demand) AS clr_EQ_demand\n",
        "  ,AVG(launch_FW_demand) AS launch_FW_demand\n",
        "  ,AVG(launch_AP_demand) AS launch_AP_demand\n",
        "  ,AVG(launch_EQ_demand) AS launch_EQ_demand\n",
        "  ,AVG(rest_demand) AS rest_demand\n",
        "    -- secondary metrics ON order history\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END) AS 1_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END)/COUNT(DISTINCT z.upm_id) AS pctBuyers_1_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN demand ELSE 0 END)/SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END) AS DPB_1_prior\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END) AS 2_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END)/COUNT(DISTINCT z.upm_id) AS pctBuyers_2_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN demand ELSE 0 END)/SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END) AS DPB_2_prior\n",
        "    -- optional first usage kpi\n",
        "  ,SUM(platform_user) AS first_usage\n",
        "\n",
        "FROM test_control_table z\n",
        "INNER JOIN balanced_aud_aa b ON z.upm_id = b.upm_id  --inner join our balanced audience to remove members that were distorting the AA period balance\n",
        "\n",
        "INNER JOIN mhub mh ON z.upm_id = mh.upm_id\n",
        "LEFT JOIN member_activities a ON mh.member_id = a.member_id\n",
        "\n",
        "LEFT JOIN aud_order_agg abp ON z.upm_id = abp.upm_id_ab_buyer\n",
        "LEFT JOIN historical_purchase hp ON z.upm_id = hp.upm_id\n",
        "\n",
        "LEFT JOIN email_delivery_agg cda ON z.upm_id = cda.upm_id\n",
        "LEFT JOIN email_response_agg cra ON z.upm_id = cra.upm_id\n",
        "\n",
        "LEFT JOIN platform_usage plt ON z.upm_id = plt.upm_id\n",
        "\n",
        "GROUP BY 1\n",
        "ORDER BY 1 DESC;\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Load the data into a PySpark DataFrame\n",
        "df_spark = spark.sql(ab_data)\n",
        "\n",
        "# Convert to Pandas DataFrame for further analysis\n",
        "df = df_spark.toPandas()\n",
        "\n",
        "# Separate control and test group data\n",
        "control = df[df['test_control'] == 'control']\n",
        "test = df[df['test_control'] == 'test']\n",
        "\n",
        "# Extract input parameters for Demand\n",
        "control_avg_sales = control['avg_demand'].values[0]\n",
        "test_avg_sales = test['avg_demand'].values[0]\n",
        "control_std_dev = control['std_demand'].values[0]\n",
        "test_std_dev = test['std_demand'].values[0]\n",
        "control_sample_size = control['buying_members'].values[0]\n",
        "test_sample_size = test['buying_members'].values[0]\n",
        "\n",
        "# Perform Statistical Analysis for Demand\n",
        "\n",
        "# Calculate delta\n",
        "delta = abs(test_avg_sales - control_avg_sales)\n",
        "\n",
        "# Calculate standard error\n",
        "standard_error = np.sqrt((control_std_dev**2 / control_sample_size) +\n",
        "                         (test_std_dev**2 / test_sample_size))\n",
        "# Calculate z-value\n",
        "z_value = delta / standard_error\n",
        "\n",
        "# Calculate p-value and confidence level\n",
        "p_value = 2 * (1 - stats.norm.cdf(abs(z_value)))  # Two-tailed test\n",
        "confidence_level = 1 - p_value\n",
        "type_i_error = p_value # In a two-tailed test, the p-value is the Type I error\n",
        "\n",
        "# Extracting input parameters for Conversion\n",
        "control_conv_rate = control['conversion_rate'].values[0]\n",
        "test_conv_rate = test['conversion_rate'].values[0]\n",
        "conv_control_sample_size = control['total_members'].values[0]\n",
        "conv_test_sample_size = test['total_members'].values[0]\n",
        "lift = abs(test_conv_rate - control_conv_rate)  # (Abs. % point)\n",
        "\n",
        "# Perform Statistical Analysis for Conversion\n",
        "\n",
        "difference_of_proportions = test_conv_rate - control_conv_rate\n",
        "# Calculate standard error\n",
        "conv_standard_error = np.sqrt((control_conv_rate * (1 - control_conv_rate) / control_sample_size) +\n",
        "                         (test_conv_rate * (1 - test_conv_rate) / test_sample_size))\n",
        "# Calculate z value\n",
        "conv_z_value = difference_of_proportions / conv_standard_error\n",
        "# Calculate p-value and confidence level\n",
        "conv_p_value = 2 * (1 - stats.norm.cdf(abs(conv_z_value)))  # Two-tailed test\n",
        "conv_confidence_level = 1 - conv_p_value\n",
        "conv_type_i_error = conv_p_value\n",
        "\n",
        "# Demand Metrics DataFrame\n",
        "data_demand = {\n",
        "    'Metric': ['Avg Demand', 'Std Deviation of Demand', 'Sample Size',\n",
        "               'Absolute Difference Between Means (Delta)', 'Standard Error', 'Z-Value'],\n",
        "    'Control Group': [control_avg_sales, control_std_dev, control_sample_size, '', '', ''],\n",
        "    'Test Group': [test_avg_sales, test_std_dev, test_sample_size, '', '', ''],\n",
        "    'Value': ['', '', '', delta, standard_error, z_value]\n",
        "}\n",
        "\n",
        "demand_metrics_df = pd.DataFrame(data_demand)\n",
        "\n",
        "# Update rows for Delta, Standard Error, and Z-Value, as they are not group-specific\n",
        "demand_metrics_df.loc[demand_metrics_df['Metric'] == 'Absolute Difference Between Means (Delta)', ['Control Group', 'Test Group', 'Value']] = ['', '', delta]\n",
        "demand_metrics_df.loc[demand_metrics_df['Metric'] == 'Standard Error', ['Control Group', 'Test Group', 'Value']] = ['', '', standard_error]\n",
        "demand_metrics_df.loc[demand_metrics_df['Metric'] == 'Z-Value', ['Control Group', 'Test Group', 'Value']] = ['', '', z_value]\n",
        "\n",
        "# Demand Significance DataFrame\n",
        "data_demand_significance = {\n",
        "    'Significance Metric': ['Achieved Confidence Level', 'Type I Error (α)'],\n",
        "    'Value': [confidence_level, type_i_error]\n",
        "}\n",
        "\n",
        "demand_significance_df = pd.DataFrame(data_demand_significance)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Demand Metrics DataFrame\n",
        "data_demand = {\n",
        "    'Metric': ['Avg Demand', 'Std Deviation of Demand', 'Sample Size',\n",
        "               'Absolute Difference Between Means (Delta)', 'Standard Error', 'Z-Value','Achieved Confidence Level', 'Type I Error (α)'],\n",
        "    'Control Group': [control_avg_sales, control_std_dev, control_sample_size, '', '', '', '', ''],\n",
        "    'Test Group': [test_avg_sales, test_std_dev, test_sample_size, '', '', '','', ''],\n",
        "    'Value': ['', '', '', delta, standard_error, z_value, confidence_level, type_i_error]\n",
        "}\n",
        "\n",
        "demand_metrics_df = pd.DataFrame(data_demand)\n",
        "\n",
        "# Update rows for Delta, Standard Error, and Z-Value, as they are in dataframe calcuations\n",
        "demand_metrics_df.loc[demand_metrics_df['Metric'] == 'Absolute Difference Between Means (Delta)', ['Control Group', 'Test Group', 'Value']] = ['', '', delta,]\n",
        "demand_metrics_df.loc[demand_metrics_df['Metric'] == 'Standard Error', ['Control Group', 'Test Group', 'Value']] = ['', '', standard_error]\n",
        "demand_metrics_df.loc[demand_metrics_df['Metric'] == 'Z-Value', ['Control Group', 'Test Group', 'Value']] = ['', '', z_value]\n",
        "\n",
        "print(demand_metrics_df)\n",
        "\n",
        "\n",
        "# Conversion Metrics DataFrame\n",
        "data_conversion = {\n",
        "    'Metric': ['Conversion Rate', 'Lift (Absolute % Point Difference)', 'Difference of Proportions',\n",
        "               'Standard Error', 'Z-Value', 'Achieved Confidence Level', 'Type I Error (α)'],\n",
        "    'Control Group': [control_conv_rate, '', '', '', '', '', ''],\n",
        "    'Test Group': [test_conv_rate, '', '', '', '','', ''],\n",
        "    'Value': ['', lift, difference_of_proportions, conv_standard_error, conv_z_value, conv_confidence_level, conv_type_i_error]\n",
        "}\n",
        "\n",
        "conversion_metrics_df = pd.DataFrame(data_conversion)\n",
        "\n",
        "# Update rows for Delta, Standard Error, and Z-Value, as they are in dataframe calcuations\n",
        "conversion_metrics_df.loc[conversion_metrics_df['Metric'] == 'Lift (Absolute % Point Difference)', ['Control Group', 'Test Group', 'Value']] = ['', '', lift]\n",
        "conversion_metrics_df.loc[conversion_metrics_df['Metric'] == 'Difference of Proportions', ['Control Group', 'Test Group', 'Value']] = ['', '', difference_of_proportions]\n",
        "conversion_metrics_df.loc[conversion_metrics_df['Metric'] == 'Standard Error', ['Control Group', 'Test Group', 'Value']] = ['', '', conv_standard_error]\n",
        "conversion_metrics_df.loc[conversion_metrics_df['Metric'] == 'Z-Value', ['Control Group', 'Test Group', 'Value']] = ['', '', conv_z_value]\n",
        "\n",
        "print(conversion_metrics_df)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Update with Stats calcs to convert to Scipy\n",
        "\n",
        "# Calculate delta\n",
        "# Calculate standard error\n",
        "# Calculate z value\n",
        "# Calculate p-value\n",
        "# Calculate type I error\n",
        "# Calculate confidence level\n",
        "# Calculate statistical significance\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming control and test groups are already defined as DataFrames\n",
        "\n",
        "# Assume 'avg_demand' and 'std_demand' are columns in the DataFrame\n",
        "\n",
        "# Perform t-test for Avg Demand\n",
        "t_stat, p_value_demand = stats.ttest_ind(test['avg_demand'], control['avg_demand'], equal_var=False)\n",
        "confidence_level_demand = 1 - p_value_demand\n",
        "type_i_error_demand = p_value_demand\n",
        "\n",
        "# Perform t-test for Conversion Rate (Replace 'conversion_rate' with the actual column names in your DataFrame)\n",
        "t_stat_conv, p_value_conv = stats.ttest_ind(test['conversion_rate'], control['conversion_rate'], equal_var=False)\n",
        "confidence_level_conv = 1 - p_value_conv\n",
        "type_i_error_conv = p_value_conv\n",
        "\n",
        "# Calculate lift for demonstration (you'll adjust this based on your metric of interest)\n",
        "# Lift calculation example for conversion rate\n",
        "lift_conversion_rate = ((test['conversion_rate'].mean() - control['conversion_rate'].mean()) / control['conversion_rate'].mean()) * 100\n",
        "\n",
        "# The above examples show how to use SciPy for p-value and confidence level calculations.\n",
        "# For delta, standard error, and z-value, these are part of the t-test calculation, and you typically wouldn't calculate these separately when using a t-test.\n",
        "# If you need to report these, they're more relevant in the context of manual calculations or when not using a t-test directly.\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Turn Word **Wrap**"
      ],
      "metadata": {
        "id": "xuMfCev6KW1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# turn on word wrap\n",
        "\n",
        "import textwrap\n",
        "\n",
        "wrapper = textwrap.TextWrapper(width=40,\n",
        "    initial_indent=\" \" * 4,\n",
        "    subsequent_indent=\" \" * 4,\n",
        "    break_long_words=False,\n",
        "    break_on_hyphens=False)"
      ],
      "metadata": {
        "id": "xHP3MUunKK9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Updated with SciPy Stats calculations, rather than user defined functions"
      ],
      "metadata": {
        "id": "XhkzKUaAIK00"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xZLDB3lICer"
      },
      "outputs": [],
      "source": [
        "\n",
        "%python\n",
        "\n",
        "# Execute SQL Query\n",
        "ab_data = \"\"\"\n",
        "WITH\n",
        " aa_purchase AS (\n",
        "  SELECT dol.upm_id\n",
        "    ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "    ,COUNT(DISTINCT order_nbr) AS orders\n",
        "    ,SUM(origl_ordered_qty) AS units\n",
        "  FROM view_dol dol\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,31) AND DATE_SUB(first_send_dt,1)\n",
        "  GROUP BY 1\n",
        "\n",
        "  -- AA period purchase. dynamically based on when the member receives their first communication.\n",
        ")\n",
        "\n",
        ", demand_bucket AS (\n",
        "  SELECT tct.upm_id\n",
        "  , tct.test_control\n",
        "  , aa.upm_id as upm_id_aa_buyer\n",
        "  , aa.demand\n",
        "  , aa.orders\n",
        "  , aa.units\n",
        "  , CASE\n",
        "    WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "    WHEN aa.demand < 50 THEN '000-050'\n",
        "    WHEN aa.demand < 100 THEN '050-100'\n",
        "    WHEN aa.demand < 200 THEN '100-200'\n",
        "    WHEN aa.demand < 500 THEN '200-500'\n",
        "    WHEN aa.demand < 1000 THEN '500-1000'\n",
        "    WHEN aa.demand >=1000 THEN '1000+'\n",
        "    END AS demand_per_buyer_bucket\n",
        "  , PERCENT_RANK() OVER (\n",
        "    PARTITION BY test_control\n",
        "    , CASE\n",
        "      WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "      WHEN aa.demand < 50 THEN '000-050'\n",
        "      WHEN aa.demand < 100 THEN '050-100'\n",
        "      WHEN aa.demand < 200 THEN '100-200'\n",
        "      WHEN aa.demand < 500 THEN '200-500'\n",
        "      WHEN aa.demand < 1000 THEN '500-1000'\n",
        "      WHEN aa.demand >=1000 THEN '1000+'\n",
        "      END ORDER BY RANDOM(0)\n",
        "    ) AS percent_rank\n",
        "  FROM test_control_table tct\n",
        "  LEFT JOIN aa_purchase aa ON tct.upm_id = aa.upm_id\n",
        ")\n",
        "\n",
        ", balanced_aud_aa as (\n",
        "  SELECT DISTINCT upm_id\n",
        "  FROM demand_bucket db\n",
        "  -- remove outliers\n",
        "  WHERE (demand < 2400 OR demand IS NULL)\n",
        "-- To rebalance, paste text block from template in place of defaults\n",
        "AND ((test_control = 'control' and (demand > 0 and demand <= 50) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand = 0 or demand is null) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand >0 and demand <= 50) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand = 0 or demand is null) and PERCENT_RANK <= 1)\n",
        "\t)\n",
        ")\n",
        "\n",
        ", mhub as (\n",
        " SELECT source_id as upm_id\n",
        " , member_id\n",
        " , preferred_gender\n",
        " FROM MEMBER.MEMBER_HUB\n",
        " WHERE ctry_2_cd = 'US'\n",
        " AND lower(dpa_status) IN ('activate','reactivate')\n",
        ")\n",
        "\n",
        "-- calculate a dynamic member activity window for each communication received by a member\n",
        "-- activity is calculated on a 7 day window, unless the member receives a communication within that timeframe\n",
        "-- When multiple messages are received, activity is only calculated up to the next message_send_dt to avoid double counting\n",
        ", member_activities as (\n",
        "  SELECT mgd.member_id\n",
        "  ,SUM(logged_in_visits_count) AS visits\n",
        "  ,SUM(pdp_favorite_count) AS pdp_favorite_count\n",
        "  ,SUM(add_to_cart_count) AS add_to_cart_count\n",
        "  ,SUM(physical_activity_count) AS physical_activity\n",
        "  ,MAX(CASE WHEN new_user_experience_flag = 'Y' THEN 1 ELSE 0 END) AS new_user_flag\n",
        "  FROM test_control_table tct\n",
        "  INNER JOIN mhub mh ON tct.upm_id = mh.upm_id\n",
        "  LEFT JOIN aud_select_workspace.member_agg_member_growth_daily mgd ON mh.member_id = mgd.member_id\n",
        "  WHERE mgd.activity_dt >= tct.campaign_send_dt\n",
        "    AND mgd.activity_dt < DATE_ADD(campaign_send_dt, INT(LEAST(7,COALESCE(difference_next_send_days,7))))\n",
        "    AND mgd.preferred_retail_geo = 'NA'\n",
        "    AND mgd.experience_name != 'agnostic'\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        "-- for each user, aggregate each order placed within our measurement window. create labels to categorize the type of demand\n",
        ", ab_purchase as (\n",
        "  SELECT dol.upm_id as upm_id_ab_buyer\n",
        "  , order_dt as order_ts\n",
        "  , order_nbr\n",
        "  ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc NOT IN ('Base Inline', 'Clearance', 'Launch') THEN grd_amt_excl_tax_usd ELSE 0 END) AS rest_demand\n",
        "  ,COUNT(DISTINCT order_nbr) AS orders\n",
        "  ,SUM(origl_ordered_qty) AS units\n",
        "  FROM view_dol dol\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN first_send_dt AND DATE_ADD(first_send_dt,7)\n",
        "  GROUP BY 1,2,3\n",
        ")\n",
        "\n",
        "-- For each member, rank each order by how near it was to the last campaign send\n",
        "-- purchases must be after the campaign send and within 7 days of the send to be attributed\n",
        ", aud_order_join as (\n",
        "SELECT a.*\n",
        ", b.*\n",
        ",(bigint(to_timestamp(b.order_ts))-bigint(to_timestamp(a.campaign_send_ts)))/60 as order_send_diff_minutes\n",
        ",RANK() OVER(PARTITION BY a.upm_id, b.order_ts ORDER BY (bigint(to_timestamp(b.order_ts))-bigint(to_timestamp(a.campaign_send_ts)))/60 ASC) as nearest_order\n",
        "FROM test_control_table a\n",
        "LEFT JOIN ab_purchase b ON (a.upm_id = b.upm_id_ab_buyer AND b.order_ts >= a.campaign_send_ts AND b.order_ts <= DATE_ADD(a.campaign_send_ts,7))\n",
        "ORDER BY a.upm_id,a.campaign_send_dt, nearest_order asc\n",
        ")\n",
        "\n",
        "-- Aggregate each distinct send, attributing only the nearest order.\n",
        "-- This prevent double counting of orders when a members receives multiple sends within the measurement window.\n",
        ", aud_order_agg as (\n",
        "SELECT upm_id AS upm_id_ab_buyer\n",
        "  ,SUM(demand) AS demand\n",
        "  ,SUM(base_FW_demand) AS base_FW_demand\n",
        "  ,SUM(base_AP_demand) AS base_AP_demand\n",
        "  ,SUM(base_EQ_demand) AS base_EQ_demand\n",
        "  ,SUM(clr_FW_demand) AS clr_FW_demand\n",
        "  ,SUM(clr_AP_demand) AS clr_AP_demand\n",
        "  ,SUM(clr_EQ_demand) AS clr_EQ_demand\n",
        "  ,SUM(launch_FW_demand) AS launch_FW_demand\n",
        "  ,SUM(launch_AP_demand) AS launch_AP_demand\n",
        "  ,SUM(launch_EQ_demand) AS launch_EQ_demand\n",
        "  ,SUM(rest_demand) AS rest_demand\n",
        "  ,SUM(orders) AS orders\n",
        "  ,SUM(units) AS units\n",
        "FROM aud_order_join a\n",
        "WHERE nearest_order = 1\n",
        "AND demand IS NOT NULL -- additional filter for extraneous records\n",
        "GROUP BY 1\n",
        "ORDER BY 1\n",
        ")\n",
        "\n",
        ", historical_purchase AS (\n",
        "  SELECT aoa.upm_id_ab_buyer as upm_id\n",
        "  , COUNT(DISTINCT order_nbr) as histOrders\n",
        "  FROM aud_order_agg aoa  -- eligible audience\n",
        "  INNER JOIN view_dol dol on aoa.upm_id_ab_buyer = dol.upm_id\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,730) AND DATE_SUB(first_send_dt,1)\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        "-- due to how messages are set up on Bluecore, the campaign_cd is not unique to a particular day. We concat with the message_sent_ts\n",
        "-- only using the date ignores cases where multiple sends occur on the same day, potentially undercounting\n",
        ",email_sent AS (\n",
        "  SELECT fed.upm_id\n",
        "  , COUNT(DISTINCT concat(campaign_cd, LEFT(message_sent_ts,21))) as email_circ\n",
        "  FROM COMMS.FACT_EMAIL_DELIVERY fed\n",
        "  WHERE (campaign_cd LIKE ${hivevar:camp_cd_tst_str} OR campaign_cd LIKE ${hivevar:camp_cd_ctl_str})\n",
        "  AND CAST(message_sent_ts AS DATE) between ${hivevar:msmt_start_dt} and ${hivevar:msmt_end_dt}\n",
        "  AND retail_geo_name = 'NA'\n",
        "  AND fed.upm_id IS NOT NULL\n",
        "  GROUP BY upm_id\n",
        ")\n",
        "\n",
        "-- returns all the instances of the campaign codes for each member along with the response types and revenue if applicable\n",
        ", email_response as (\n",
        "  SELECT DISTINCT fer.upm_id\n",
        "  , LEFT(response_ts,10) AS response_ts\n",
        "  , response_type\n",
        "  , revenue_dollar\n",
        "  FROM comms.fact_email_response fer\n",
        "  INNER JOIN test_control_table tct ON fer.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        "  WHERE (campaign_cd LIKE ${hivevar:camp_cd_tst_str} OR campaign_cd LIKE ${hivevar:camp_cd_ctl_str})\n",
        "  AND CAST(LEFT(response_ts,10) AS DATE) between first_send_dt and DATE_ADD(last_send_dt,7)\n",
        "  AND retail_geo_name = 'NA'\n",
        "  AND fer.upm_id IS NOT NULL\n",
        " )\n",
        "\n",
        "-- calculates the the total number of opens, clicks, purchases and last touch attributed demand for each member for the campaign\n",
        "-- responses are only considered if they occur between when the message was sent and 7 days later.\n",
        ", email_calc AS (\n",
        "  SELECT fer.upm_id\n",
        "  , COUNT(CASE WHEN response_type= 'email open' THEN fer.upm_id ELSE NULL END) AS email_opens\n",
        "  , COUNT(CASE WHEN response_type='link click' THEN fer.upm_id ELSE NULL END) AS email_clicks\n",
        "  , COUNT(CASE WHEN response_type='ecommerce-purchase' THEN fer.upm_id ELSE NULL END) AS email_purch\n",
        "  , SUM(CASE WHEN response_type='ecommerce-purchase' THEN revenue_dollar ELSE 0 END) AS email_demand\n",
        "  FROM email_response fer\n",
        "  INNER JOIN test_control_table tct ON fer.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        "  WHERE CAST(fer.response_ts as DATE) BETWEEN first_send_dt AND DATE_ADD(last_send_dt,7)\n",
        "  GROUP BY 1\n",
        ")\n",
        "-- total number of total messages send for each member during the campaign.\n",
        "-- For a singular campaign, this should be one (1), for triggers it could be more depending on how the trigger is set up\n",
        ", email_delivery_agg as (\n",
        "  SELECT upm_id\n",
        "  ,SUM(email_circ) AS email_circ\n",
        "  FROM email_sent\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", email_response_agg as (\n",
        "  SELECT upm_id\n",
        "  ,SUM(email_opens) AS email_opens\n",
        "  ,SUM(email_clicks) AS email_clicks\n",
        "  ,SUM(email_purch) AS email_purch\n",
        "  ,SUM(email_demand) AS email_demand\n",
        "  FROM email_calc\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", platform_usage as (\n",
        "  SELECT tct.upm_id\n",
        "  , CASE WHEN CAST(first_${hivevar:usage_platform}_activity_dt AS DATE) BETWEEN first_send_dt AND DATE_ADD(last_send_dt,7) THEN 1\n",
        "         ELSE 0\n",
        "    END AS platform_user\n",
        "  FROM aud_select_workspace.gck_first_and_last_touchpoint flt\n",
        "  INNER JOIN test_control_table tct ON flt.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        ")\n",
        "\n",
        "---------------------\n",
        "-- MAIN QUERY\n",
        "----------------------\n",
        "SELECT z.test_control\n",
        "  -- primary metrics ON incremental revenue: conversion rate, average demand per purchaser\n",
        "  ,COUNT(DISTINCT z.upm_id) AS total_members\n",
        "  ,COUNT(DISTINCT abp.upm_id_ab_buyer) AS buying_members\n",
        "  ,COUNT(DISTINCT abp.upm_id_ab_buyer) / COUNT(DISTINCT z.upm_id) AS conversion_rate\n",
        "  ,AVG(demand) AS avg_demand\n",
        "  ,stddev(demand) AS std_demand\n",
        "  -- secondary metrics ON order double click\n",
        "  ,SUM(demand) / SUM(orders) AS AOV\n",
        "  ,SUM(demand) / SUM(units) AS AUR\n",
        "  ,SUM(units) / SUM(orders) AS UPT\n",
        "\n",
        "  -- secondary metrics ON emails\n",
        "  ,SUM(email_circ) AS email_sent\n",
        "  ,SUM(email_opens) AS email_opens\n",
        "  ,SUM(email_clicks) AS email_clicks\n",
        "  ,SUM(email_purch) AS email_purch\n",
        "  ,SUM(email_demand) AS email_demand\n",
        "  ,SUM(email_opens) / SUM(email_circ) AS email_open_rate\n",
        "  ,SUM(email_clicks) / SUM(email_circ) AS email_click_rate --against sends\n",
        "  --,SUM(email_clicks) / SUM(email_opens) AS email_click_rate --against clicks\n",
        "  ,SUM(email_purch) / SUM(email_circ) AS email_conversion_rate\n",
        "\n",
        "  -- secondary metrics ON site/app engagement\n",
        "  ,AVG(visits) AS avg_site_app_visits\n",
        "  ,AVG(PDP_FAVORITE_COUNT) AS avg_PDP_FAVORITE_COUNT\n",
        "  ,AVG(ADD_TO_CART_COUNT) AS avg_ADD_TO_CART_COUNT\n",
        "  ,AVG(physical_activity) AS avg_physical_activity\n",
        "\n",
        "  -- secondary metrics ON margin\n",
        "  ,(AVG(base_FW_demand) * 0.38 + AVG(base_AP_demand) * 0.28 + AVG(base_EQ_demand) * 0.32\n",
        "    + AVG(clr_FW_demand) * 0.23 + AVG(base_AP_demand) * 0.17 + AVG(clr_EQ_demand) * 0.12\n",
        "    + AVG(launch_FW_demand) * 0.25 + AVG(launch_AP_demand) * 0.08 + AVG(launch_EQ_demand) * 0.19\n",
        "    + AVG(rest_demand) * 0.3\n",
        "  ) AS avg_margin\n",
        "  ,AVG(base_FW_demand) AS base_FW_demand\n",
        "  ,AVG(base_AP_demand) AS base_AP_demand\n",
        "  ,AVG(base_EQ_demand) AS base_EQ_demand\n",
        "  ,AVG(clr_FW_demand) AS clr_FW_demand\n",
        "  ,AVG(clr_AP_demand) AS clr_AP_demand\n",
        "  ,AVG(clr_EQ_demand) AS clr_EQ_demand\n",
        "  ,AVG(launch_FW_demand) AS launch_FW_demand\n",
        "  ,AVG(launch_AP_demand) AS launch_AP_demand\n",
        "  ,AVG(launch_EQ_demand) AS launch_EQ_demand\n",
        "  ,AVG(rest_demand) AS rest_demand\n",
        "    -- secondary metrics ON order history\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END) AS 1_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END)/COUNT(DISTINCT z.upm_id) AS pctBuyers_1_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN demand ELSE 0 END)/SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END) AS DPB_1_prior\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END) AS 2_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END)/COUNT(DISTINCT z.upm_id) AS pctBuyers_2_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN demand ELSE 0 END)/SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END) AS DPB_2_prior\n",
        "    -- optional first usage kpi\n",
        "  ,SUM(platform_user) AS first_usage\n",
        "\n",
        "FROM test_control_table z\n",
        "INNER JOIN balanced_aud_aa b ON z.upm_id = b.upm_id  --inner join our balanced audience to remove members that were distorting the AA period balance\n",
        "\n",
        "INNER JOIN mhub mh ON z.upm_id = mh.upm_id\n",
        "LEFT JOIN member_activities a ON mh.member_id = a.member_id\n",
        "\n",
        "LEFT JOIN aud_order_agg abp ON z.upm_id = abp.upm_id_ab_buyer\n",
        "LEFT JOIN historical_purchase hp ON z.upm_id = hp.upm_id\n",
        "\n",
        "LEFT JOIN email_delivery_agg cda ON z.upm_id = cda.upm_id\n",
        "LEFT JOIN email_response_agg cra ON z.upm_id = cra.upm_id\n",
        "\n",
        "LEFT JOIN platform_usage plt ON z.upm_id = plt.upm_id\n",
        "\n",
        "GROUP BY 1\n",
        "ORDER BY 1 DESC;\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Load the data into a PySpark DataFrame\n",
        "df_spark = spark.sql(ab_data)\n",
        "\n",
        "# Convert to Pandas DataFrame for further analysis\n",
        "df = df_spark.toPandas()\n",
        "\n",
        "# Separate control and test group data\n",
        "control = df[df['test_control'] == 'control']\n",
        "test = df[df['test_control'] == 'test']\n",
        "\n",
        "# Extract input parameters for Demand\n",
        "\n",
        "# Assuming 'avg_demand' and 'std_demand' were calculated in SQL and are columns in the DataFrame\n",
        "control_avg_sales = control['avg_demand'].values[0]\n",
        "test_avg_sales = test['avg_demand'].values[0]\n",
        "control_std_dev = control['std_demand'].values[0]\n",
        "test_std_dev = test['std_demand'].values[0]\n",
        "control_sample_size = control['buying_members'].values[0]\n",
        "test_sample_size = test['buying_members'].values[0]\n",
        "\n",
        "# Perform Statistical Analysis for Demand\n",
        "\n",
        "\n",
        "# Calculate delta and z-value\n",
        "    \"\"\"\n",
        "    The delta, standard error, and z-value, are part of the t-test calculation.\n",
        "\n",
        "    You typically wouldn't calculate these separately when using a t-test.\n",
        "\n",
        "    If you need to see these, they're more relevant in the context of manual calculations or when not using a t-test directly.\n",
        "    \"\"\"\n",
        "\n",
        "# Calculate standard error\n",
        "    \"\"\"\n",
        "    A note about standard error:\n",
        "\n",
        "    * When using stats.ttest_ind with equal_var=False, the test internally computes a version of standard error adjusted for the Welch's t-test.\n",
        "    * Welch's t-test doesn't assume equal population variances.\n",
        "    * For educational purposes, you can still calculate a simple standard error of the mean (SEM) for each group.\n",
        "    * Or calculate the standard error of the difference between two means if you're comparing them directly.\n",
        "    * Code to do so follows below...\n",
        "\n",
        "    # Calculate the standard error of the mean for each group\n",
        "    sem_control = control['avg_demand'].sem()\n",
        "    sem_test = test['avg_demand'].sem()\n",
        "\n",
        "    # Calculate the standard error for the difference between two means\n",
        "    # Again, this formula is for illustrative purposes; the t-test already accounts for this in its calculation\n",
        "    n_control = len(control['avg_demand'])\n",
        "    n_test = len(test['avg_demand'])\n",
        "    std_error_diff = np.sqrt(sem_control**2 / n_control + sem_test**2 / n_test)\n",
        "\n",
        "    print(f\"Standard Error (Control): {sem_control}\")\n",
        "    print(f\"Standard Error (Test): {sem_test}\")\n",
        "    print(f\"Standard Error of Difference: {std_error_diff}\")\n",
        "    \"\"\"\n",
        "\n",
        "# Perform t-test for Avg Demand\n",
        "t_stat, p_value_demand = stats.ttest_ind(test['avg_demand'], control['avg_demand'], equal_var=False)\n",
        "confidence_level_demand = 1 - p_value_demand\n",
        "type_i_error_demand = p_value_demand\n",
        "\n",
        "# Perform t-test for Conversion Rate\n",
        "t_stat_conv, p_value_conv = stats.ttest_ind(test['conversion_rate'], control['conversion_rate'], equal_var=False)\n",
        "confidence_level_conv = 1 - p_value_conv\n",
        "type_i_error_conv = p_value_conv\n",
        "\n",
        "# Perform Statistical Analysis for Conversion\n",
        "\n",
        "# Extracting input parameters for Conversion\n",
        "control_conv_rate = control['conversion_rate'].values[0]\n",
        "test_conv_rate = test['conversion_rate'].values[0]\n",
        "conv_control_sample_size = control['total_members'].values[0]\n",
        "conv_test_sample_size = test['total_members'].values[0]\n",
        "lift_conversion_rate = ((test['conversion_rate'].mean() - control['conversion_rate'].mean()) / control['conversion_rate'].mean()) * 100\n",
        "difference_of_proportions = test_conv_rate - control_conv_rate\n",
        "\n",
        "# Populate the scorecard DataFrame\n",
        "scorecard_df = pd.DataFrame({\n",
        "    'Metric': ['Avg Demand', 'Conversion Rate', 'Lift - Conversion Rate'],\n",
        "    'P-Value': [p_value_demand, p_value_conv, '-'],\n",
        "    'Confidence Level': [f\"{confidence_level_demand:.2f}\", f\"{confidence_level_conv:.2f}\", '-'],\n",
        "    'Lift': ['-', '-', f\"{lift_conversion_rate:.2f}%\"]\n",
        "})\n",
        "\n",
        "scorecard_df\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Demand Metrics DataFrame\n",
        "data_demand = {\n",
        "    'Metric': ['Avg Demand', 'Std Deviation of Demand', 'Sample Size',\n",
        "               'Absolute Difference Between Means (Delta)', 'Standard Error', 'Z-Value'],\n",
        "    'Control Group': [control_avg_sales, control_std_dev, control_sample_size, '', '', ''],\n",
        "    'Test Group': [test_avg_sales, test_std_dev, test_sample_size, '', '', ''],\n",
        "    'Value': ['', '', '', delta, standard_error, z_value]\n",
        "}\n",
        "\n",
        "demand_metrics_df = pd.DataFrame(data_demand)\n",
        "\n",
        "# Update rows for Delta, Standard Error, and Z-Value, as they are not group-specific\n",
        "demand_metrics_df.loc[demand_metrics_df['Metric'] == 'Absolute Difference Between Means (Delta)', ['Control Group', 'Test Group', 'Value']] = ['', '', delta]\n",
        "demand_metrics_df.loc[demand_metrics_df['Metric'] == 'Standard Error', ['Control Group', 'Test Group', 'Value']] = ['', '', standard_error]\n",
        "demand_metrics_df.loc[demand_metrics_df['Metric'] == 'Z-Value', ['Control Group', 'Test Group', 'Value']] = ['', '', z_value]\n",
        "\n",
        "# Demand Significance DataFrame\n",
        "data_demand_significance = {\n",
        "    'Significance Metric': ['Achieved Confidence Level', 'Type I Error (α)'],\n",
        "    'Value': [confidence_level, type_i_error]\n",
        "}\n",
        "\n",
        "demand_significance_df = pd.DataFrame(data_demand_significance)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Demand Metrics DataFrame\n",
        "data_demand = {\n",
        "    'Metric': ['Avg Demand', 'Std Deviation of Demand', 'Sample Size',\n",
        "               'Absolute Difference Between Means (Delta)', 'Standard Error', 'Z-Value','Achieved Confidence Level', 'Type I Error (α)'],\n",
        "    'Control Group': [control_avg_sales, control_std_dev, control_sample_size, '', '', '', '', ''],\n",
        "    'Test Group': [test_avg_sales, test_std_dev, test_sample_size, '', '', '','', ''],\n",
        "    'Value': ['', '', '', delta, standard_error, z_value, confidence_level, type_i_error]\n",
        "}\n",
        "\n",
        "demand_metrics_df = pd.DataFrame(data_demand)\n",
        "\n",
        "# Update rows for Delta, Standard Error, and Z-Value, as they are in dataframe calcuations\n",
        "demand_metrics_df.loc[demand_metrics_df['Metric'] == 'Absolute Difference Between Means (Delta)', ['Control Group', 'Test Group', 'Value']] = ['', '', delta,]\n",
        "demand_metrics_df.loc[demand_metrics_df['Metric'] == 'Standard Error', ['Control Group', 'Test Group', 'Value']] = ['', '', standard_error]\n",
        "demand_metrics_df.loc[demand_metrics_df['Metric'] == 'Z-Value', ['Control Group', 'Test Group', 'Value']] = ['', '', z_value]\n",
        "\n",
        "print(demand_metrics_df)\n",
        "\n",
        "\n",
        "# Conversion Metrics DataFrame\n",
        "data_conversion = {\n",
        "    'Metric': ['Conversion Rate', 'Lift (Absolute % Point Difference)', 'Difference of Proportions',\n",
        "               'Standard Error', 'Z-Value', 'Achieved Confidence Level', 'Type I Error (α)'],\n",
        "    'Control Group': [control_conv_rate, '', '', '', '', '', ''],\n",
        "    'Test Group': [test_conv_rate, '', '', '', '','', ''],\n",
        "    'Value': ['', lift, difference_of_proportions, conv_standard_error, conv_z_value, conv_confidence_level, conv_type_i_error]\n",
        "}\n",
        "\n",
        "conversion_metrics_df = pd.DataFrame(data_conversion)\n",
        "\n",
        "# Update rows for Delta, Standard Error, and Z-Value, as they are in dataframe calcuations\n",
        "conversion_metrics_df.loc[conversion_metrics_df['Metric'] == 'Lift (Absolute % Point Difference)', ['Control Group', 'Test Group', 'Value']] = ['', '', lift]\n",
        "conversion_metrics_df.loc[conversion_metrics_df['Metric'] == 'Difference of Proportions', ['Control Group', 'Test Group', 'Value']] = ['', '', difference_of_proportions]\n",
        "conversion_metrics_df.loc[conversion_metrics_df['Metric'] == 'Standard Error', ['Control Group', 'Test Group', 'Value']] = ['', '', conv_standard_error]\n",
        "conversion_metrics_df.loc[conversion_metrics_df['Metric'] == 'Z-Value', ['Control Group', 'Test Group', 'Value']] = ['', '', conv_z_value]\n",
        "\n",
        "print(conversion_metrics_df)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Update with Stats calcs to convert to Scipy\n",
        "\n",
        "# Calculate delta\n",
        "# Calculate standard error\n",
        "# Calculate z value\n",
        "# Calculate p-value\n",
        "# Calculate type I error\n",
        "# Calculate confidence level\n",
        "# Calculate statistical significance\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming control and test groups are already defined as DataFrames\n",
        "\n",
        "# Assume 'avg_demand' and 'std_demand' are columns in the DataFrame\n",
        "\n",
        "# Perform t-test for Avg Demand\n",
        "t_stat, p_value_demand = stats.ttest_ind(test['avg_demand'], control['avg_demand'], equal_var=False)\n",
        "confidence_level_demand = 1 - p_value_demand\n",
        "type_i_error_demand = p_value_demand\n",
        "\n",
        "# Perform t-test for Conversion Rate (Replace 'conversion_rate' with the actual column names in your DataFrame)\n",
        "t_stat_conv, p_value_conv = stats.ttest_ind(test['conversion_rate'], control['conversion_rate'], equal_var=False)\n",
        "confidence_level_conv = 1 - p_value_conv\n",
        "type_i_error_conv = p_value_conv\n",
        "\n",
        "# Calculate lift for conversion\n",
        "lift_conversion_rate = ((test['conversion_rate'].mean() - control['conversion_rate'].mean()) / control['conversion_rate'].mean()) * 100\n",
        "\n",
        "# The above examples show how to use SciPy for p-value and confidence level calculations.\n",
        "# For delta, standard error, and z-value, these are part of the t-test calculation, and you typically wouldn't calculate these separately when using a t-test.\n",
        "# If you need to understand these, they're more relevant in the context of manual calculations or when not using a t-test directly.\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a Pandas dataframe to bypass SQL calculations entirely"
      ],
      "metadata": {
        "id": "fQsGrNjVypKt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bAQoBY4zyyvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gky9fCDzyzQN"
      },
      "outputs": [],
      "source": [
        "\n",
        "%sql\n",
        "\n",
        "\n",
        "WITH\n",
        " aa_purchase AS (\n",
        "  SELECT dol.upm_id\n",
        "    ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "    ,COUNT(DISTINCT order_nbr) AS orders\n",
        "    ,SUM(origl_ordered_qty) AS units\n",
        "  FROM view_dol dol\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,31) AND DATE_SUB(first_send_dt,1)\n",
        "  GROUP BY 1\n",
        "\n",
        "  -- AA period purchase. dynamically based on when the member receives their first communication.\n",
        ")\n",
        "\n",
        ", demand_bucket AS (\n",
        "  SELECT tct.upm_id\n",
        "  , tct.test_control\n",
        "  , aa.upm_id as upm_id_aa_buyer\n",
        "  , aa.demand\n",
        "  , aa.orders\n",
        "  , aa.units\n",
        "  , CASE\n",
        "    WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "    WHEN aa.demand < 50 THEN '000-050'\n",
        "    WHEN aa.demand < 100 THEN '050-100'\n",
        "    WHEN aa.demand < 200 THEN '100-200'\n",
        "    WHEN aa.demand < 500 THEN '200-500'\n",
        "    WHEN aa.demand < 1000 THEN '500-1000'\n",
        "    WHEN aa.demand >=1000 THEN '1000+'\n",
        "    END AS demand_per_buyer_bucket\n",
        "  , PERCENT_RANK() OVER (\n",
        "    PARTITION BY test_control\n",
        "    , CASE\n",
        "      WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "      WHEN aa.demand < 50 THEN '000-050'\n",
        "      WHEN aa.demand < 100 THEN '050-100'\n",
        "      WHEN aa.demand < 200 THEN '100-200'\n",
        "      WHEN aa.demand < 500 THEN '200-500'\n",
        "      WHEN aa.demand < 1000 THEN '500-1000'\n",
        "      WHEN aa.demand >=1000 THEN '1000+'\n",
        "      END ORDER BY RANDOM(0)\n",
        "    ) AS percent_rank\n",
        "  FROM test_control_table tct\n",
        "  LEFT JOIN aa_purchase aa ON tct.upm_id = aa.upm_id\n",
        ")\n",
        "\n",
        ", balanced_aud_aa as (\n",
        "  SELECT DISTINCT upm_id\n",
        "  FROM demand_bucket db\n",
        "  -- remove outliers\n",
        "  WHERE (demand < 2400 OR demand IS NULL)\n",
        "-- To rebalance, paste text block from template in place of defaults\n",
        "AND ((test_control = 'control' and (demand > 0 and demand <= 50) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand = 0 or demand is null) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand >0 and demand <= 50) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand = 0 or demand is null) and PERCENT_RANK <= 1)\n",
        "\t)\n",
        ")\n",
        "\n",
        ", mhub as (\n",
        " SELECT source_id as upm_id\n",
        " , member_id\n",
        " , preferred_gender\n",
        " FROM MEMBER.MEMBER_HUB\n",
        " WHERE ctry_2_cd = 'US'\n",
        " AND lower(dpa_status) IN ('activate','reactivate')\n",
        ")\n",
        "\n",
        "-- calculate a dynamic member activity window for each communication received by a member\n",
        "-- activity is calculated on a 7 day window, unless the member receives a communication within that timeframe\n",
        "-- When multiple messages are received, activity is only calculated up to the next message_send_dt to avoid double counting\n",
        ", member_activities as (\n",
        "  SELECT mgd.member_id\n",
        "  ,SUM(logged_in_visits_count) AS visits\n",
        "  ,SUM(pdp_favorite_count) AS pdp_favorite_count\n",
        "  ,SUM(add_to_cart_count) AS add_to_cart_count\n",
        "  ,SUM(physical_activity_count) AS physical_activity\n",
        "  ,MAX(CASE WHEN new_user_experience_flag = 'Y' THEN 1 ELSE 0 END) AS new_user_flag\n",
        "  FROM test_control_table tct\n",
        "  INNER JOIN mhub mh ON tct.upm_id = mh.upm_id\n",
        "  LEFT JOIN aud_select_workspace.member_agg_member_growth_daily mgd ON mh.member_id = mgd.member_id\n",
        "  WHERE mgd.activity_dt >= tct.campaign_send_dt\n",
        "    AND mgd.activity_dt < DATE_ADD(campaign_send_dt, INT(LEAST(7,COALESCE(difference_next_send_days,7))))\n",
        "    AND mgd.preferred_retail_geo = 'NA'\n",
        "    AND mgd.experience_name != 'agnostic'\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        "-- for each user, aggregate each order placed within our measurement window. create labels to categorize the type of demand\n",
        ", ab_purchase as (\n",
        "  SELECT dol.upm_id as upm_id_ab_buyer\n",
        "  , order_dt as order_ts\n",
        "  , order_nbr\n",
        "  ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc NOT IN ('Base Inline', 'Clearance', 'Launch') THEN grd_amt_excl_tax_usd ELSE 0 END) AS rest_demand\n",
        "  ,COUNT(DISTINCT order_nbr) AS orders\n",
        "  ,SUM(origl_ordered_qty) AS units\n",
        "  FROM view_dol dol\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN first_send_dt AND DATE_ADD(first_send_dt,7)\n",
        "  GROUP BY 1,2,3\n",
        ")\n",
        "\n",
        "-- For each member, rank each order by how near it was to the last campaign send\n",
        "-- purchases must be after the campaign send and within 7 days of the send to be attributed\n",
        ", aud_order_join as (\n",
        "SELECT a.*\n",
        ", b.*\n",
        ",(bigint(to_timestamp(b.order_ts))-bigint(to_timestamp(a.campaign_send_ts)))/60 as order_send_diff_minutes\n",
        ",RANK() OVER(PARTITION BY a.upm_id, b.order_ts ORDER BY (bigint(to_timestamp(b.order_ts))-bigint(to_timestamp(a.campaign_send_ts)))/60 ASC) as nearest_order\n",
        "FROM test_control_table a\n",
        "LEFT JOIN ab_purchase b ON (a.upm_id = b.upm_id_ab_buyer AND b.order_ts >= a.campaign_send_ts AND b.order_ts <= DATE_ADD(a.campaign_send_ts,7))\n",
        "ORDER BY a.upm_id,a.campaign_send_dt, nearest_order asc\n",
        ")\n",
        "\n",
        "-- Aggregate each distinct send, attributing only the nearest order.\n",
        "-- This prevent double counting of orders when a members receives multiple sends within the measurement window.\n",
        ", aud_order_agg as (\n",
        "SELECT upm_id AS upm_id_ab_buyer\n",
        "  ,SUM(demand) AS demand\n",
        "  ,SUM(base_FW_demand) AS base_FW_demand\n",
        "  ,SUM(base_AP_demand) AS base_AP_demand\n",
        "  ,SUM(base_EQ_demand) AS base_EQ_demand\n",
        "  ,SUM(clr_FW_demand) AS clr_FW_demand\n",
        "  ,SUM(clr_AP_demand) AS clr_AP_demand\n",
        "  ,SUM(clr_EQ_demand) AS clr_EQ_demand\n",
        "  ,SUM(launch_FW_demand) AS launch_FW_demand\n",
        "  ,SUM(launch_AP_demand) AS launch_AP_demand\n",
        "  ,SUM(launch_EQ_demand) AS launch_EQ_demand\n",
        "  ,SUM(rest_demand) AS rest_demand\n",
        "  ,SUM(orders) AS orders\n",
        "  ,SUM(units) AS units\n",
        "FROM aud_order_join a\n",
        "WHERE nearest_order = 1\n",
        "AND demand IS NOT NULL -- additional filter for extraneous records\n",
        "GROUP BY 1\n",
        "ORDER BY 1\n",
        ")\n",
        "\n",
        ", historical_purchase AS (\n",
        "  SELECT aoa.upm_id_ab_buyer as upm_id\n",
        "  , COUNT(DISTINCT order_nbr) as histOrders\n",
        "  FROM aud_order_agg aoa  -- eligible audience\n",
        "  INNER JOIN view_dol dol on aoa.upm_id_ab_buyer = dol.upm_id\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,730) AND DATE_SUB(first_send_dt,1)\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        "-- due to how messages are set up on Bluecore, the campaign_cd is not unique to a particular day. We concat with the message_sent_ts\n",
        "-- only using the date ignores cases where multiple sends occur on the same day, potentially undercounting\n",
        ",email_sent AS (\n",
        "  SELECT fed.upm_id\n",
        "  , COUNT(DISTINCT concat(campaign_cd, LEFT(message_sent_ts,21))) as email_circ\n",
        "  FROM COMMS.FACT_EMAIL_DELIVERY fed\n",
        "  WHERE (campaign_cd LIKE ${hivevar:camp_cd_tst_str} OR campaign_cd LIKE ${hivevar:camp_cd_ctl_str})\n",
        "  AND CAST(message_sent_ts AS DATE) between ${hivevar:msmt_start_dt} and ${hivevar:msmt_end_dt}\n",
        "  AND retail_geo_name = 'NA'\n",
        "  AND fed.upm_id IS NOT NULL\n",
        "  GROUP BY upm_id\n",
        ")\n",
        "\n",
        "-- returns all the instances of the campaign codes for each member along with the response types and revenue if applicable\n",
        ", email_response as (\n",
        "  SELECT DISTINCT fer.upm_id\n",
        "  , LEFT(response_ts,10) AS response_ts\n",
        "  , response_type\n",
        "  , revenue_dollar\n",
        "  FROM comms.fact_email_response fer\n",
        "  INNER JOIN test_control_table tct ON fer.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        "  WHERE (campaign_cd LIKE ${hivevar:camp_cd_tst_str} OR campaign_cd LIKE ${hivevar:camp_cd_ctl_str})\n",
        "  AND CAST(LEFT(response_ts,10) AS DATE) between first_send_dt and DATE_ADD(last_send_dt,7)\n",
        "  AND retail_geo_name = 'NA'\n",
        "  AND fer.upm_id IS NOT NULL\n",
        " )\n",
        "\n",
        "-- calculates the the total number of opens, clicks, purchases and last touch attributed demand for each member for the campaign\n",
        "-- responses are only considered if they occur between when the message was sent and 7 days later.\n",
        ", email_calc AS (\n",
        "  SELECT fer.upm_id\n",
        "  , COUNT(CASE WHEN response_type= 'email open' THEN fer.upm_id ELSE NULL END) AS email_opens\n",
        "  , COUNT(CASE WHEN response_type='link click' THEN fer.upm_id ELSE NULL END) AS email_clicks\n",
        "  , COUNT(CASE WHEN response_type='ecommerce-purchase' THEN fer.upm_id ELSE NULL END) AS email_purch\n",
        "  , SUM(CASE WHEN response_type='ecommerce-purchase' THEN revenue_dollar ELSE 0 END) AS email_demand\n",
        "  FROM email_response fer\n",
        "  INNER JOIN test_control_table tct ON fer.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        "  WHERE CAST(fer.response_ts as DATE) BETWEEN first_send_dt AND DATE_ADD(last_send_dt,7)\n",
        "  GROUP BY 1\n",
        ")\n",
        "-- total number of total messages send for each member during the campaign.\n",
        "-- For a singular campaign, this should be one (1), for triggers it could be more depending on how the trigger is set up\n",
        ", email_delivery_agg as (\n",
        "  SELECT upm_id\n",
        "  ,SUM(email_circ) AS email_circ\n",
        "  FROM email_sent\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", email_response_agg as (\n",
        "  SELECT upm_id\n",
        "  ,SUM(email_opens) AS email_opens\n",
        "  ,SUM(email_clicks) AS email_clicks\n",
        "  ,SUM(email_purch) AS email_purch\n",
        "  ,SUM(email_demand) AS email_demand\n",
        "  FROM email_calc\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", platform_usage as (\n",
        "  SELECT tct.upm_id\n",
        "  , CASE WHEN CAST(first_${hivevar:usage_platform}_activity_dt AS DATE) BETWEEN first_send_dt AND DATE_ADD(last_send_dt,7) THEN 1\n",
        "         ELSE 0\n",
        "    END AS platform_user\n",
        "  FROM aud_select_workspace.gck_first_and_last_touchpoint flt\n",
        "  INNER JOIN test_control_table tct ON flt.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        ")\n",
        "\n",
        "---------------------\n",
        "-- MAIN QUERY\n",
        "----------------------\n",
        "SELECT z.upm_id\n",
        "  , z.test_control\n",
        "\n",
        "  -- primary metrics ON incremental revenue: conversion rate, average demand per purchaser\n",
        "  ,COUNT(DISTINCT abp.upm_id_ab_buyer) AS buying_members\n",
        "  ,demand\n",
        "\n",
        "  -- secondary metrics ON order double click\n",
        "  ,orders\n",
        "  ,units\n",
        "\n",
        "  -- secondary metrics ON emails\n",
        "  , email_circ AS email_sent\n",
        "  , email_opens\n",
        "  , email_clicks\n",
        "  , email_purch\n",
        "  , email_demand\n",
        "\n",
        "  -- secondary metrics ON site/app engagement\n",
        "  , visits AS site_app_visits\n",
        "  , PDP_FAVORITE_COUNT\n",
        "  , ADD_TO_CART_COUNT\n",
        "  , physical_activity\n",
        "\n",
        "  -- secondary metrics ON margin\n",
        "\n",
        "  -- calculate margin\n",
        "  /*\n",
        "  ,(AVG(base_FW_demand) * 0.38 + AVG(base_AP_demand) * 0.28 + AVG(base_EQ_demand) * 0.32\n",
        "    + AVG(clr_FW_demand) * 0.23 + AVG(base_AP_demand) * 0.17 + AVG(clr_EQ_demand) * 0.12\n",
        "    + AVG(launch_FW_demand) * 0.25 + AVG(launch_AP_demand) * 0.08 + AVG(launch_EQ_demand) * 0.19\n",
        "    + AVG(rest_demand) * 0.3\n",
        "  ) AS avg_margin\n",
        "  */\n",
        "\n",
        "  , base_FW_demand\n",
        "  , base_AP_demand\n",
        "  , base_EQ_demand\n",
        "  , clr_FW_demand\n",
        "  , clr_AP_demand\n",
        "  , clr_EQ_demand\n",
        "  , launch_FW_demand\n",
        "  , launch_AP_demand\n",
        "  , launch_EQ_demand\n",
        "  , rest_demand\n",
        "\n",
        "    -- secondary metrics ON order history\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END) AS 1_priorOrder\n",
        "\n",
        "  /*\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END)/COUNT(DISTINCT z.upm_id) AS pctBuyers_1_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN demand ELSE 0 END)/SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END) AS DPB_1_prior\n",
        "  */\n",
        "\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END) AS 2_priorOrder\n",
        "\n",
        "  /*\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END)/COUNT(DISTINCT z.upm_id) AS pctBuyers_2_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN demand ELSE 0 END)/SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END) AS DPB_2_prior\n",
        "  */\n",
        "\n",
        "    -- optional first usage kpi\n",
        "  , platform_user\n",
        "\n",
        "FROM test_control_table z\n",
        "INNER JOIN balanced_aud_aa b ON z.upm_id = b.upm_id  --inner join our balanced audience to remove members that were distorting the AA period balance\n",
        "\n",
        "INNER JOIN mhub mh ON z.upm_id = mh.upm_id\n",
        "LEFT JOIN member_activities a ON mh.member_id = a.member_id\n",
        "\n",
        "LEFT JOIN aud_order_agg abp ON z.upm_id = abp.upm_id_ab_buyer\n",
        "LEFT JOIN historical_purchase hp ON z.upm_id = hp.upm_id\n",
        "\n",
        "LEFT JOIN email_delivery_agg cda ON z.upm_id = cda.upm_id\n",
        "LEFT JOIN email_response_agg cra ON z.upm_id = cra.upm_id\n",
        "\n",
        "LEFT JOIN platform_usage plt ON z.upm_id = plt.upm_id\n",
        "\n",
        "GROUP BY 1\n",
        "ORDER BY 1 DESC;\n",
        "\n",
        "\n",
        "\n",
        "# Load the data into a PySpark DataFrame\n",
        "df_spark = spark.sql(ab_data)\n",
        "\n",
        "# Convert to Pandas DataFrame for further analysis\n",
        "df = df_spark.toPandas()\n",
        "\n",
        "# Separate control and test group data\n",
        "control = df[df['test_control'] == 'control']\n",
        "test = df[df['test_control'] == 'test']\n",
        "\n",
        "# Extract input parameters for Demand\n",
        "\n",
        "# Assuming 'avg_demand' and 'std_demand' were calculated in SQL and are columns in the DataFrame\n",
        "control_avg_sales = control['avg_demand'].values[0]\n",
        "test_avg_sales = test['avg_demand'].values[0]\n",
        "control_std_dev = control['std_demand'].values[0]\n",
        "test_std_dev = test['std_demand'].values[0]\n",
        "control_sample_size = control['buying_members'].values[0]\n",
        "test_sample_size = test['buying_members'].values[0]\n",
        "\n",
        "# Perform Statistical Analysis for Demand\n",
        "\n",
        "\n",
        "# Calculate delta and z-value\n",
        "    \"\"\"\n",
        "    The delta, standard error, and z-value, are part of the t-test calculation.\n",
        "\n",
        "    You typically wouldn't calculate these separately when using a t-test.\n",
        "\n",
        "    If you need to see these, they're more relevant in the context of manual calculations or when not using a t-test directly.\n",
        "    \"\"\"\n",
        "\n",
        "# Calculate standard error\n",
        "    \"\"\"\n",
        "    A note about standard error:\n",
        "\n",
        "    * When using stats.ttest_ind with equal_var=False, the test internally computes a version of standard error adjusted for the Welch's t-test.\n",
        "    * Welch's t-test doesn't assume equal population variances.\n",
        "    * For educational purposes, you can still calculate a simple standard error of the mean (SEM) for each group.\n",
        "    * Or calculate the standard error of the difference between two means if you're comparing them directly.\n",
        "    * Code to do so follows below...\n",
        "\n",
        "    # Calculate the standard error of the mean for each group\n",
        "    sem_control = control['avg_demand'].sem()\n",
        "    sem_test = test['avg_demand'].sem()\n",
        "\n",
        "    # Calculate the standard error for the difference between two means\n",
        "    # Again, this formula is for illustrative purposes; the t-test already accounts for this in its calculation\n",
        "    n_control = len(control['avg_demand'])\n",
        "    n_test = len(test['avg_demand'])\n",
        "    std_error_diff = np.sqrt(sem_control**2 / n_control + sem_test**2 / n_test)\n",
        "\n",
        "    print(f\"Standard Error (Control): {sem_control}\")\n",
        "    print(f\"Standard Error (Test): {sem_test}\")\n",
        "    print(f\"Standard Error of Difference: {std_error_diff}\")\n",
        "    \"\"\"\n",
        "\n",
        "# Perform t-test for Avg Demand\n",
        "t_stat, p_value_demand = stats.ttest_ind(test['avg_demand'], control['avg_demand'], equal_var=False)\n",
        "confidence_level_demand = 1 - p_value_demand\n",
        "type_i_error_demand = p_value_demand\n",
        "\n",
        "# Perform t-test for Conversion Rate\n",
        "t_stat_conv, p_value_conv = stats.ttest_ind(test['conversion_rate'], control['conversion_rate'], equal_var=False)\n",
        "confidence_level_conv = 1 - p_value_conv\n",
        "type_i_error_conv = p_value_conv\n",
        "\n",
        "# Perform Statistical Analysis for Conversion\n",
        "\n",
        "# Extracting input parameters for Conversion\n",
        "control_conv_rate = control['conversion_rate'].values[0]\n",
        "test_conv_rate = test['conversion_rate'].values[0]\n",
        "conv_control_sample_size = control['total_members'].values[0]\n",
        "conv_test_sample_size = test['total_members'].values[0]\n",
        "lift_conversion_rate = ((test['conversion_rate'].mean() - control['conversion_rate'].mean()) / control['conversion_rate'].mean()) * 100\n",
        "difference_of_proportions = test_conv_rate - control_conv_rate\n",
        "\n",
        "# Populate the scorecard DataFrame\n",
        "scorecard_df = pd.DataFrame({\n",
        "    'Metric': ['Avg Demand', 'Conversion Rate', 'Lift - Conversion Rate'],\n",
        "    'P-Value': [p_value_demand, p_value_conv, '-'],\n",
        "    'Confidence Level': [f\"{confidence_level_demand:.2f}\", f\"{confidence_level_conv:.2f}\", '-'],\n",
        "    'Lift': ['-', '-', f\"{lift_conversion_rate:.2f}%\"]\n",
        "})\n",
        "\n",
        "scorecard_df\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Demand Metrics DataFrame\n",
        "data_demand = {\n",
        "    'Metric': ['Avg Demand', 'Std Deviation of Demand', 'Sample Size',\n",
        "               'Absolute Difference Between Means (Delta)', 'Standard Error', 'Z-Value'],\n",
        "    'Control Group': [control_avg_sales, control_std_dev, control_sample_size, '', '', ''],\n",
        "    'Test Group': [test_avg_sales, test_std_dev, test_sample_size, '', '', ''],\n",
        "    'Value': ['', '', '', delta, standard_error, z_value]\n",
        "}\n",
        "\n",
        "demand_metrics_df = pd.DataFrame(data_demand)\n",
        "\n",
        "# Update rows for Delta, Standard Error, and Z-Value, as they are not group-specific\n",
        "demand_metrics_df.loc[demand_metrics_df['Metric'] == 'Absolute Difference Between Means (Delta)', ['Control Group', 'Test Group', 'Value']] = ['', '', delta]\n",
        "demand_metrics_df.loc[demand_metrics_df['Metric'] == 'Standard Error', ['Control Group', 'Test Group', 'Value']] = ['', '', standard_error]\n",
        "demand_metrics_df.loc[demand_metrics_df['Metric'] == 'Z-Value', ['Control Group', 'Test Group', 'Value']] = ['', '', z_value]\n",
        "\n",
        "# Demand Significance DataFrame\n",
        "data_demand_significance = {\n",
        "    'Significance Metric': ['Achieved Confidence Level', 'Type I Error (α)'],\n",
        "    'Value': [confidence_level, type_i_error]\n",
        "}\n",
        "\n",
        "demand_significance_df = pd.DataFrame(data_demand_significance)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Demand Metrics DataFrame\n",
        "data_demand = {\n",
        "    'Metric': ['Avg Demand', 'Std Deviation of Demand', 'Sample Size',\n",
        "               'Absolute Difference Between Means (Delta)', 'Standard Error', 'Z-Value','Achieved Confidence Level', 'Type I Error (α)'],\n",
        "    'Control Group': [control_avg_sales, control_std_dev, control_sample_size, '', '', '', '', ''],\n",
        "    'Test Group': [test_avg_sales, test_std_dev, test_sample_size, '', '', '','', ''],\n",
        "    'Value': ['', '', '', delta, standard_error, z_value, confidence_level, type_i_error]\n",
        "}\n",
        "\n",
        "demand_metrics_df = pd.DataFrame(data_demand)\n",
        "\n",
        "# Update rows for Delta, Standard Error, and Z-Value, as they are in dataframe calcuations\n",
        "demand_metrics_df.loc[demand_metrics_df['Metric'] == 'Absolute Difference Between Means (Delta)', ['Control Group', 'Test Group', 'Value']] = ['', '', delta,]\n",
        "demand_metrics_df.loc[demand_metrics_df['Metric'] == 'Standard Error', ['Control Group', 'Test Group', 'Value']] = ['', '', standard_error]\n",
        "demand_metrics_df.loc[demand_metrics_df['Metric'] == 'Z-Value', ['Control Group', 'Test Group', 'Value']] = ['', '', z_value]\n",
        "\n",
        "print(demand_metrics_df)\n",
        "\n",
        "\n",
        "# Conversion Metrics DataFrame\n",
        "data_conversion = {\n",
        "    'Metric': ['Conversion Rate', 'Lift (Absolute % Point Difference)', 'Difference of Proportions',\n",
        "               'Standard Error', 'Z-Value', 'Achieved Confidence Level', 'Type I Error (α)'],\n",
        "    'Control Group': [control_conv_rate, '', '', '', '', '', ''],\n",
        "    'Test Group': [test_conv_rate, '', '', '', '','', ''],\n",
        "    'Value': ['', lift, difference_of_proportions, conv_standard_error, conv_z_value, conv_confidence_level, conv_type_i_error]\n",
        "}\n",
        "\n",
        "conversion_metrics_df = pd.DataFrame(data_conversion)\n",
        "\n",
        "# Update rows for Delta, Standard Error, and Z-Value, as they are in dataframe calcuations\n",
        "conversion_metrics_df.loc[conversion_metrics_df['Metric'] == 'Lift (Absolute % Point Difference)', ['Control Group', 'Test Group', 'Value']] = ['', '', lift]\n",
        "conversion_metrics_df.loc[conversion_metrics_df['Metric'] == 'Difference of Proportions', ['Control Group', 'Test Group', 'Value']] = ['', '', difference_of_proportions]\n",
        "conversion_metrics_df.loc[conversion_metrics_df['Metric'] == 'Standard Error', ['Control Group', 'Test Group', 'Value']] = ['', '', conv_standard_error]\n",
        "conversion_metrics_df.loc[conversion_metrics_df['Metric'] == 'Z-Value', ['Control Group', 'Test Group', 'Value']] = ['', '', conv_z_value]\n",
        "\n",
        "print(conversion_metrics_df)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Update with Stats calcs to convert to Scipy\n",
        "\n",
        "# Calculate delta\n",
        "# Calculate standard error\n",
        "# Calculate z value\n",
        "# Calculate p-value\n",
        "# Calculate type I error\n",
        "# Calculate confidence level\n",
        "# Calculate statistical significance\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming control and test groups are already defined as DataFrames\n",
        "\n",
        "# Assume 'avg_demand' and 'std_demand' are columns in the DataFrame\n",
        "\n",
        "# Perform t-test for Avg Demand\n",
        "t_stat, p_value_demand = stats.ttest_ind(test['avg_demand'], control['avg_demand'], equal_var=False)\n",
        "confidence_level_demand = 1 - p_value_demand\n",
        "type_i_error_demand = p_value_demand\n",
        "\n",
        "# Perform t-test for Conversion Rate (Replace 'conversion_rate' with the actual column names in your DataFrame)\n",
        "t_stat_conv, p_value_conv = stats.ttest_ind(test['conversion_rate'], control['conversion_rate'], equal_var=False)\n",
        "confidence_level_conv = 1 - p_value_conv\n",
        "type_i_error_conv = p_value_conv\n",
        "\n",
        "# Calculate lift for conversion\n",
        "lift_conversion_rate = ((test['conversion_rate'].mean() - control['conversion_rate'].mean()) / control['conversion_rate'].mean()) * 100\n",
        "\n",
        "# The above examples show how to use SciPy for p-value and confidence level calculations.\n",
        "# For delta, standard error, and z-value, these are part of the t-test calculation, and you typically wouldn't calculate these separately when using a t-test.\n",
        "# If you need to understand these, they're more relevant in the context of manual calculations or when not using a t-test directly.\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SQL Only\n",
        "\n",
        "*   checked, this code runs.\n",
        "\n"
      ],
      "metadata": {
        "id": "53TAliPCBv06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        " WITH\n",
        " aa_purchase AS (SELECT dol.upm_id\n",
        "    ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "    ,COUNT(DISTINCT order_nbr) AS orders\n",
        "    ,SUM(origl_ordered_qty) AS units\n",
        "  FROM view_dol dol\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,31) AND DATE_SUB(first_send_dt,1)\n",
        "  GROUP BY 1\n",
        "\n",
        "  -- AA period purchase. dynamically based on when the member receives their first communication.\n",
        ")\n",
        "\n",
        ", demand_bucket AS (\n",
        "  SELECT tct.upm_id\n",
        "  , tct.test_control\n",
        "  , aa.upm_id as upm_id_aa_buyer\n",
        "  , aa.demand\n",
        "  , aa.orders\n",
        "  , aa.units\n",
        "  , CASE\n",
        "    WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "    WHEN aa.demand < 50 THEN '000-050'\n",
        "    WHEN aa.demand < 100 THEN '050-100'\n",
        "    WHEN aa.demand < 200 THEN '100-200'\n",
        "    WHEN aa.demand < 500 THEN '200-500'\n",
        "    WHEN aa.demand < 1000 THEN '500-1000'\n",
        "    WHEN aa.demand >=1000 THEN '1000+'\n",
        "    END AS demand_per_buyer_bucket\n",
        "  , PERCENT_RANK() OVER (\n",
        "    PARTITION BY test_control\n",
        "    , CASE\n",
        "      WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "      WHEN aa.demand < 50 THEN '000-050'\n",
        "      WHEN aa.demand < 100 THEN '050-100'\n",
        "      WHEN aa.demand < 200 THEN '100-200'\n",
        "      WHEN aa.demand < 500 THEN '200-500'\n",
        "      WHEN aa.demand < 1000 THEN '500-1000'\n",
        "      WHEN aa.demand >=1000 THEN '1000+'\n",
        "      END ORDER BY RANDOM(0)\n",
        "    ) AS percent_rank\n",
        "  FROM test_control_table tct\n",
        "  LEFT JOIN aa_purchase aa ON tct.upm_id = aa.upm_id\n",
        ")\n",
        "\n",
        ", balanced_aud_aa as (\n",
        "  SELECT DISTINCT upm_id\n",
        "  FROM demand_bucket db\n",
        "  -- remove outliers\n",
        "  WHERE (demand < 2400 OR demand IS NULL)\n",
        "-- To rebalance, paste text block from template in place of defaults\n",
        "AND ((test_control = 'control' and (demand > 0 and demand <= 50) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand = 0 or demand is null) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand >0 and demand <= 50) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand = 0 or demand is null) and PERCENT_RANK <= 1)\n",
        "\t)\n",
        ")\n",
        "\n",
        ", mhub as (\n",
        " SELECT source_id as upm_id\n",
        " , member_id\n",
        " , preferred_gender\n",
        " FROM MEMBER.MEMBER_HUB\n",
        " WHERE ctry_2_cd = 'US'\n",
        " AND lower(dpa_status) IN ('activate','reactivate')\n",
        ")\n",
        "\n",
        "-- calculate a dynamic member activity window for each communication received by a member\n",
        "-- activity is calculated on a 7 day window, unless the member receives a communication within that timeframe\n",
        "-- When multiple messages are received, activity is only calculated up to the next message_send_dt to avoid double counting\n",
        ", member_activities as (\n",
        "  SELECT mgd.member_id\n",
        "  ,SUM(logged_in_visits_count) AS visits\n",
        "  ,SUM(pdp_favorite_count) AS pdp_favorite_count\n",
        "  ,SUM(add_to_cart_count) AS add_to_cart_count\n",
        "  ,SUM(physical_activity_count) AS physical_activity\n",
        "  ,MAX(CASE WHEN new_user_experience_flag = 'Y' THEN 1 ELSE 0 END) AS new_user_flag\n",
        "  FROM test_control_table tct\n",
        "  INNER JOIN mhub mh ON tct.upm_id = mh.upm_id\n",
        "  LEFT JOIN aud_select_workspace.member_agg_member_growth_daily mgd ON mh.member_id = mgd.member_id\n",
        "  WHERE mgd.activity_dt >= tct.campaign_send_dt\n",
        "    AND mgd.activity_dt < DATE_ADD(campaign_send_dt, INT(LEAST(7,COALESCE(difference_next_send_days,7))))\n",
        "    AND mgd.preferred_retail_geo = 'NA'\n",
        "    AND mgd.experience_name != 'agnostic'\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        "-- for each user, aggregate each order placed within our measurement window. create labels to categorize the type of demand\n",
        ", ab_purchase as (\n",
        "  SELECT dol.upm_id as upm_id_ab_buyer\n",
        "  , order_dt as order_ts\n",
        "  , order_nbr\n",
        "  ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc NOT IN ('Base Inline', 'Clearance', 'Launch') THEN grd_amt_excl_tax_usd ELSE 0 END) AS rest_demand\n",
        "  ,COUNT(DISTINCT order_nbr) AS orders\n",
        "  ,SUM(origl_ordered_qty) AS units\n",
        "  FROM view_dol dol\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN first_send_dt AND DATE_ADD(first_send_dt,7)\n",
        "  GROUP BY 1,2,3\n",
        ")\n",
        "\n",
        "-- For each member, rank each order by how near it was to the last campaign send\n",
        "-- purchases must be after the campaign send and within 7 days of the send to be attributed\n",
        ", aud_order_join as (\n",
        "SELECT a.*\n",
        ", b.*\n",
        ",(bigint(to_timestamp(b.order_ts))-bigint(to_timestamp(a.campaign_send_ts)))/60 as order_send_diff_minutes\n",
        ",RANK() OVER(PARTITION BY a.upm_id, b.order_ts ORDER BY (bigint(to_timestamp(b.order_ts))-bigint(to_timestamp(a.campaign_send_ts)))/60 ASC) as nearest_order\n",
        "FROM test_control_table a\n",
        "LEFT JOIN ab_purchase b ON (a.upm_id = b.upm_id_ab_buyer AND b.order_ts >= a.campaign_send_ts AND b.order_ts <= DATE_ADD(a.campaign_send_ts,7))\n",
        "ORDER BY a.upm_id,a.campaign_send_dt, nearest_order asc\n",
        ")\n",
        "\n",
        "-- Aggregate each distinct send, attributing only the nearest order.\n",
        "-- This prevent double counting of orders when a members receives multiple sends within the measurement window.\n",
        ", aud_order_agg as (\n",
        "SELECT upm_id AS upm_id_ab_buyer\n",
        "  ,SUM(demand) AS demand\n",
        "  ,SUM(base_FW_demand) AS base_FW_demand\n",
        "  ,SUM(base_AP_demand) AS base_AP_demand\n",
        "  ,SUM(base_EQ_demand) AS base_EQ_demand\n",
        "  ,SUM(clr_FW_demand) AS clr_FW_demand\n",
        "  ,SUM(clr_AP_demand) AS clr_AP_demand\n",
        "  ,SUM(clr_EQ_demand) AS clr_EQ_demand\n",
        "  ,SUM(launch_FW_demand) AS launch_FW_demand\n",
        "  ,SUM(launch_AP_demand) AS launch_AP_demand\n",
        "  ,SUM(launch_EQ_demand) AS launch_EQ_demand\n",
        "  ,SUM(rest_demand) AS rest_demand\n",
        "  ,SUM(orders) AS orders\n",
        "  ,SUM(units) AS units\n",
        "FROM aud_order_join a\n",
        "WHERE nearest_order = 1\n",
        "AND demand IS NOT NULL -- additional filter for extraneous records\n",
        "GROUP BY 1\n",
        "ORDER BY 1\n",
        ")\n",
        "\n",
        ", historical_purchase AS (\n",
        "  SELECT aoa.upm_id_ab_buyer as upm_id\n",
        "  , COUNT(DISTINCT order_nbr) as histOrders\n",
        "  FROM aud_order_agg aoa  -- eligible audience\n",
        "  INNER JOIN view_dol dol on aoa.upm_id_ab_buyer = dol.upm_id\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,730) AND DATE_SUB(first_send_dt,1)\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        "-- due to how messages are set up on Bluecore, the campaign_cd is not unique to a particular day. We concat with the message_sent_ts\n",
        "-- only using the date ignores cases where multiple sends occur on the same day, potentially undercounting\n",
        ",email_sent AS (\n",
        "  SELECT fed.upm_id\n",
        "  , COUNT(DISTINCT concat(campaign_cd, LEFT(message_sent_ts,21))) as email_circ\n",
        "  FROM COMMS.FACT_EMAIL_DELIVERY fed\n",
        "  WHERE (campaign_cd LIKE ${hivevar:camp_cd_tst_str} OR campaign_cd LIKE ${hivevar:camp_cd_ctl_str})\n",
        "  AND CAST(message_sent_ts AS DATE) between ${hivevar:msmt_start_dt} and ${hivevar:msmt_end_dt}\n",
        "  AND retail_geo_name = 'NA'\n",
        "  AND fed.upm_id IS NOT NULL\n",
        "  GROUP BY upm_id\n",
        ")\n",
        "\n",
        "-- returns all the instances of the campaign codes for each member along with the response types and revenue if applicable\n",
        ", email_response as (\n",
        "  SELECT DISTINCT fer.upm_id\n",
        "  , LEFT(response_ts,10) AS response_ts\n",
        "  , response_type\n",
        "  , revenue_dollar\n",
        "  FROM comms.fact_email_response fer\n",
        "  INNER JOIN test_control_table tct ON fer.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        "  WHERE (campaign_cd LIKE ${hivevar:camp_cd_tst_str} OR campaign_cd LIKE ${hivevar:camp_cd_ctl_str})\n",
        "  AND CAST(LEFT(response_ts,10) AS DATE) between first_send_dt and DATE_ADD(last_send_dt,7)\n",
        "  AND retail_geo_name = 'NA'\n",
        "  AND fer.upm_id IS NOT NULL\n",
        " )\n",
        "\n",
        "-- calculates the the total number of opens, clicks, purchases and last touch attributed demand for each member for the campaign\n",
        "-- responses are only considered if they occur between when the message was sent and 7 days later.\n",
        ", email_calc AS (\n",
        "  SELECT fer.upm_id\n",
        "  , COUNT(CASE WHEN response_type= 'email open' THEN fer.upm_id ELSE NULL END) AS email_opens\n",
        "  , COUNT(CASE WHEN response_type='link click' THEN fer.upm_id ELSE NULL END) AS email_clicks\n",
        "  , COUNT(CASE WHEN response_type='ecommerce-purchase' THEN fer.upm_id ELSE NULL END) AS email_purch\n",
        "  , SUM(CASE WHEN response_type='ecommerce-purchase' THEN revenue_dollar ELSE 0 END) AS email_demand\n",
        "  FROM email_response fer\n",
        "  INNER JOIN test_control_table tct ON fer.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        "  WHERE CAST(fer.response_ts as DATE) BETWEEN first_send_dt AND DATE_ADD(last_send_dt,7)\n",
        "  GROUP BY 1\n",
        ")\n",
        "-- total number of total messages send for each member during the campaign.\n",
        "-- For a singular campaign, this should be one (1), for triggers it could be more depending on how the trigger is set up\n",
        ", email_delivery_agg as (\n",
        "  SELECT upm_id\n",
        "  ,SUM(email_circ) AS email_circ\n",
        "  FROM email_sent\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", email_response_agg as (\n",
        "  SELECT upm_id\n",
        "  ,SUM(email_opens) AS email_opens\n",
        "  ,SUM(email_clicks) AS email_clicks\n",
        "  ,SUM(email_purch) AS email_purch\n",
        "  ,SUM(email_demand) AS email_demand\n",
        "  FROM email_calc\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", platform_usage as (\n",
        "  SELECT tct.upm_id\n",
        "  , CASE WHEN CAST(first_${hivevar:usage_platform}_activity_dt AS DATE) BETWEEN first_send_dt AND DATE_ADD(last_send_dt,7) THEN 1\n",
        "         ELSE 0\n",
        "    END AS platform_user\n",
        "  FROM aud_select_workspace.gck_first_and_last_touchpoint flt\n",
        "  INNER JOIN test_control_table tct ON flt.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        ")\n",
        "\n",
        "---------------------\n",
        "-- MAIN QUERY\n",
        "----------------------\n",
        "SELECT z.upm_id\n",
        "  , z.test_control\n",
        "\n",
        "  -- primary metrics ON incremental revenue: conversion rate, average demand per purchaser\n",
        "\n",
        "  ,SUM(demand)\n",
        "\n",
        "  -- secondary metrics ON order double click\n",
        "\n",
        "  ,SUM(orders)\n",
        "  ,SUM(units)\n",
        "\n",
        "  -- secondary metrics ON emails\n",
        "\n",
        "  , SUM(email_circ) AS email_sent\n",
        "  , SUM(email_opens)\n",
        "  , SUM(email_clicks)\n",
        "  , SUM(email_purch)\n",
        "  , SUM(email_demand)\n",
        "\n",
        "  -- secondary metrics ON site/app engagement\n",
        "\n",
        "  , SUM(visits) AS site_app_visits\n",
        "  , SUM(PDP_FAVORITE_COUNT)\n",
        "  , SUM(ADD_TO_CART_COUNT)\n",
        "  , SUM(physical_activity)\n",
        "\n",
        "  -- secondary metrics ON margin\n",
        "\n",
        "  , SUM(base_FW_demand)\n",
        "  , SUM(base_AP_demand)\n",
        "  , SUM(base_EQ_demand)\n",
        "  , SUM(clr_FW_demand)\n",
        "  , SUM(clr_AP_demand)\n",
        "  , SUM(clr_EQ_demand)\n",
        "  , SUM(launch_FW_demand)\n",
        "  , SUM(launch_AP_demand)\n",
        "  , SUM(launch_EQ_demand)\n",
        "  , SUM(rest_demand)\n",
        "\n",
        "  -- optional first usage kpi\n",
        "\n",
        "  , SUM(platform_user) AS platform_user\n",
        "\n",
        "  -- secondary metrics ON order history\n",
        "\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END) AS 1_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END) AS 2_priorOrder\n",
        "\n",
        "\n",
        "FROM test_control_table z\n",
        "INNER JOIN balanced_aud_aa b ON z.upm_id = b.upm_id  --inner join our balanced audience to remove members that were distorting the AA period balance\n",
        "\n",
        "INNER JOIN mhub mh ON z.upm_id = mh.upm_id\n",
        "LEFT JOIN member_activities a ON mh.member_id = a.member_id\n",
        "\n",
        "LEFT JOIN aud_order_agg abp ON z.upm_id = abp.upm_id_ab_buyer\n",
        "LEFT JOIN historical_purchase hp ON z.upm_id = hp.upm_id\n",
        "\n",
        "LEFT JOIN email_delivery_agg cda ON z.upm_id = cda.upm_id\n",
        "LEFT JOIN email_response_agg cra ON z.upm_id = cra.upm_id\n",
        "\n",
        "LEFT JOIN platform_usage plt ON z.upm_id = plt.upm_id\n",
        "\n",
        "GROUP BY 1,2\n",
        "ORDER BY 1 DESC;"
      ],
      "metadata": {
        "id": "VGWWfc9byyOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "-- metrics to be aggregated in numpy later:\n",
        "\n",
        "  ,COUNT(DISTINCT abp.upm_id_ab_buyer) AS buying_members\n",
        "\n",
        "  -- calculate margin\n",
        "\n",
        "  ,(AVG(base_FW_demand) * 0.38 + AVG(base_AP_demand) * 0.28 + AVG(base_EQ_demand) * 0.32\n",
        "    + AVG(clr_FW_demand) * 0.23 + AVG(base_AP_demand) * 0.17 + AVG(clr_EQ_demand) * 0.12\n",
        "    + AVG(launch_FW_demand) * 0.25 + AVG(launch_AP_demand) * 0.08 + AVG(launch_EQ_demand) * 0.19\n",
        "    + AVG(rest_demand) * 0.3\n",
        "  ) AS avg_margin\n",
        "\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END)/COUNT(DISTINCT z.upm_id) AS pctBuyers_1_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN demand ELSE 0 END)/SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END) AS DPB_1_prior\n",
        "\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END)/COUNT(DISTINCT z.upm_id) AS pctBuyers_2_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN demand ELSE 0 END)/SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END) AS DPB_2_prior"
      ],
      "metadata": {
        "id": "d19hhhv8RDri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KMqkMSobRrI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Integrating Updated SQL into new dataframe"
      ],
      "metadata": {
        "id": "LjraBHUAUUdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%python\n",
        "\n",
        "# Execute SQL Query\n",
        "ab_data = \"\"\"\n",
        "\n",
        " WITH\n",
        " aa_purchase AS (SELECT dol.upm_id\n",
        "    ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "    ,COUNT(DISTINCT order_nbr) AS orders\n",
        "    ,SUM(origl_ordered_qty) AS units\n",
        "  FROM view_dol dol\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,31) AND DATE_SUB(first_send_dt,1)\n",
        "  GROUP BY 1\n",
        "\n",
        "  -- AA period purchase. dynamically based on when the member receives their first communication.\n",
        ")\n",
        "\n",
        ", demand_bucket AS (\n",
        "  SELECT tct.upm_id\n",
        "  , tct.test_control\n",
        "  , aa.upm_id as upm_id_aa_buyer\n",
        "  , aa.demand\n",
        "  , aa.orders\n",
        "  , aa.units\n",
        "  , CASE\n",
        "    WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "    WHEN aa.demand < 50 THEN '000-050'\n",
        "    WHEN aa.demand < 100 THEN '050-100'\n",
        "    WHEN aa.demand < 200 THEN '100-200'\n",
        "    WHEN aa.demand < 500 THEN '200-500'\n",
        "    WHEN aa.demand < 1000 THEN '500-1000'\n",
        "    WHEN aa.demand >=1000 THEN '1000+'\n",
        "    END AS demand_per_buyer_bucket\n",
        "  , PERCENT_RANK() OVER (\n",
        "    PARTITION BY test_control\n",
        "    , CASE\n",
        "      WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "      WHEN aa.demand < 50 THEN '000-050'\n",
        "      WHEN aa.demand < 100 THEN '050-100'\n",
        "      WHEN aa.demand < 200 THEN '100-200'\n",
        "      WHEN aa.demand < 500 THEN '200-500'\n",
        "      WHEN aa.demand < 1000 THEN '500-1000'\n",
        "      WHEN aa.demand >=1000 THEN '1000+'\n",
        "      END ORDER BY RANDOM(0)\n",
        "    ) AS percent_rank\n",
        "  FROM test_control_table tct\n",
        "  LEFT JOIN aa_purchase aa ON tct.upm_id = aa.upm_id\n",
        ")\n",
        "\n",
        ", balanced_aud_aa as (\n",
        "  SELECT DISTINCT upm_id\n",
        "  FROM demand_bucket db\n",
        "  -- remove outliers\n",
        "  WHERE (demand < 2400 OR demand IS NULL)\n",
        "-- To rebalance, paste text block from template in place of defaults\n",
        "AND ((test_control = 'control' and (demand > 0 and demand <= 50) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand = 0 or demand is null) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand >0 and demand <= 50) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand = 0 or demand is null) and PERCENT_RANK <= 1)\n",
        "\t)\n",
        ")\n",
        "\n",
        ", mhub as (\n",
        " SELECT source_id as upm_id\n",
        " , member_id\n",
        " , preferred_gender\n",
        " FROM MEMBER.MEMBER_HUB\n",
        " WHERE ctry_2_cd = 'US'\n",
        " AND lower(dpa_status) IN ('activate','reactivate')\n",
        ")\n",
        "\n",
        "-- calculate a dynamic member activity window for each communication received by a member\n",
        "-- activity is calculated on a 7 day window, unless the member receives a communication within that timeframe\n",
        "-- When multiple messages are received, activity is only calculated up to the next message_send_dt to avoid double counting\n",
        ", member_activities as (\n",
        "  SELECT mgd.member_id\n",
        "  ,SUM(logged_in_visits_count) AS visits\n",
        "  ,SUM(pdp_favorite_count) AS pdp_favorite_count\n",
        "  ,SUM(add_to_cart_count) AS add_to_cart_count\n",
        "  ,SUM(physical_activity_count) AS physical_activity\n",
        "  ,MAX(CASE WHEN new_user_experience_flag = 'Y' THEN 1 ELSE 0 END) AS new_user_flag\n",
        "  FROM test_control_table tct\n",
        "  INNER JOIN mhub mh ON tct.upm_id = mh.upm_id\n",
        "  LEFT JOIN aud_select_workspace.member_agg_member_growth_daily mgd ON mh.member_id = mgd.member_id\n",
        "  WHERE mgd.activity_dt >= tct.campaign_send_dt\n",
        "    AND mgd.activity_dt < DATE_ADD(campaign_send_dt, INT(LEAST(7,COALESCE(difference_next_send_days,7))))\n",
        "    AND mgd.preferred_retail_geo = 'NA'\n",
        "    AND mgd.experience_name != 'agnostic'\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        "-- for each user, aggregate each order placed within our measurement window. create labels to categorize the type of demand\n",
        ", ab_purchase as (\n",
        "  SELECT dol.upm_id as upm_id_ab_buyer\n",
        "  , order_dt as order_ts\n",
        "  , order_nbr\n",
        "  ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc NOT IN ('Base Inline', 'Clearance', 'Launch') THEN grd_amt_excl_tax_usd ELSE 0 END) AS rest_demand\n",
        "  ,COUNT(DISTINCT order_nbr) AS orders\n",
        "  ,SUM(origl_ordered_qty) AS units\n",
        "  FROM view_dol dol\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN first_send_dt AND DATE_ADD(first_send_dt,7)\n",
        "  GROUP BY 1,2,3\n",
        ")\n",
        "\n",
        "-- For each member, rank each order by how near it was to the last campaign send\n",
        "-- purchases must be after the campaign send and within 7 days of the send to be attributed\n",
        ", aud_order_join as (\n",
        "SELECT a.*\n",
        ", b.*\n",
        ",(bigint(to_timestamp(b.order_ts))-bigint(to_timestamp(a.campaign_send_ts)))/60 as order_send_diff_minutes\n",
        ",RANK() OVER(PARTITION BY a.upm_id, b.order_ts ORDER BY (bigint(to_timestamp(b.order_ts))-bigint(to_timestamp(a.campaign_send_ts)))/60 ASC) as nearest_order\n",
        "FROM test_control_table a\n",
        "LEFT JOIN ab_purchase b ON (a.upm_id = b.upm_id_ab_buyer AND b.order_ts >= a.campaign_send_ts AND b.order_ts <= DATE_ADD(a.campaign_send_ts,7))\n",
        "ORDER BY a.upm_id,a.campaign_send_dt, nearest_order asc\n",
        ")\n",
        "\n",
        "-- Aggregate each distinct send, attributing only the nearest order.\n",
        "-- This prevent double counting of orders when a members receives multiple sends within the measurement window.\n",
        ", aud_order_agg as (\n",
        "SELECT upm_id AS upm_id_ab_buyer\n",
        "  ,SUM(demand) AS demand\n",
        "  ,SUM(base_FW_demand) AS base_FW_demand\n",
        "  ,SUM(base_AP_demand) AS base_AP_demand\n",
        "  ,SUM(base_EQ_demand) AS base_EQ_demand\n",
        "  ,SUM(clr_FW_demand) AS clr_FW_demand\n",
        "  ,SUM(clr_AP_demand) AS clr_AP_demand\n",
        "  ,SUM(clr_EQ_demand) AS clr_EQ_demand\n",
        "  ,SUM(launch_FW_demand) AS launch_FW_demand\n",
        "  ,SUM(launch_AP_demand) AS launch_AP_demand\n",
        "  ,SUM(launch_EQ_demand) AS launch_EQ_demand\n",
        "  ,SUM(rest_demand) AS rest_demand\n",
        "  ,SUM(orders) AS orders\n",
        "  ,SUM(units) AS units\n",
        "FROM aud_order_join a\n",
        "WHERE nearest_order = 1\n",
        "AND demand IS NOT NULL -- additional filter for extraneous records\n",
        "GROUP BY 1\n",
        "ORDER BY 1\n",
        ")\n",
        "\n",
        ", historical_purchase AS (\n",
        "  SELECT aoa.upm_id_ab_buyer as upm_id\n",
        "  , COUNT(DISTINCT order_nbr) as histOrders\n",
        "  FROM aud_order_agg aoa  -- eligible audience\n",
        "  INNER JOIN view_dol dol on aoa.upm_id_ab_buyer = dol.upm_id\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,730) AND DATE_SUB(first_send_dt,1)\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        "-- due to how messages are set up on Bluecore, the campaign_cd is not unique to a particular day. We concat with the message_sent_ts\n",
        "-- only using the date ignores cases where multiple sends occur on the same day, potentially undercounting\n",
        ",email_sent AS (\n",
        "  SELECT fed.upm_id\n",
        "  , COUNT(DISTINCT concat(campaign_cd, LEFT(message_sent_ts,21))) as email_circ\n",
        "  FROM COMMS.FACT_EMAIL_DELIVERY fed\n",
        "  WHERE (campaign_cd LIKE ${hivevar:camp_cd_tst_str} OR campaign_cd LIKE ${hivevar:camp_cd_ctl_str})\n",
        "  AND CAST(message_sent_ts AS DATE) between ${hivevar:msmt_start_dt} and ${hivevar:msmt_end_dt}\n",
        "  AND retail_geo_name = 'NA'\n",
        "  AND fed.upm_id IS NOT NULL\n",
        "  GROUP BY upm_id\n",
        ")\n",
        "\n",
        "-- returns all the instances of the campaign codes for each member along with the response types and revenue if applicable\n",
        ", email_response as (\n",
        "  SELECT DISTINCT fer.upm_id\n",
        "  , LEFT(response_ts,10) AS response_ts\n",
        "  , response_type\n",
        "  , revenue_dollar\n",
        "  FROM comms.fact_email_response fer\n",
        "  INNER JOIN test_control_table tct ON fer.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        "  WHERE (campaign_cd LIKE ${hivevar:camp_cd_tst_str} OR campaign_cd LIKE ${hivevar:camp_cd_ctl_str})\n",
        "  AND CAST(LEFT(response_ts,10) AS DATE) between first_send_dt and DATE_ADD(last_send_dt,7)\n",
        "  AND retail_geo_name = 'NA'\n",
        "  AND fer.upm_id IS NOT NULL\n",
        " )\n",
        "\n",
        "-- calculates the the total number of opens, clicks, purchases and last touch attributed demand for each member for the campaign\n",
        "-- responses are only considered if they occur between when the message was sent and 7 days later.\n",
        ", email_calc AS (\n",
        "  SELECT fer.upm_id\n",
        "  , COUNT(CASE WHEN response_type= 'email open' THEN fer.upm_id ELSE NULL END) AS email_opens\n",
        "  , COUNT(CASE WHEN response_type='link click' THEN fer.upm_id ELSE NULL END) AS email_clicks\n",
        "  , COUNT(CASE WHEN response_type='ecommerce-purchase' THEN fer.upm_id ELSE NULL END) AS email_purch\n",
        "  , SUM(CASE WHEN response_type='ecommerce-purchase' THEN revenue_dollar ELSE 0 END) AS email_demand\n",
        "  FROM email_response fer\n",
        "  INNER JOIN test_control_table tct ON fer.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        "  WHERE CAST(fer.response_ts as DATE) BETWEEN first_send_dt AND DATE_ADD(last_send_dt,7)\n",
        "  GROUP BY 1\n",
        ")\n",
        "-- total number of total messages send for each member during the campaign.\n",
        "-- For a singular campaign, this should be one (1), for triggers it could be more depending on how the trigger is set up\n",
        ", email_delivery_agg as (\n",
        "  SELECT upm_id\n",
        "  ,SUM(email_circ) AS email_circ\n",
        "  FROM email_sent\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", email_response_agg as (\n",
        "  SELECT upm_id\n",
        "  ,SUM(email_opens) AS email_opens\n",
        "  ,SUM(email_clicks) AS email_clicks\n",
        "  ,SUM(email_purch) AS email_purch\n",
        "  ,SUM(email_demand) AS email_demand\n",
        "  FROM email_calc\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", platform_usage as (\n",
        "  SELECT tct.upm_id\n",
        "  , CASE WHEN CAST(first_${hivevar:usage_platform}_activity_dt AS DATE) BETWEEN first_send_dt AND DATE_ADD(last_send_dt,7) THEN 1\n",
        "         ELSE 0\n",
        "    END AS platform_user\n",
        "  FROM aud_select_workspace.gck_first_and_last_touchpoint flt\n",
        "  INNER JOIN test_control_table tct ON flt.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        ")\n",
        "\n",
        "---------------------\n",
        "-- MAIN QUERY\n",
        "----------------------\n",
        "SELECT z.upm_id\n",
        "  , z.test_control\n",
        "\n",
        "  -- primary metrics ON incremental revenue: conversion rate, average demand per purchaser\n",
        "  ,SUM(demand) AS demand\n",
        "\n",
        "  -- secondary metrics ON order double click\n",
        "  ,SUM(orders) AS orders\n",
        "  ,SUM(units) AS units\n",
        "\n",
        "  -- secondary metrics ON emails\n",
        "  , SUM(email_circ) AS email_sent\n",
        "  , SUM(email_opens) AS email_opens\n",
        "  , SUM(email_clicks) AS email_clicks\n",
        "  , SUM(email_purch) AS email_purch\n",
        "  , SUM(email_demand) AS email_demand\n",
        "\n",
        "  -- secondary metrics ON site/app engagement\n",
        "  , SUM(visits) AS site_app_visits\n",
        "  , SUM(PDP_FAVORITE_COUNT) AS PDP_FAVORITE_COUNT\n",
        "  , SUM(ADD_TO_CART_COUNT) AS ADD_TO_CART_COUNT\n",
        "  , SUM(physical_activity) AS physical_activity\n",
        "\n",
        "  -- secondary metrics ON margin\n",
        "  , SUM(base_FW_demand) AS base_FW_demand\n",
        "  , SUM(base_AP_demand) AS base_AP_demand\n",
        "  , SUM(base_EQ_demand) AS base_EQ_demand\n",
        "  , SUM(clr_FW_demand) AS clr_FW_demand\n",
        "  , SUM(clr_AP_demand) AS clr_AP_demand\n",
        "  , SUM(clr_EQ_demand) AS clr_EQ_demand\n",
        "  , SUM(launch_FW_demand) AS launch_FW_demand\n",
        "  , SUM(launch_AP_demand) AS launch_AP_demand\n",
        "  , SUM(launch_EQ_demand) AS launch_EQ_demand\n",
        "  , SUM(rest_demand) AS rest_demand\n",
        "\n",
        "   -- optional first usage kpi\n",
        "  , SUM(platform_user) AS platform_user\n",
        "\n",
        "    -- secondary metrics ON order history\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END) AS 1_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END) AS 2_priorOrder\n",
        "\n",
        "\n",
        "FROM test_control_table z\n",
        "INNER JOIN balanced_aud_aa b ON z.upm_id = b.upm_id  --inner join our balanced audience to remove members that were distorting the AA period balance\n",
        "\n",
        "INNER JOIN mhub mh ON z.upm_id = mh.upm_id\n",
        "LEFT JOIN member_activities a ON mh.member_id = a.member_id\n",
        "\n",
        "LEFT JOIN aud_order_agg abp ON z.upm_id = abp.upm_id_ab_buyer\n",
        "LEFT JOIN historical_purchase hp ON z.upm_id = hp.upm_id\n",
        "\n",
        "LEFT JOIN email_delivery_agg cda ON z.upm_id = cda.upm_id\n",
        "LEFT JOIN email_response_agg cra ON z.upm_id = cra.upm_id\n",
        "\n",
        "LEFT JOIN platform_usage plt ON z.upm_id = plt.upm_id\n",
        "\n",
        "GROUP BY 1,2\n",
        "ORDER BY 1 DESC;\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Load the data into a PySpark DataFrame\n",
        "df_spark = spark.sql(ab_data)\n",
        "\n",
        "# Convert to Pandas DataFrame for further analysis\n",
        "df = df_spark.toPandas()\n",
        "\n",
        "# Separate control and test group data\n",
        "control = df[df['test_control'] == 'control']\n",
        "test = df[df['test_control'] == 'test']\n",
        "\n",
        "\n",
        "## Extracting input parameters for Demand\n",
        "\n",
        "\n",
        "# Calculate average sales (demand) for control and test groups\n",
        "control_avg_demand = control['demand'].mean()\n",
        "test_avg_demand = test['demand'].mean()\n",
        "\n",
        "# Calculate standard deviation of demand for control and test groups\n",
        "control_std_dev = control['demand'].std()\n",
        "test_std_dev = test['demand'].std()\n",
        "\n",
        "# Calculate the # of buying members for the control and test groups\n",
        "control_sample_size = control[control['orders'] > 0]['upm_id'].nunique()\n",
        "test_sample_size = test[test['orders'] > 0]['upm_id'].nunique()\n",
        "\n",
        "\n",
        "\n",
        "## Perform Statistical Analysis for Demand\n",
        "\n",
        "\n",
        "# Calculate delta and z-value\n",
        "    \"\"\"\n",
        "    The delta, standard error, and z-value, are part of the t-test calculation.\n",
        "\n",
        "    You typically wouldn't calculate these separately when using a t-test.\n",
        "\n",
        "    If you need to see these, they're more relevant in the context of manual calculations or when not using a t-test directly.\n",
        "    \"\"\"\n",
        "\n",
        "# Calculate standard error\n",
        "    \"\"\"\n",
        "    A note about standard error:\n",
        "\n",
        "    * When using stats.ttest_ind with equal_var=False, the test internally computes a version of standard error adjusted for the Welch's t-test.\n",
        "    * Welch's t-test doesn't assume equal population variances.\n",
        "    * For educational purposes, you can still calculate a simple standard error of the mean (SEM) for each group.\n",
        "    * Or calculate the standard error of the difference between two means if you're comparing them directly.\n",
        "    * Code to do so follows below...\n",
        "\n",
        "    # Calculate the standard error of the mean for each group\n",
        "    sem_control = control['avg_demand'].sem()\n",
        "    sem_test = test['avg_demand'].sem()\n",
        "\n",
        "    # Calculate the standard error for the difference between two means\n",
        "    # Again, this formula is for illustrative purposes; the t-test already accounts for this in its calculation\n",
        "    n_control = len(control['avg_demand'])\n",
        "    n_test = len(test['avg_demand'])\n",
        "    std_error_diff = np.sqrt(sem_control**2 / n_control + sem_test**2 / n_test)\n",
        "\n",
        "    print(f\"Standard Error (Control): {sem_control}\")\n",
        "    print(f\"Standard Error (Test): {sem_test}\")\n",
        "    print(f\"Standard Error of Difference: {std_error_diff}\")\n",
        "    \"\"\"\n",
        "\n",
        "# Perform t-test for Avg Demand\n",
        "t_stat, p_value_demand = stats.ttest_ind(test['demand'], control['demand'], equal_var=False)\n",
        "confidence_level_demand = 1 - p_value_demand\n",
        "type_i_error_demand = p_value_demand\n",
        "\n",
        "print(f\"T-statistic for Demand: {t_stat}\")\n",
        "print(f\"P-value for Demand: {p_value_demand}\")\n",
        "print(f\"Confidence Level for Demand: {confidence_level_demand*100:.2f}%\")\n",
        "print(f\"Type I Error for Demand: {type_i_error_demand:.4f}\")\n",
        "\n",
        "\n",
        "## Extracting input parameters for Conversion\n",
        "\n",
        "\n",
        "# Total count of all unique members in the test and control groups\n",
        "conv_control_sample_size = control['upm_id'].nunique()\n",
        "conv_test_sample_size = test['upm_id'].nunique()\n",
        "\n",
        "# Calculate conversion rate for the test and control groups (total buying members / total members)\n",
        "control_conv_rate = control_sample_size / conv_control_sample_size if conv_control_sample_size > 0 else 0\n",
        "test_conv_rate = test_sample_size / conv_test_sample_size if conv_test_sample_size > 0 else 0\n",
        "\n",
        "# Calculate lift for the conversion rate\n",
        "lift = abs(test_conv_rate - control_conv_rate)\n",
        "\n",
        "# Calculate difference of proportions\n",
        "difference_of_proportions = test_conv_rate - control_conv_rate\n",
        "\n",
        "print(f\"Control Conversion Rate: {control_conv_rate*100:.2f}%\")\n",
        "print(f\"Test Conversion Rate: {test_conv_rate*100:.2f}%\")\n",
        "print(f\"Absolute Lift in Conversion Rate: {lift*100:.2f}%\")\n",
        "print(f\"Difference_of_proportions: {difference_of_proportions:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "## Perform Statistical Analysis for Conversion\n",
        "\n",
        "# Add a 'buyer' column to both test and control DataFrames\n",
        "# 'buyer' = 1 if 'orders' > 0, else 0\n",
        "control['buyer'] = (control['orders'] > 0).astype(int)\n",
        "test['buyer'] = (test['orders'] > 0).astype(int)\n",
        "\n",
        "# Perform t-test for Conversion Rate\n",
        "t_stat_conversion, p_value_conversion = stats.ttest_ind(test['buyer'], control['buyer'], equal_var=False)\n",
        "\n",
        "# Calculate confidence level and Type I error for Conversion Rate\n",
        "confidence_level_conversion = 1 - p_value_conversion\n",
        "type_i_error_conversion = p_value_conversion\n",
        "\n",
        "print(f\"T-statistic for Conversion: {t_stat_conversion}\")\n",
        "print(f\"P-value for Conversion: {p_value_conversion}\")\n",
        "print(f\"Confidence Level for Conversion: {confidence_level_conversion*100:.2f}%\")\n",
        "print(f\"Type I Error for Conversion: {type_i_error_conversion:.4f}\")\n",
        "\n",
        "\n",
        "## Populate the scorecard DataFrame\n",
        "\n",
        "scorecard_df = pd.DataFrame({\n",
        "    'Metric': ['Avg Demand', 'Conversion Rate', 'Lift - Conversion Rate'],\n",
        "    'P-Value': [p_value_demand, p_value_conv, '-'],\n",
        "    'Confidence Level': [f\"{confidence_level_demand:.2f}\", f\"{confidence_level_conv:.2f}\", '-'],\n",
        "    'Lift': ['-', '-', f\"{lift_conversion_rate:.2f}%\"]\n",
        "})\n",
        "\n",
        "scorecard_df\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Demand Metrics DataFrame\n",
        "data_demand = {\n",
        "    'Metric': ['Avg Demand', 'Std Deviation of Demand', 'Sample Size',\n",
        "               'Absolute Difference Between Means (Delta)', 'Standard Error', 'Z-Value'],\n",
        "    'Control Group': [control_avg_sales, control_std_dev, control_sample_size, '', '', ''],\n",
        "    'Test Group': [test_avg_sales, test_std_dev, test_sample_size, '', '', ''],\n",
        "    'Value': ['', '', '', delta, standard_error, z_value]\n",
        "}\n",
        "\n",
        "demand_metrics_df = pd.DataFrame(data_demand)\n",
        "\n",
        "# Update rows for Delta, Standard Error, and Z-Value, as they are not group-specific\n",
        "demand_metrics_df.loc[demand_metrics_df['Metric'] == 'Absolute Difference Between Means (Delta)', ['Control Group', 'Test Group', 'Value']] = ['', '', delta]\n",
        "demand_metrics_df.loc[demand_metrics_df['Metric'] == 'Standard Error', ['Control Group', 'Test Group', 'Value']] = ['', '', standard_error]\n",
        "demand_metrics_df.loc[demand_metrics_df['Metric'] == 'Z-Value', ['Control Group', 'Test Group', 'Value']] = ['', '', z_value]\n",
        "\n",
        "# Demand Significance DataFrame\n",
        "data_demand_significance = {\n",
        "    'Significance Metric': ['Achieved Confidence Level', 'Type I Error (α)'],\n",
        "    'Value': [confidence_level, type_i_error]\n",
        "}\n",
        "\n",
        "demand_significance_df = pd.DataFrame(data_demand_significance)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Demand Metrics DataFrame\n",
        "data_demand = {\n",
        "    'Metric': ['Avg Demand', 'Std Deviation of Demand', 'Sample Size',\n",
        "               'Absolute Difference Between Means (Delta)', 'Standard Error', 'Z-Value','Achieved Confidence Level', 'Type I Error (α)'],\n",
        "    'Control Group': [control_avg_sales, control_std_dev, control_sample_size, '', '', '', '', ''],\n",
        "    'Test Group': [test_avg_sales, test_std_dev, test_sample_size, '', '', '','', ''],\n",
        "    'Value': ['', '', '', delta, standard_error, z_value, confidence_level, type_i_error]\n",
        "}\n",
        "\n",
        "demand_metrics_df = pd.DataFrame(data_demand)\n",
        "\n",
        "# Update rows for Delta, Standard Error, and Z-Value, as they are in dataframe calcuations\n",
        "demand_metrics_df.loc[demand_metrics_df['Metric'] == 'Absolute Difference Between Means (Delta)', ['Control Group', 'Test Group', 'Value']] = ['', '', delta,]\n",
        "demand_metrics_df.loc[demand_metrics_df['Metric'] == 'Standard Error', ['Control Group', 'Test Group', 'Value']] = ['', '', standard_error]\n",
        "demand_metrics_df.loc[demand_metrics_df['Metric'] == 'Z-Value', ['Control Group', 'Test Group', 'Value']] = ['', '', z_value]\n",
        "\n",
        "print(demand_metrics_df)\n",
        "\n",
        "\n",
        "# Conversion Metrics DataFrame\n",
        "data_conversion = {\n",
        "    'Metric': ['Conversion Rate', 'Lift (Absolute % Point Difference)', 'Difference of Proportions',\n",
        "               'Standard Error', 'Z-Value', 'Achieved Confidence Level', 'Type I Error (α)'],\n",
        "    'Control Group': [control_conv_rate, '', '', '', '', '', ''],\n",
        "    'Test Group': [test_conv_rate, '', '', '', '','', ''],\n",
        "    'Value': ['', lift, difference_of_proportions, conv_standard_error, conv_z_value, conv_confidence_level, conv_type_i_error]\n",
        "}\n",
        "\n",
        "conversion_metrics_df = pd.DataFrame(data_conversion)\n",
        "\n",
        "# Update rows for Delta, Standard Error, and Z-Value, as they are in dataframe calcuations\n",
        "conversion_metrics_df.loc[conversion_metrics_df['Metric'] == 'Lift (Absolute % Point Difference)', ['Control Group', 'Test Group', 'Value']] = ['', '', lift]\n",
        "conversion_metrics_df.loc[conversion_metrics_df['Metric'] == 'Difference of Proportions', ['Control Group', 'Test Group', 'Value']] = ['', '', difference_of_proportions]\n",
        "conversion_metrics_df.loc[conversion_metrics_df['Metric'] == 'Standard Error', ['Control Group', 'Test Group', 'Value']] = ['', '', conv_standard_error]\n",
        "conversion_metrics_df.loc[conversion_metrics_df['Metric'] == 'Z-Value', ['Control Group', 'Test Group', 'Value']] = ['', '', conv_z_value]\n",
        "\n",
        "print(conversion_metrics_df)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "aQACZ5elUYel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Metrics\n",
        "\n",
        "Add these to dataframe later"
      ],
      "metadata": {
        "id": "O7wcbJxTtkO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Calculate secondary metrics ON order double click\n",
        "\n",
        "  ,SUM(demand) / SUM(orders) AS AOV\n",
        "  ,SUM(demand) / SUM(units) AS AUR\n",
        "  ,SUM(units) / SUM(orders) AS UPT\n",
        "\n",
        "# secondary metrics ON emails\n",
        "\n",
        " ,SUM(email_opens) / SUM(email_circ) AS email_open_rate\n",
        "  ,SUM(email_clicks) / SUM(email_circ) AS email_click_rate --against sends\n",
        "  --,SUM(email_clicks) / SUM(email_opens) AS email_click_rate --against clicks\n",
        "  ,SUM(email_purch) / SUM(email_circ) AS email_conversion_rate\n",
        "\n",
        "# secondary metrics ON margin\n",
        "\n",
        "  ,(AVG(base_FW_demand) * 0.38 + AVG(base_AP_demand) * 0.28 + AVG(base_EQ_demand) * 0.32\n",
        "    + AVG(clr_FW_demand) * 0.23 + AVG(base_AP_demand) * 0.17 + AVG(clr_EQ_demand) * 0.12\n",
        "    + AVG(launch_FW_demand) * 0.25 + AVG(launch_AP_demand) * 0.08 + AVG(launch_EQ_demand) * 0.19\n",
        "    + AVG(rest_demand) * 0.3\n",
        "  ) AS avg_margin\n",
        "\n",
        "# secondary metrics ON order history\n",
        "\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END)/COUNT(DISTINCT z.upm_id) AS pctBuyers_1_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN demand ELSE 0 END)/SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END) AS DPB_1_prior\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END)/COUNT(DISTINCT z.upm_id) AS pctBuyers_2_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN demand ELSE 0 END)/SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END) AS DPB_2_prior"
      ],
      "metadata": {
        "id": "srJnIes6sTY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%python\n",
        "\n",
        "# Execute SQL Query\n",
        "ab_data = \"\"\"\n",
        "\n",
        " WITH\n",
        " aa_purchase AS (SELECT dol.upm_id\n",
        "    ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "    ,COUNT(DISTINCT order_nbr) AS orders\n",
        "    ,SUM(origl_ordered_qty) AS units\n",
        "  FROM view_dol dol\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,31) AND DATE_SUB(first_send_dt,1)\n",
        "  GROUP BY 1\n",
        "\n",
        "  -- AA period purchase. dynamically based on when the member receives their first communication.\n",
        ")\n",
        "\n",
        ", demand_bucket AS (\n",
        "  SELECT tct.upm_id\n",
        "  , tct.test_control\n",
        "  , aa.upm_id as upm_id_aa_buyer\n",
        "  , aa.demand\n",
        "  , aa.orders\n",
        "  , aa.units\n",
        "  , CASE\n",
        "    WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "    WHEN aa.demand < 50 THEN '000-050'\n",
        "    WHEN aa.demand < 100 THEN '050-100'\n",
        "    WHEN aa.demand < 200 THEN '100-200'\n",
        "    WHEN aa.demand < 500 THEN '200-500'\n",
        "    WHEN aa.demand < 1000 THEN '500-1000'\n",
        "    WHEN aa.demand >=1000 THEN '1000+'\n",
        "    END AS demand_per_buyer_bucket\n",
        "  , PERCENT_RANK() OVER (\n",
        "    PARTITION BY test_control\n",
        "    , CASE\n",
        "      WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "      WHEN aa.demand < 50 THEN '000-050'\n",
        "      WHEN aa.demand < 100 THEN '050-100'\n",
        "      WHEN aa.demand < 200 THEN '100-200'\n",
        "      WHEN aa.demand < 500 THEN '200-500'\n",
        "      WHEN aa.demand < 1000 THEN '500-1000'\n",
        "      WHEN aa.demand >=1000 THEN '1000+'\n",
        "      END ORDER BY RANDOM(0)\n",
        "    ) AS percent_rank\n",
        "  FROM test_control_table tct\n",
        "  LEFT JOIN aa_purchase aa ON tct.upm_id = aa.upm_id\n",
        ")\n",
        "\n",
        ", balanced_aud_aa as (\n",
        "  SELECT DISTINCT upm_id\n",
        "  FROM demand_bucket db\n",
        "  -- remove outliers\n",
        "  WHERE (demand < 2400 OR demand IS NULL)\n",
        "-- To rebalance, paste text block from template in place of defaults\n",
        "AND ((test_control = 'control' and (demand > 0 and demand <= 50) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand = 0 or demand is null) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand >0 and demand <= 50) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand = 0 or demand is null) and PERCENT_RANK <= 1)\n",
        "\t)\n",
        ")\n",
        "\n",
        ", mhub as (\n",
        " SELECT source_id as upm_id\n",
        " , member_id\n",
        " , preferred_gender\n",
        " FROM MEMBER.MEMBER_HUB\n",
        " WHERE ctry_2_cd = 'US'\n",
        " AND lower(dpa_status) IN ('activate','reactivate')\n",
        ")\n",
        "\n",
        "/*\n",
        "-- calculate a dynamic member activity window for each communication received by a member\n",
        "-- activity is calculated on a 7 day window, unless the member receives a communication within that timeframe\n",
        "-- When multiple messages are received, activity is only calculated up to the next message_send_dt to avoid double counting\n",
        ", member_activities as (\n",
        "  SELECT mgd.member_id\n",
        "  ,SUM(logged_in_visits_count) AS visits\n",
        "  ,SUM(pdp_favorite_count) AS pdp_favorite_count\n",
        "  ,SUM(add_to_cart_count) AS add_to_cart_count\n",
        "  ,SUM(physical_activity_count) AS physical_activity\n",
        "  ,MAX(CASE WHEN new_user_experience_flag = 'Y' THEN 1 ELSE 0 END) AS new_user_flag\n",
        "  FROM test_control_table tct\n",
        "  INNER JOIN mhub mh ON tct.upm_id = mh.upm_id\n",
        "  LEFT JOIN aud_select_workspace.member_agg_member_growth_daily mgd ON mh.member_id = mgd.member_id\n",
        "  WHERE mgd.activity_dt >= tct.campaign_send_dt\n",
        "    AND mgd.activity_dt < DATE_ADD(campaign_send_dt, INT(LEAST(7,COALESCE(difference_next_send_days,7))))\n",
        "    AND mgd.preferred_retail_geo = 'NA'\n",
        "    AND mgd.experience_name != 'agnostic'\n",
        "  GROUP BY 1\n",
        ")\n",
        "*/\n",
        "\n",
        "-- for each user, aggregate each order placed within our measurement window. create labels to categorize the type of demand\n",
        ", ab_purchase as (\n",
        "  SELECT dol.upm_id as upm_id_ab_buyer\n",
        "  , order_dt as order_ts\n",
        "  , order_nbr\n",
        "  ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc NOT IN ('Base Inline', 'Clearance', 'Launch') THEN grd_amt_excl_tax_usd ELSE 0 END) AS rest_demand\n",
        "  ,COUNT(DISTINCT order_nbr) AS orders\n",
        "  ,SUM(origl_ordered_qty) AS units\n",
        "  FROM view_dol dol\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN first_send_dt AND DATE_ADD(first_send_dt,7)\n",
        "  GROUP BY 1,2,3\n",
        ")\n",
        "\n",
        "-- For each member, rank each order by how near it was to the last campaign send\n",
        "-- purchases must be after the campaign send and within 7 days of the send to be attributed\n",
        ", aud_order_join as (\n",
        "SELECT a.*\n",
        ", b.*\n",
        ",(bigint(to_timestamp(b.order_ts))-bigint(to_timestamp(a.campaign_send_ts)))/60 as order_send_diff_minutes\n",
        ",RANK() OVER(PARTITION BY a.upm_id, b.order_ts ORDER BY (bigint(to_timestamp(b.order_ts))-bigint(to_timestamp(a.campaign_send_ts)))/60 ASC) as nearest_order\n",
        "FROM test_control_table a\n",
        "LEFT JOIN ab_purchase b ON (a.upm_id = b.upm_id_ab_buyer AND b.order_ts >= a.campaign_send_ts AND b.order_ts <= DATE_ADD(a.campaign_send_ts,7))\n",
        "ORDER BY a.upm_id,a.campaign_send_dt, nearest_order asc\n",
        ")\n",
        "\n",
        "-- Aggregate each distinct send, attributing only the nearest order.\n",
        "-- This prevent double counting of orders when a members receives multiple sends within the measurement window.\n",
        ", aud_order_agg as (\n",
        "SELECT upm_id AS upm_id_ab_buyer\n",
        "  ,SUM(demand) AS demand\n",
        "  ,SUM(base_FW_demand) AS base_FW_demand\n",
        "  ,SUM(base_AP_demand) AS base_AP_demand\n",
        "  ,SUM(base_EQ_demand) AS base_EQ_demand\n",
        "  ,SUM(clr_FW_demand) AS clr_FW_demand\n",
        "  ,SUM(clr_AP_demand) AS clr_AP_demand\n",
        "  ,SUM(clr_EQ_demand) AS clr_EQ_demand\n",
        "  ,SUM(launch_FW_demand) AS launch_FW_demand\n",
        "  ,SUM(launch_AP_demand) AS launch_AP_demand\n",
        "  ,SUM(launch_EQ_demand) AS launch_EQ_demand\n",
        "  ,SUM(rest_demand) AS rest_demand\n",
        "  ,SUM(orders) AS orders\n",
        "  ,SUM(units) AS units\n",
        "FROM aud_order_join a\n",
        "WHERE nearest_order = 1\n",
        "AND demand IS NOT NULL -- additional filter for extraneous records\n",
        "GROUP BY 1\n",
        "ORDER BY 1\n",
        ")\n",
        "\n",
        ", historical_purchase AS (\n",
        "  SELECT aoa.upm_id_ab_buyer as upm_id\n",
        "  , COUNT(DISTINCT order_nbr) as histOrders\n",
        "  FROM aud_order_agg aoa  -- eligible audience\n",
        "  INNER JOIN view_dol dol on aoa.upm_id_ab_buyer = dol.upm_id\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,730) AND DATE_SUB(first_send_dt,1)\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        "-- due to how messages are set up on Bluecore, the campaign_cd is not unique to a particular day. We concat with the message_sent_ts\n",
        "-- only using the date ignores cases where multiple sends occur on the same day, potentially undercounting\n",
        ",email_sent AS (\n",
        "  SELECT fed.upm_id\n",
        "  , COUNT(DISTINCT concat(campaign_cd, LEFT(message_sent_ts,21))) as email_circ\n",
        "  FROM COMMS.FACT_EMAIL_DELIVERY fed\n",
        "  WHERE (campaign_cd LIKE ${hivevar:camp_cd_tst_str} OR campaign_cd LIKE ${hivevar:camp_cd_ctl_str})\n",
        "  AND CAST(message_sent_ts AS DATE) between ${hivevar:msmt_start_dt} and ${hivevar:msmt_end_dt}\n",
        "  AND retail_geo_name = 'NA'\n",
        "  AND fed.upm_id IS NOT NULL\n",
        "  GROUP BY upm_id\n",
        ")\n",
        "\n",
        "-- returns all the instances of the campaign codes for each member along with the response types and revenue if applicable\n",
        ", email_response as (\n",
        "  SELECT DISTINCT fer.upm_id\n",
        "  , LEFT(response_ts,10) AS response_ts\n",
        "  , response_type\n",
        "  , revenue_dollar\n",
        "  FROM comms.fact_email_response fer\n",
        "  INNER JOIN test_control_table tct ON fer.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        "  WHERE (campaign_cd LIKE ${hivevar:camp_cd_tst_str} OR campaign_cd LIKE ${hivevar:camp_cd_ctl_str})\n",
        "  AND CAST(LEFT(response_ts,10) AS DATE) between first_send_dt and DATE_ADD(last_send_dt,7)\n",
        "  AND retail_geo_name = 'NA'\n",
        "  AND fer.upm_id IS NOT NULL\n",
        " )\n",
        "\n",
        "-- calculates the the total number of opens, clicks, purchases and last touch attributed demand for each member for the campaign\n",
        "-- responses are only considered if they occur between when the message was sent and 7 days later.\n",
        ", email_calc AS (\n",
        "  SELECT fer.upm_id\n",
        "  , COUNT(CASE WHEN response_type= 'email open' THEN fer.upm_id ELSE NULL END) AS email_opens\n",
        "  , COUNT(CASE WHEN response_type='link click' THEN fer.upm_id ELSE NULL END) AS email_clicks\n",
        "  , COUNT(CASE WHEN response_type='ecommerce-purchase' THEN fer.upm_id ELSE NULL END) AS email_purch\n",
        "  , SUM(CASE WHEN response_type='ecommerce-purchase' THEN revenue_dollar ELSE 0 END) AS email_demand\n",
        "  FROM email_response fer\n",
        "  INNER JOIN test_control_table tct ON fer.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        "  WHERE CAST(fer.response_ts as DATE) BETWEEN first_send_dt AND DATE_ADD(last_send_dt,7)\n",
        "  GROUP BY 1\n",
        ")\n",
        "-- total number of total messages send for each member during the campaign.\n",
        "-- For a singular campaign, this should be one (1), for triggers it could be more depending on how the trigger is set up\n",
        ", email_delivery_agg as (\n",
        "  SELECT upm_id\n",
        "  ,SUM(email_circ) AS email_circ\n",
        "  FROM email_sent\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", email_response_agg as (\n",
        "  SELECT upm_id\n",
        "  ,SUM(email_opens) AS email_opens\n",
        "  ,SUM(email_clicks) AS email_clicks\n",
        "  ,SUM(email_purch) AS email_purch\n",
        "  ,SUM(email_demand) AS email_demand\n",
        "  FROM email_calc\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", platform_usage as (\n",
        "  SELECT tct.upm_id\n",
        "  , CASE WHEN CAST(first_${hivevar:usage_platform}_activity_dt AS DATE) BETWEEN first_send_dt AND DATE_ADD(last_send_dt,7) THEN 1\n",
        "         ELSE 0\n",
        "    END AS platform_user\n",
        "  FROM aud_select_workspace.gck_first_and_last_touchpoint flt\n",
        "  INNER JOIN test_control_table tct ON flt.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        ")\n",
        "\n",
        "---------------------\n",
        "-- MAIN QUERY\n",
        "----------------------\n",
        "SELECT z.upm_id\n",
        "  , z.test_control\n",
        "\n",
        "  -- primary metrics ON incremental revenue: conversion rate, average demand per purchaser\n",
        "  ,SUM(demand) AS demand\n",
        "\n",
        "  -- secondary metrics ON order double click\n",
        "  ,SUM(orders) AS orders\n",
        "  ,SUM(units) AS units\n",
        "\n",
        "  -- secondary metrics ON emails\n",
        "  , SUM(email_circ) AS email_sent\n",
        "  , SUM(email_opens) AS email_opens\n",
        "  , SUM(email_clicks) AS email_clicks\n",
        "  , SUM(email_purch) AS email_purch\n",
        "  , SUM(email_demand) AS email_demand\n",
        "\n",
        "  -- secondary metrics ON site/app engagement\n",
        " -- , SUM(visits) AS site_app_visits\n",
        "  , SUM(PDP_FAVORITE_COUNT) AS PDP_FAVORITE_COUNT\n",
        "  , SUM(ADD_TO_CART_COUNT) AS ADD_TO_CART_COUNT\n",
        "  , SUM(physical_activity) AS physical_activity\n",
        "\n",
        "  -- secondary metrics ON margin\n",
        "  , SUM(base_FW_demand) AS base_FW_demand\n",
        "  , SUM(base_AP_demand) AS base_AP_demand\n",
        "  , SUM(base_EQ_demand) AS base_EQ_demand\n",
        "  , SUM(clr_FW_demand) AS clr_FW_demand\n",
        "  , SUM(clr_AP_demand) AS clr_AP_demand\n",
        "  , SUM(clr_EQ_demand) AS clr_EQ_demand\n",
        "  , SUM(launch_FW_demand) AS launch_FW_demand\n",
        "  , SUM(launch_AP_demand) AS launch_AP_demand\n",
        "  , SUM(launch_EQ_demand) AS launch_EQ_demand\n",
        "  , SUM(rest_demand) AS rest_demand\n",
        "\n",
        "   -- optional first usage kpi\n",
        "  , SUM(platform_user) AS platform_user\n",
        "\n",
        "    -- secondary metrics ON order history\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END) AS 1_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END) AS 2_priorOrder\n",
        "\n",
        "\n",
        "FROM test_control_table z\n",
        "INNER JOIN balanced_aud_aa b ON z.upm_id = b.upm_id  --inner join our balanced audience to remove members that were distorting the AA period balance\n",
        "\n",
        "INNER JOIN mhub mh ON z.upm_id = mh.upm_id\n",
        "LEFT JOIN member_activities a ON mh.member_id = a.member_id\n",
        "\n",
        "LEFT JOIN aud_order_agg abp ON z.upm_id = abp.upm_id_ab_buyer\n",
        "LEFT JOIN historical_purchase hp ON z.upm_id = hp.upm_id\n",
        "\n",
        "LEFT JOIN email_delivery_agg cda ON z.upm_id = cda.upm_id\n",
        "LEFT JOIN email_response_agg cra ON z.upm_id = cra.upm_id\n",
        "\n",
        "LEFT JOIN platform_usage plt ON z.upm_id = plt.upm_id\n",
        "\n",
        "GROUP BY 1,2\n",
        "ORDER BY 1 DESC;\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Load the data into a PySpark DataFrame\n",
        "df_spark = spark.sql(ab_data)\n",
        "\n",
        "# Convert to Pandas DataFrame for further analysis\n",
        "df = df_spark.toPandas()\n",
        "\n",
        "# Separate control and test group data\n",
        "control = df[df['test_control'] == 'control']\n",
        "test = df[df['test_control'] == 'test']\n",
        "\n",
        "\n",
        "## Extracting input parameters for Demand\n",
        "\n",
        "\n",
        "# Calculate average sales (demand) for control and test groups\n",
        "# And, replace NaN values in 'demand' with 0\n",
        "\n",
        "control_avg_demand = control['demand'].fillna(0).mean()\n",
        "test_avg_demand = test['demand'].fillna(0).mean()\n",
        "# control_avg_demand = control['demand'].mean()\n",
        "# test_avg_demand = test['demand'].mean()\n",
        "\n",
        "# Calculate standard deviation of demand for control and test groups\n",
        "control_std_dev = control['demand'].fillna(0).std()\n",
        "test_std_dev = test['demand'].fillna(0).std()\n",
        "\n",
        "# Calculate the # of buying members for the control and test groups\n",
        "control_sample_size = control[control['orders'] > 0]['upm_id'].nunique()\n",
        "test_sample_size = test[test['orders'] > 0]['upm_id'].nunique()\n",
        "\n",
        "\n",
        "\n",
        "## Perform Statistical Analysis for Demand\n",
        "\n",
        "\n",
        "# Calculate delta and z-value\n",
        "\"\"\"\n",
        "The delta, standard error, and z-value, are part of the t-test calculation.\n",
        "\n",
        "You typically wouldn't calculate these separately when using a t-test.\n",
        "\n",
        "If you need to see these, they're more relevant in the context of manual calculations or when not using a t-test directly.\n",
        "\"\"\"\n",
        "\n",
        "# Calculate standard error\n",
        "\"\"\"\n",
        "A note about standard error:\n",
        "\n",
        "* When using stats.ttest_ind with equal_var=False, the test internally computes a version of standard error adjusted for the Welch's t-test.\n",
        "* Welch's t-test doesn't assume equal population variances.\n",
        "* For educational purposes, you can still calculate a simple standard error of the mean (SEM) for each group.\n",
        "* Or calculate the standard error of the difference between two means if you're comparing them directly.\n",
        "* Code to do so follows below...\n",
        "\n",
        "# Calculate the standard error of the mean for each group\n",
        "sem_control = control['avg_demand'].sem()\n",
        "sem_test = test['avg_demand'].sem()\n",
        "\n",
        "# Calculate the standard error for the difference between two means\n",
        "# Again, this formula is for illustrative purposes; the t-test already accounts for this in its calculation\n",
        "n_control = len(control['avg_demand'])\n",
        "n_test = len(test['avg_demand'])\n",
        "std_error_diff = np.sqrt(sem_control**2 / n_control + sem_test**2 / n_test)\n",
        "\n",
        "print(f\"Standard Error (Control): {sem_control}\")\n",
        "print(f\"Standard Error (Test): {sem_test}\")\n",
        "print(f\"Standard Error of Difference: {std_error_diff}\")\n",
        "\"\"\"\n",
        "\n",
        "# Perform t-test for Avg Demand\n",
        "t_stat, p_value_demand = stats.ttest_ind(test['demand'], control['demand'], equal_var=False)\n",
        "confidence_level_demand = 1 - p_value_demand\n",
        "type_i_error_demand = p_value_demand\n",
        "\n",
        "print(f\"T-statistic for Demand: {t_stat}\")\n",
        "print(f\"P-value for Demand: {p_value_demand}\")\n",
        "print(f\"Confidence Level for Demand: {confidence_level_demand*100:.2f}%\")\n",
        "print(f\"Type I Error for Demand: {type_i_error_demand:.4f}\")\n",
        "\n",
        "\n",
        "## Extracting input parameters for Conversion\n",
        "\n",
        "\n",
        "# Total count of all unique members in the test and control groups\n",
        "conv_control_sample_size = control['upm_id'].nunique()\n",
        "conv_test_sample_size = test['upm_id'].nunique()\n",
        "\n",
        "# Calculate conversion rate for the test and control groups (total buying members / total members)\n",
        "control_conv_rate = control_sample_size / conv_control_sample_size if conv_control_sample_size > 0 else 0\n",
        "test_conv_rate = test_sample_size / conv_test_sample_size if conv_test_sample_size > 0 else 0\n",
        "\n",
        "# Calculate lift for the conversion rate\n",
        "lift = abs(test_conv_rate - control_conv_rate)\n",
        "\n",
        "# Calculate difference of proportions\n",
        "difference_of_proportions = test_conv_rate - control_conv_rate\n",
        "\n",
        "print(f\"Control Conversion Rate: {control_conv_rate*100:.2f}%\")\n",
        "print(f\"Test Conversion Rate: {test_conv_rate*100:.2f}%\")\n",
        "print(f\"Absolute Lift in Conversion Rate: {lift*100:.2f}%\")\n",
        "print(f\"Difference_of_proportions: {difference_of_proportions:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "## Perform Statistical Analysis for Conversion\n",
        "\n",
        "# Add a 'buyer' column to both test and control DataFrames\n",
        "# 'buyer' = 1 if 'orders' > 0, else 0\n",
        "control['buyer'] = (control['orders'] > 0).astype(int)\n",
        "test['buyer'] = (test['orders'] > 0).astype(int)\n",
        "\n",
        "# Perform t-test for Conversion Rate\n",
        "t_stat_conversion, p_value_conversion = stats.ttest_ind(test['buyer'], control['buyer'], equal_var=False)\n",
        "\n",
        "# Calculate confidence level and Type I error for Conversion Rate\n",
        "confidence_level_conversion = 1 - p_value_conversion\n",
        "type_i_error_conversion = p_value_conversion\n",
        "\n",
        "print(f\"T-statistic for Conversion: {t_stat_conversion}\")\n",
        "print(f\"P-value for Conversion: {p_value_conversion}\")\n",
        "print(f\"Confidence Level for Conversion: {confidence_level_conversion*100:.2f}%\")\n",
        "print(f\"Type I Error for Conversion: {type_i_error_conversion:.4f}\")\n"
      ],
      "metadata": {
        "id": "07L9n9fI8_8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Updated calculations for ttest"
      ],
      "metadata": {
        "id": "d9hdWdvCKigF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# handling members with a demand of 0\n",
        "\n",
        "# Replace NaN values in 'demand' with 0\n",
        "control['demand'] = control['demand'].fillna(0)\n",
        "test['demand'] = test['demand'].fillna(0)\n",
        "\n",
        "# Perform the t-test\n",
        "t_stat, p_value_demand = stats.ttest_ind(test['demand'], control['demand'], equal_var=False)\n",
        "\n",
        "\n",
        "# handling members with a demand of 0, ignoring those values\n",
        "\n",
        "# Extracting input parameters for Demand with non-NaN values\n",
        "control_demand_non_nan = control[control['demand'].notna()]['demand']\n",
        "test_demand_non_nan = test[test['demand'].notna()]['demand']\n",
        "\n",
        "# Calculate average sales (demand) for control and test groups without NaN values\n",
        "control_avg_demand = control_demand_non_nan.mean()\n",
        "test_avg_demand = test_demand_non_nan.mean()\n",
        "\n",
        "control_avg_demand = control['demand'].notna().mean()\n",
        "test_avg_demand = test['demand'].notna().mean()\n",
        "\n",
        "\n",
        "# Calculate standard deviation of demand for control and test groups without NaN values\n",
        "control_std_dev = control['demand'].notna().std()\n",
        "test_std_dev = test['demand'].notna().std()\n",
        "\n",
        "# Perform t-test for Avg Demand excluding NaN values\n",
        "t_stat_demand, p_value_demand = stats.ttest_ind(test_demand_non_nan, control_demand_non_nan, equal_var=False, nan_policy='omit')\n",
        "\n"
      ],
      "metadata": {
        "id": "lCKMSmVgLS7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build the Scorecard dataframe"
      ],
      "metadata": {
        "id": "gZoOZT6b9n_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%python\n",
        "## Extracting input parameters for Demand\n",
        "\n",
        "\n",
        "# Calculate average sales (demand) for control and test groups\n",
        "# And, replace NaN values in 'demand' with 0\n",
        "\n",
        "# control_avg_demand = control['demand'].notna().mean()\n",
        "# test_avg_demand = test['demand'].notna().mean()\n",
        "control_avg_demand = control['demand'].mean()\n",
        "test_avg_demand = test['demand'].mean()\n",
        "\n",
        "# Calculate standard deviation of demand for control and test groups\n",
        "# control_std_dev = control['demand'].notna().std()\n",
        "# test_std_dev = test['demand'].notna().std()\n",
        "control_std_dev = control['demand'].std()\n",
        "test_std_dev = test['demand'].std()\n",
        "\n",
        "# Calculate the # of buying members for the control and test groups\n",
        "control_sample_size = control[control['orders'] > 0]['upm_id'].nunique()\n",
        "test_sample_size = test[test['orders'] > 0]['upm_id'].nunique()\n",
        "\n",
        "\n",
        "\n",
        "## Perform Statistical Analysis for Demand\n",
        "\n",
        "\n",
        "# Calculate delta and z-value\n",
        "\"\"\"\n",
        "The delta, standard error, and z-value, are part of the t-test calculation.\n",
        "\n",
        "You typically wouldn't calculate these separately when using a t-test.\n",
        "\n",
        "If you need to see these, they're more relevant in the context of manual calculations or when not using a t-test directly.\n",
        "\"\"\"\n",
        "\n",
        "# Calculate standard error\n",
        "\"\"\"\n",
        "A note about standard error:\n",
        "\n",
        "* When using stats.ttest_ind with equal_var=False, the test internally computes a version of standard error adjusted for the Welch's t-test.\n",
        "* Welch's t-test doesn't assume equal population variances.\n",
        "* For educational purposes, you can still calculate a simple standard error of the mean (SEM) for each group.\n",
        "* Or, calculate the standard error of the difference between two means if you're comparing them directly.\n",
        "* Code to do so follows below...\n",
        "\n",
        "# Calculate the standard error of the mean for each group\n",
        "sem_control = control['avg_demand'].sem()\n",
        "sem_test = test['avg_demand'].sem()\n",
        "\n",
        "# Calculate the standard error for the difference between two means\n",
        "# Again, this formula is for illustrative purposes; the t-test already accounts for this in its calculation\n",
        "n_control = len(control['avg_demand'])\n",
        "n_test = len(test['avg_demand'])\n",
        "std_error_diff = np.sqrt(sem_control**2 / n_control + sem_test**2 / n_test)\n",
        "\n",
        "print(f\"Standard Error (Control): {sem_control}\")\n",
        "print(f\"Standard Error (Test): {sem_test}\")\n",
        "print(f\"Standard Error of Difference: {std_error_diff}\")\n",
        "\"\"\"\n",
        "\n",
        "# Perform t-test for Avg Demand, omitting 'nan' values\n",
        "# t_stat, p_value_demand = stats.ttest_ind(test['demand'].fillna(0), control['demand'].fillna(0), equal_var=False)\n",
        "t_stat_demand, p_value_demand = stats.ttest_ind(test['demand'], control['demand'], equal_var=False, nan_policy='omit')\n",
        "confidence_level_demand = 1 - p_value_demand\n",
        "type_i_error_demand = p_value_demand\n",
        "\n",
        "\n",
        "print(f\"Demand Sample Size - Control: {control_sample_size}\")\n",
        "print(f\"Demand Sample Size - Test: {test_sample_size}\")\n",
        "print(f\"Average Demand - Control: {control_avg_demand:.2f}\")\n",
        "print(f\"Average Demand - Test: {test_avg_demand:.2f}\")\n",
        "print(f\"Standard Deviation - Test: {test_std_dev:.2f}\")\n",
        "print(f\"Standard Deviation - Control: {control_std_dev:.2f}\")\n",
        "\n",
        "print(f\"T-statistic for Demand: {t_stat_demand}\")\n",
        "print(f\"P-value for Demand: {p_value_demand}\")\n",
        "print(f\"Confidence Level for Demand: {confidence_level_demand*100:.2f}%\")\n",
        "print(f\"Type I Error for Demand: {type_i_error_demand:.4f}\")\n",
        "\n",
        "\n",
        "## Extracting input parameters for Conversion\n",
        "\n",
        "\n",
        "# Total count of all unique members in the test and control groups\n",
        "conv_control_sample_size = control['upm_id'].nunique()\n",
        "conv_test_sample_size = test['upm_id'].nunique()\n",
        "\n",
        "# Calculate conversion rate for the test and control groups (total buying members / total members)\n",
        "control_conv_rate = control_sample_size / conv_control_sample_size if conv_control_sample_size > 0 else 0\n",
        "test_conv_rate = test_sample_size / conv_test_sample_size if conv_test_sample_size > 0 else 0\n",
        "\n",
        "# Calculate lift for the conversion rate\n",
        "lift = abs(test_conv_rate - control_conv_rate)\n",
        "\n",
        "# Calculate difference of proportions\n",
        "difference_of_proportions = test_conv_rate - control_conv_rate\n",
        "\n",
        "print(f\"Control Conversion Rate: {control_conv_rate*100:.2f}%\")\n",
        "print(f\"Test Conversion Rate: {test_conv_rate*100:.2f}%\")\n",
        "print(f\"Absolute Lift in Conversion Rate: {lift*100:.4f}%\")\n",
        "print(f\"Difference_of_proportions: {difference_of_proportions:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "## Perform Statistical Analysis for Conversion\n",
        "\n",
        "# Add a 'buyer' column to both test and control DataFrames\n",
        "# Make sure these are not views but standalone DataFrames to avoid SettingWithCopyWarning\n",
        "control = control.copy()\n",
        "test = test.copy()\n",
        "# Now safely add the 'buyer' column\n",
        "control.loc[:, 'buyer'] = (control['orders'] > 0).astype(int) # 'buyer' = 1 if 'orders' > 0, else 0\n",
        "test.loc[:, 'buyer'] = (test['orders'] > 0).astype(int)\n",
        "\n",
        "# Perform t-test for Conversion Rate\n",
        "t_stat_conversion, p_value_conversion = stats.ttest_ind(test['buyer'], control['buyer'], equal_var=False)\n",
        "\n",
        "# Calculate confidence level and Type I error for Conversion Rate\n",
        "confidence_level_conversion = 1 - p_value_conversion\n",
        "type_i_error_conversion = p_value_conversion\n",
        "\n",
        "print(f\"T-statistic for Conversion: {t_stat_conversion:.4f}\")\n",
        "print(f\"P-value for Conversion: {p_value_conversion:.4f}\")\n",
        "print(f\"Confidence Level for Conversion: {confidence_level_conversion*100:.2f}%\")\n",
        "print(f\"Type I Error for Conversion: {type_i_error_conversion:.4f}\")"
      ],
      "metadata": {
        "id": "KpBbn6sC9ndT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming control and test are defined DataFrames\n",
        "# Make sure these are not views but standalone DataFrames to avoid SettingWithCopyWarning\n",
        "control = control.copy()\n",
        "test = test.copy()\n",
        "\n",
        "# Now safely add the 'buyer' column\n",
        "control.loc[:, 'buyer'] = (control['orders'] > 0).astype(int)\n",
        "test.loc[:, 'buyer'] = (test['orders'] > 0).astype(int)\n"
      ],
      "metadata": {
        "id": "RjiyhA-E_i-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AA OG Code"
      ],
      "metadata": {
        "id": "2mlXaKGtRQN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "--------------------------------------------------------------------------------------------------------------------------------------------\n",
        "-- AA Period\n",
        "--------------------------------------------------------------------------------------------------------------------------------------------\n",
        "WITH\n",
        "\n",
        "test_control_table AS (\n",
        "    SELECT aud.upm_id\n",
        "    , test_control\n",
        "    , MIN(message_sent_ts) as first_send\n",
        "    FROM (\n",
        "        (SELECT distinct fed.upm_id, 'test' as test_control, message_sent_ts from ${hivevar:aud_table_tst} tst INNER JOIN comms.fact_email_delivery fed ON tst.upm_id = fed.upm_id AND campaign_geo_name = 'NA' AND campaign_cd LIKE ${hivevar:camp_cd_tst_str} AND CAST(message_sent_ts AS DATE) BETWEEN ${hivevar:msmt_start_dt} AND ${hivevar:msmt_end_dt})\n",
        "      UNION\n",
        "        (SELECT distinct upm_id, 'control' as test_control, ${hivevar:msmt_start_dt} as first_send from ${hivevar:aud_table_ctl})\n",
        "    ) aud\n",
        "    ANTI JOIN gwan13.bot_master_nike_com bot ON aud.upm_id = bot.upm_id\n",
        "    ANTI JOIN aud_select_workspace.reseller_new slr ON aud.upm_id = slr.upm_user_id\n",
        "    GROUP BY 1,2\n",
        ")\n",
        "\n",
        "-- buying member level: purchase behavior\n",
        ", dol AS (\n",
        "  SELECT\n",
        "    dol.upm_id\n",
        "    ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "    ,COUNT(DISTINCT order_nbr) AS orders\n",
        "    ,SUM(origl_ordered_qty) AS units\n",
        "  FROM dtc_integrated.dtc_digital_order_line dol\n",
        "  INNER JOIN test_control_table tct on dol.upm_id = tct.upm_id\n",
        "  WHERE dol.region_key = 1\n",
        "    AND dol.rec_excl_ind = 0\n",
        "    AND dol.ttl_demand_ind = 1\n",
        "    AND UPPER(univ_cat_desc) <> 'CONVERSE'\n",
        "    AND dol.upm_id IS NOT NULL\n",
        "    AND CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send,31) AND DATE_SUB(first_send,1)\n",
        "    AND (dol.rtn_qty = 0 or dol.rtn_qty is NULL)\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        "-- Bucket each member into a demand group, then randomly rank them within that group\n",
        ", demand_bucket AS (\n",
        "  SELECT z.*\n",
        "  , d.upm_id as upm_id_buyer\n",
        "  , d.demand\n",
        "  , CASE WHEN d.demand = 0 OR d.demand IS NULL THEN 'non-buyer'\n",
        "         WHEN d.demand < 50 THEN '000-050'\n",
        "         WHEN d.demand < 100 THEN '050-100'\n",
        "         WHEN d.demand < 200 THEN '100-200'\n",
        "         WHEN d.demand < 500 THEN '200-500'\n",
        "         WHEN d.demand < 1000 THEN '500-1000'\n",
        "         WHEN d.demand >=1000 THEN '1000+'\n",
        "    END AS demand_per_buyer_bucket\n",
        "  , d.orders\n",
        "  , d.units\n",
        "  , PERCENT_RANK() OVER (PARTITION BY\n",
        "                      test_control\n",
        "                    , CASE\n",
        "                      WHEN d.demand = 0 OR d.demand is NULL THEN 'non-buyer'\n",
        "                      WHEN d.demand < 50 THEN '000-050'\n",
        "                      WHEN d.demand < 100 THEN '050-100'\n",
        "                      WHEN d.demand < 200 THEN '100-200'\n",
        "                      WHEN d.demand < 500 THEN '200-500'\n",
        "                      WHEN d.demand < 1000 THEN '500-1000'\n",
        "                      WHEN d.demand >=1000 THEN '1000+'\n",
        "                      END ORDER BY RAND(0)) AS percent_rank\n",
        "  FROM test_control_table z\n",
        "  LEFT JOIN dol d ON z.upm_id = d.upm_id\n",
        ")\n",
        "\n",
        "-----------------\n",
        "-- MAIN QUERY\n",
        "-----------------\n",
        "\n",
        "SELECT\n",
        "  test_control\n",
        "  -- primary metrics on incremental revenue: conversion rate, average demand per purchaser\n",
        "  ,COUNT(DISTINCT upm_id) AS total_members\n",
        "  ,COUNT(DISTINCT upm_id_buyer) AS buying_members\n",
        "  ,COUNT(DISTINCT upm_id_buyer) / COUNT(DISTINCT upm_id) AS conversion_rate\n",
        "  ,AVG(demand) AS avg_demand\n",
        "  ,STDDEV(demand) AS std_demand\n",
        "\n",
        "  -- secondary metrics ON order double click\n",
        "  ,SUM(demand) / SUM(orders) AS AOV\n",
        "  ,SUM(demand) / SUM(units) AS AUR\n",
        "  ,SUM(units) / SUM(orders) AS UPT\n",
        "\n",
        "\n",
        "FROM demand_bucket db\n",
        "WHERE (demand < 2400 OR demand IS NULL)  -- remove outliers\n",
        "-- To rebalance, paste text block from template in place of defaults\n",
        "AND ((test_control = 'control' and (demand > 0 and demand <= 50) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand = 0 or demand is null) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand >0 and demand <= 50) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand = 0 or demand is null) and PERCENT_RANK <= 1)\n",
        "\t)\n",
        "\n",
        "GROUP BY 1\n",
        "ORDER BY 1 DESC;"
      ],
      "metadata": {
        "id": "Gnn-tsyMRSEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AB OG Code"
      ],
      "metadata": {
        "id": "HBPSDaUORTTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "-----------------------------------------------------------------------------------------------------------------------------------\n",
        "-- AB Period\n",
        "-----------------------------------------------------------------------------------------------------------------------------------\n",
        "WITH\n",
        "\n",
        "tc_camp_sends AS (\n",
        "    SELECT upm_id\n",
        "    , CASE WHEN final_exposure_or_holdout like ${hivevar:node_tst_str} THEN 'test'\n",
        "           WHEN final_exposure_or_holdout like ${hivevar:node_ctl_str} THEN 'control'\n",
        "      ELSE 'na' END AS test_control\n",
        "    , MIN(LEFT(timestamp,10)) AS campaign_send_dt\n",
        "    , MIN(CAST(LEFT(timestamp,19) AS timestamp)) AS campaign_send_ts\n",
        "    FROM ${hivevar:aud_table} aud\n",
        "    ANTI JOIN gwan13.bot_master_nike_com bot ON aud.upm_id = bot.upm_id\n",
        "    ANTI JOIN aud_select_workspace.resellers slr ON aud.upm_id = slr.upm_id\n",
        "    WHERE timestamp BETWEEN ${hivevar:msmt_start_dt} AND ${hivevar:msmt_end_dt}\n",
        "    GROUP BY 1,2\n",
        ")\n",
        "\n",
        ", tc_first_last_send AS (\n",
        "  SELECT upm_id\n",
        "  , MIN(campaign_send_dt) as first_send_dt\n",
        "  , MAX(campaign_send_dt) as last_send_dt\n",
        "  FROM tc_camp_sends\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", test_control_table AS (\n",
        "    SELECT aud.*\n",
        "    , fls.first_send_dt\n",
        "    , fls.last_send_dt\n",
        "    , LEAD(campaign_send_dt) OVER (PARTITION BY aud.upm_id ORDER BY campaign_send_dt) AS next_send_dt\n",
        "    , LAG(campaign_send_dt) OVER (PARTITION BY aud.upm_id ORDER BY campaign_send_dt) AS previous_send_dt\n",
        "    , (int(to_timestamp(campaign_send_dt)) - int(to_timestamp(LEAD(campaign_send_dt) OVER (PARTITION BY aud.upm_id ORDER BY campaign_send_dt))))/86400 AS difference_next_send_days\n",
        "    , (int(to_timestamp(campaign_send_dt)) - int(to_timestamp(LAG(campaign_send_dt) OVER (PARTITION BY aud.upm_id ORDER BY campaign_send_dt))))/86400 AS difference_previous_send_days\n",
        "    FROM tc_camp_sends aud\n",
        "    INNER JOIN tc_first_last_send fls ON aud.upm_id = fls.upm_id\n",
        "    ANTI JOIN gwan13.bot_master_nike_com bot ON aud.upm_id = bot.upm_id\n",
        "    ANTI JOIN aud_select_workspace.resellers slr ON aud.upm_id = slr.upm_id\n",
        "    AND test_control != 'na'\n",
        ")\n",
        "\n",
        ", view_dol AS (\n",
        "  SELECT dol.upm_id\n",
        "  , order_nbr\n",
        "  , order_dt\n",
        "  , origl_ordered_qty\n",
        "  , grd_amt_excl_tax_usd\n",
        "  , line_of_business_desc\n",
        "  , univ_div_desc\n",
        "  , first_send_dt\n",
        "  , last_send_dt\n",
        "  FROM DTC_INTEGRATED.DTC_DIGITAL_ORDER_LINE dol\n",
        "  INNER JOIN test_control_table tct ON dol.upm_id = tct.upm_id\n",
        "  WHERE region_key = 1\n",
        "    AND rec_excl_ind = 0\n",
        "    AND ttl_demand_ind = 1\n",
        "    AND univ_cat_desc <> 'Converse'\n",
        "    AND dol.upm_id IS NOT NULL\n",
        "    AND CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,730) AND DATE_ADD(last_send_dt,7)\n",
        "    AND (rtn_qty = 0 or rtn_qty IS NULL)\n",
        ")\n",
        "\n",
        "-- AA period purchase. dynamically based on when the member receives their first communication. Only returns demand for members in the\n",
        ", aa_purchase AS (\n",
        "  SELECT dol.upm_id\n",
        "    ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "    ,COUNT(DISTINCT order_nbr) AS orders\n",
        "    ,SUM(origl_ordered_qty) AS units\n",
        "  FROM view_dol dol\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,31) AND DATE_SUB(first_send_dt,1)\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", demand_bucket AS (\n",
        "  SELECT tct.upm_id\n",
        "  , tct.test_control\n",
        "  , aa.upm_id as upm_id_aa_buyer\n",
        "  , aa.demand\n",
        "  , aa.orders\n",
        "  , aa.units\n",
        "  , CASE\n",
        "    WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "    WHEN aa.demand < 50 THEN '000-050'\n",
        "    WHEN aa.demand < 100 THEN '050-100'\n",
        "    WHEN aa.demand < 200 THEN '100-200'\n",
        "    WHEN aa.demand < 500 THEN '200-500'\n",
        "    WHEN aa.demand < 1000 THEN '500-1000'\n",
        "    WHEN aa.demand >=1000 THEN '1000+'\n",
        "    END AS demand_per_buyer_bucket\n",
        "  , PERCENT_RANK() OVER (\n",
        "    PARTITION BY test_control\n",
        "    , CASE\n",
        "      WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "      WHEN aa.demand < 50 THEN '000-050'\n",
        "      WHEN aa.demand < 100 THEN '050-100'\n",
        "      WHEN aa.demand < 200 THEN '100-200'\n",
        "      WHEN aa.demand < 500 THEN '200-500'\n",
        "      WHEN aa.demand < 1000 THEN '500-1000'\n",
        "      WHEN aa.demand >=1000 THEN '1000+'\n",
        "      END ORDER BY RANDOM(0)\n",
        "    ) AS percent_rank\n",
        "  FROM test_control_table tct\n",
        "  LEFT JOIN aa_purchase aa ON tct.upm_id = aa.upm_id\n",
        ")\n",
        "\n",
        ", balanced_aud_aa as (\n",
        "  SELECT DISTINCT upm_id\n",
        "  FROM demand_bucket db\n",
        "  -- remove outliers\n",
        "  WHERE (demand < 2400 OR demand IS NULL)\n",
        "-- To rebalance, paste text block from template in place of defaults\n",
        "AND ((test_control = 'control' and (demand > 0 and demand <= 50) and PERCENT_RANK <= 0.98994)\n",
        "OR (test_control = 'control' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 0.99803)\n",
        "OR (test_control = 'control' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 0.99741)\n",
        "OR (test_control = 'control' and (demand > 1000) and PERCENT_RANK <= 0.99965)\n",
        "OR (test_control = 'control' and (demand = 0 or demand is null) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand >0 and demand <= 50) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 0.99982)\n",
        "OR (test_control = 'test' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 0.99515)\n",
        "OR (test_control = 'test' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand = 0 or demand is null) and PERCENT_RANK <= 0.98176))\n",
        "\n",
        ", mhub as (\n",
        " SELECT source_id as upm_id\n",
        " , member_id\n",
        " , preferred_gender\n",
        " FROM MEMBER.MEMBER_HUB\n",
        " WHERE ctry_2_cd = 'US'\n",
        " AND lower(dpa_status) IN ('activate','reactivate')\n",
        ")\n",
        "\n",
        "-- calculate a dynamic member activity window for each communication received by a member\n",
        "-- activity is calculated on a 7 day window, unless the member receives a communication within that timeframe\n",
        "-- When multiple messages are received, activity is only calculated up to the next message_send_dt to avoid double counting\n",
        ", member_activities as (\n",
        "  SELECT mgd.member_id\n",
        "  ,SUM(logged_in_visits_count) AS visits\n",
        "  ,SUM(pdp_favorite_count) AS pdp_favorite_count\n",
        "  ,SUM(add_to_cart_count) AS add_to_cart_count\n",
        "  ,SUM(physical_activity_count) AS physical_activity\n",
        "  ,MAX(CASE WHEN new_user_experience_flag = 'Y' THEN 1 ELSE 0 END) AS new_user_flag\n",
        "  ,SUM(eod_chat_count) AS NEOD_Session\n",
        "  FROM test_control_table tct\n",
        "  INNER JOIN mhub mh ON tct.upm_id = mh.upm_id\n",
        "  LEFT JOIN aud_select_workspace.member_agg_member_growth_daily mgd ON mh.member_id = mgd.member_id\n",
        "  WHERE mgd.activity_dt >= tct.campaign_send_dt\n",
        "    AND mgd.activity_dt < DATE_ADD(campaign_send_dt, INT(LEAST(7,COALESCE(difference_next_send_days,7))))\n",
        "    AND mgd.preferred_retail_geo = 'NA'\n",
        "    AND mgd.experience_name != 'agnostic'\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        "-- for each user, aggregate each order placed within our measurement window. create labels to categorize the type of demand\n",
        ", ab_purchase as (\n",
        "  SELECT dol.upm_id as upm_id_ab_buyer\n",
        "  , order_dt as order_ts\n",
        "  , order_nbr\n",
        "  ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc NOT IN ('Base Inline', 'Clearance', 'Launch') THEN grd_amt_excl_tax_usd ELSE 0 END) AS rest_demand\n",
        "  ,COUNT(DISTINCT order_nbr) AS orders\n",
        "  ,SUM(origl_ordered_qty) AS units\n",
        "  FROM view_dol dol\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN first_send_dt AND DATE_ADD(first_send_dt,7)\n",
        "  GROUP BY 1,2,3\n",
        ")\n",
        "\n",
        "-- For each member, rank each order by how near it was to the last campaign send\n",
        "-- purchases must be after the campaign send and within 7 days of the send to be attributed\n",
        ", aud_order_join as (\n",
        "SELECT a.*\n",
        ", b.*\n",
        ",(bigint(to_timestamp(b.order_ts))-bigint(to_timestamp(a.campaign_send_ts)))/60 as order_send_diff_minutes\n",
        ",RANK() OVER(PARTITION BY a.upm_id, b.order_ts ORDER BY (bigint(to_timestamp(b.order_ts))-bigint(to_timestamp(a.campaign_send_ts)))/60 ASC) as nearest_order\n",
        "FROM test_control_table a\n",
        "LEFT JOIN ab_purchase b ON (a.upm_id = b.upm_id_ab_buyer AND b.order_ts >= a.campaign_send_ts AND b.order_ts <= DATE_ADD(a.campaign_send_ts,7))\n",
        "ORDER BY a.upm_id,a.campaign_send_dt, nearest_order asc\n",
        ")\n",
        "\n",
        "-- Aggregate each distinct send, attributing only the nearest order.\n",
        "-- This prevent double counting of orders when a members receives multiple sends within the measurement window.\n",
        ", aud_order_agg as (\n",
        "SELECT upm_id AS upm_id_ab_buyer\n",
        "  ,SUM(demand) AS demand\n",
        "  ,SUM(base_FW_demand) AS base_FW_demand\n",
        "  ,SUM(base_AP_demand) AS base_AP_demand\n",
        "  ,SUM(base_EQ_demand) AS base_EQ_demand\n",
        "  ,SUM(clr_FW_demand) AS clr_FW_demand\n",
        "  ,SUM(clr_AP_demand) AS clr_AP_demand\n",
        "  ,SUM(clr_EQ_demand) AS clr_EQ_demand\n",
        "  ,SUM(launch_FW_demand) AS launch_FW_demand\n",
        "  ,SUM(launch_AP_demand) AS launch_AP_demand\n",
        "  ,SUM(launch_EQ_demand) AS launch_EQ_demand\n",
        "  ,SUM(rest_demand) AS rest_demand\n",
        "  ,SUM(orders) AS orders\n",
        "  ,SUM(units) AS units\n",
        "FROM aud_order_join a\n",
        "WHERE nearest_order = 1\n",
        "AND demand IS NOT NULL -- additional filter for extraneous records\n",
        "GROUP BY 1\n",
        "ORDER BY 1\n",
        ")\n",
        "\n",
        ", historical_purchase AS (\n",
        "  SELECT aoa.upm_id_ab_buyer as upm_id\n",
        "  , COUNT(DISTINCT order_nbr) as histOrders\n",
        "  FROM aud_order_agg aoa  -- eligible audience\n",
        "  INNER JOIN view_dol dol on aoa.upm_id_ab_buyer = dol.upm_id\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,730) AND DATE_SUB(first_send_dt,1)\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        "-- due to how messages are set up on Bluecore, the campaign_cd is not unique to a particular day. We concat with the message_sent_ts\n",
        "-- only using the date ignores cases where multiple sends occur on the same day, potentially undercounting\n",
        ",email_sent AS (\n",
        "    SELECT fed.upm_id\n",
        "    , COUNT(DISTINCT concat(campaign_cd, LEFT(message_sent_ts,21))) as email_circ\n",
        "    FROM COMMS.FACT_EMAIL_DELIVERY fed\n",
        "    WHERE campaign_cd LIKE ${hivevar:camp_cd_str}  -- exclude control codes if using holdout\n",
        "    AND CAST(LEFT(message_sent_ts,10) AS DATE) between ${hivevar:msmt_start_dt} and ${hivevar:msmt_end_dt}\n",
        "    AND retail_geo_name = 'NA'\n",
        "    AND fed.upm_id IS NOT NULL\n",
        "    GROUP BY upm_id\n",
        ")\n",
        "\n",
        "-- returns all the instances of the campaign codes for each member along with the response types and revenue if applicable\n",
        ", email_response as (\n",
        "SELECT DISTINCT fer.upm_id\n",
        ", LEFT(response_ts,10) AS response_ts\n",
        ", response_type\n",
        ", revenue_dollar\n",
        "FROM comms.fact_email_response fer\n",
        "INNER JOIN test_control_table tct ON fer.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        "WHERE campaign_cd LIKE ${hivevar:camp_cd_str}   -- exclude control codes if using holdout\n",
        "AND LEFT(response_ts,10) between first_send_dt and DATE_ADD(last_send_dt,7)\n",
        "AND retail_geo_name = 'NA'\n",
        "AND fer.upm_id IS NOT NULL\n",
        " )\n",
        "\n",
        "-- calculates the the total number of opens, clicks, purchases and last touch attributed demand for each member for the campaign\n",
        "-- responses are only considered if they occur between when the message was sent and 7 days later.\n",
        ", email_calc AS (\n",
        "    SELECT fer.upm_id\n",
        "    , COUNT(CASE WHEN response_type= 'email open' THEN fer.upm_id ELSE NULL END) AS email_opens\n",
        "    , COUNT(CASE WHEN response_type='link click' THEN fer.upm_id ELSE NULL END) AS email_clicks\n",
        "    , COUNT(CASE WHEN response_type='ecommerce-purchase' THEN fer.upm_id ELSE NULL END) AS email_purch\n",
        "    , SUM(CASE WHEN response_type='ecommerce-purchase' THEN revenue_dollar ELSE 0 END) AS email_demand\n",
        "    FROM email_response fer\n",
        "    INNER JOIN test_control_table tct ON fer.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        "    WHERE CAST(fer.response_ts as DATE) BETWEEN first_send_dt AND DATE_ADD(last_send_dt,7)\n",
        "    GROUP BY 1\n",
        ")\n",
        "-- total number of total messages send for each member during the campaign.\n",
        "-- For a singular campaign, this should be one (1), for triggers it could be more depending on how the trigger is set up\n",
        ", email_delivery_agg as (\n",
        "  SELECT upm_id\n",
        "  ,SUM(email_circ) AS email_circ\n",
        "  FROM email_sent\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", email_response_agg as (\n",
        "  SELECT upm_id\n",
        "  ,SUM(email_opens) AS email_opens\n",
        "  ,SUM(email_clicks) AS email_clicks\n",
        "  ,SUM(email_purch) AS email_purch\n",
        "  ,SUM(email_demand) AS email_demand\n",
        "  FROM email_calc\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", platform_usage as (\n",
        "  SELECT tct.upm_id\n",
        "  , CASE WHEN CAST(first_${hivevar:usage_platform}_activity_dt AS DATE) BETWEEN first_send_dt AND DATE_ADD(last_send_dt,7) THEN 1\n",
        "         ELSE 0\n",
        "    END AS platform_user\n",
        "  FROM aud_select_workspace.gck_first_and_last_touchpoint flt\n",
        "  INNER JOIN test_control_table tct ON flt.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        ")\n",
        "\n",
        "---------------------\n",
        "-- MAIN QUERY\n",
        "----------------------\n",
        "SELECT z.test_control\n",
        "  -- primary metrics ON incremental revenue: conversion rate, average demand per purchaser\n",
        "  ,COUNT(DISTINCT z.upm_id) AS total_members\n",
        "  ,COUNT(DISTINCT abp.upm_id_ab_buyer) AS buying_members\n",
        "  ,COUNT(DISTINCT abp.upm_id_ab_buyer) / COUNT(DISTINCT z.upm_id) AS conversion_rate\n",
        "  ,AVG(demand) AS avg_demand\n",
        "  ,stddev(demand) AS std_demand\n",
        "  -- secondary metrics ON order double click\n",
        "  ,SUM(demand) / SUM(orders) AS AOV\n",
        "  ,SUM(demand) / SUM(units) AS AUR\n",
        "  ,SUM(units) / SUM(orders) AS UPT\n",
        "\n",
        "  -- secondary metrics ON emails\n",
        "  ,SUM(email_circ) AS email_sent\n",
        "  ,SUM(email_opens) AS email_opens\n",
        "  ,SUM(email_clicks) AS email_clicks\n",
        "  ,SUM(email_purch) AS email_purch\n",
        "  ,SUM(email_demand) AS email_demand\n",
        "  ,(CASE WHEN sum(email_circ) = 0 THEN 0 ELSE SUM(email_opens) / SUM(email_circ) END) AS email_open_rate\n",
        "  ,(CASE WHEN sum(email_opens) = 0 THEN 0 ELSE SUM(email_clicks) / SUM(email_circ) END) AS email_click_rate --against sends\n",
        "  --,(CASE WHEN sum(email_opens) = 0 THEN 0 ELSE SUM(email_clicks) / SUM(email_opens) END) AS email_click_rate --against clicks\n",
        "  ,(CASE WHEN sum(email_circ) = 0 THEN 0 ELSE SUM(email_purch) / SUM(email_circ) END) AS email_conversion_rate\n",
        "\n",
        "  -- secondary metrics ON site/app engagement\n",
        "  ,AVG(visits) AS avg_site_app_visits\n",
        "  ,AVG(PDP_FAVORITE_COUNT) AS avg_PDP_FAVORITE_COUNT\n",
        "  ,AVG(ADD_TO_CART_COUNT) AS avg_ADD_TO_CART_COUNT\n",
        "  ,AVG(physical_activity) AS avg_physical_activity\n",
        "\n",
        "  -- secondary metrics ON margin\n",
        "  ,(AVG(base_FW_demand) * 0.38 + AVG(base_AP_demand) * 0.28 + AVG(base_EQ_demand) * 0.32\n",
        "    + AVG(clr_FW_demand) * 0.23 + AVG(base_AP_demand) * 0.17 + AVG(clr_EQ_demand) * 0.12\n",
        "    + AVG(launch_FW_demand) * 0.25 + AVG(launch_AP_demand) * 0.08 + AVG(launch_EQ_demand) * 0.19\n",
        "    + AVG(rest_demand) * 0.3\n",
        "  ) AS avg_margin\n",
        "  ,AVG(base_FW_demand) AS base_FW_demand\n",
        "  ,AVG(base_AP_demand) AS base_AP_demand\n",
        "  ,AVG(base_EQ_demand) AS base_EQ_demand\n",
        "  ,AVG(clr_FW_demand) AS clr_FW_demand\n",
        "  ,AVG(clr_AP_demand) AS clr_AP_demand\n",
        "  ,AVG(clr_EQ_demand) AS clr_EQ_demand\n",
        "  ,AVG(launch_FW_demand) AS launch_FW_demand\n",
        "  ,AVG(launch_AP_demand) AS launch_AP_demand\n",
        "  ,AVG(launch_EQ_demand) AS launch_EQ_demand\n",
        "  ,AVG(rest_demand) AS rest_demand\n",
        "    -- secondary metrics ON order history\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END) AS 1_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END)/COUNT(DISTINCT z.upm_id) AS pctBuyers_1_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN demand ELSE 0 END)/SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END) AS DPB_1_prior\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END) AS 2_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END)/COUNT(DISTINCT z.upm_id) AS pctBuyers_2_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN demand ELSE 0 END)/SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END) AS DPB_2_prior\n",
        "    -- optional first usage kpi\n",
        "  ,SUM(platform_user) AS first_usage\n",
        "\n",
        "/*  -- Optional KPI's  --\n",
        "  ,SUM(platform_user)/COUNT(DISTINCT z.upm_id) AS first_usage_pct\n",
        "  ,SUM(NEOD_Session) AS NEOD_Session\n",
        "*/\n",
        "\n",
        "FROM test_control_table z\n",
        "INNER JOIN balanced_aud_aa b ON z.upm_id = b.upm_id  --inner join our balanced audience to remove members that were distorting the AA period balance\n",
        "\n",
        "INNER JOIN mhub mh ON z.upm_id = mh.upm_id\n",
        "LEFT JOIN member_activities a ON mh.member_id = a.member_id\n",
        "\n",
        "LEFT JOIN aud_order_agg abp ON z.upm_id = abp.upm_id_ab_buyer\n",
        "LEFT JOIN historical_purchase hp ON z.upm_id = hp.upm_id\n",
        "\n",
        "LEFT JOIN email_delivery_agg cda ON z.upm_id = cda.upm_id\n",
        "LEFT JOIN email_response_agg cra ON z.upm_id = cra.upm_id\n",
        "\n",
        "LEFT JOIN platform_usage plt ON z.upm_id = plt.upm_id\n",
        "\n",
        "WHERE (demand < 600 OR demand IS NULL)\n",
        "\n",
        "GROUP BY 1\n",
        "ORDER BY 1 DESC;"
      ],
      "metadata": {
        "id": "uzdnqFbyRVJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full AB Code - V5"
      ],
      "metadata": {
        "id": "2oNTNFUz0doO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%python\n",
        "\n",
        "# Execute SQL Query\n",
        "ab_data = \"\"\"\n",
        "-----------------------------------------------------------------------------------------------------------------------------------\n",
        "-- AB Period\n",
        "-----------------------------------------------------------------------------------------------------------------------------------\n",
        "WITH\n",
        "\n",
        "tc_first_last_send AS (\n",
        "    SELECT *\n",
        "    FROM (\n",
        "         (SELECT distinct tst.upm_id\n",
        "          , 'test' as test_control\n",
        "          , MIN(LEFT(message_sent_ts,10)) AS campaign_send_dt\n",
        "          , MIN(LEFT(message_sent_ts,21)) AS campaign_send_ts\n",
        "          , MIN(LEFT(message_sent_ts,10)) as first_send_dt\n",
        "          , MAX(LEFT(message_sent_ts,10)) as last_send_dt\n",
        "          FROM ${hivevar:aud_table_tst} tst\n",
        "          INNER JOIN comms.fact_email_delivery fed ON tst.upm_id = fed.upm_id AND campaign_geo_name = 'NA' AND campaign_cd LIKE ${hivevar:camp_cd_tst_str} AND CAST(message_sent_ts AS DATE) BETWEEN ${hivevar:msmt_start_dt} AND ${hivevar:msmt_end_dt}\n",
        "          GROUP BY 1,2\n",
        "          )\n",
        "      UNION\n",
        "        (SELECT distinct upm_id\n",
        "         , 'control' as test_control\n",
        "         , ${hivevar:msmt_start_dt} AS campaign_send_dt\n",
        "         , to_timestamp(${hivevar:msmt_start_dt}, 'yyyy-MM-dd') AS campaign_send_ts\n",
        "         , ${hivevar:msmt_start_dt} as first_send_dt\n",
        "         , ${hivevar:msmt_start_dt} as last_send_dt\n",
        "         from ${hivevar:aud_table_ctl}\n",
        "         )\n",
        "    )\n",
        ")\n",
        "\n",
        ", test_control_table AS (\n",
        "    SELECT *\n",
        "    , LEAD(a.campaign_send_dt) OVER (PARTITION BY a.upm_id ORDER BY a.campaign_send_dt) AS next_send_dt\n",
        "    , LAG(a.campaign_send_dt) OVER (PARTITION BY a.upm_id ORDER BY a.campaign_send_dt) AS previous_send_dt\n",
        "    , (int(to_timestamp(a.campaign_send_dt)) - int(to_timestamp(LEAD(a.campaign_send_dt) OVER (PARTITION BY a.upm_id ORDER BY a.campaign_send_dt))))/86400 AS difference_next_send_days\n",
        "    , (int(to_timestamp(a.campaign_send_dt)) - int(to_timestamp(LAG(a.campaign_send_dt) OVER (PARTITION BY a.upm_id ORDER BY a.campaign_send_dt))))/86400 AS difference_previous_send_days\n",
        "    FROM tc_first_last_send a\n",
        "    ANTI JOIN gwan13.bot_master_nike_com bot ON a.upm_id = bot.upm_id\n",
        "    ANTI JOIN aud_select_workspace.reseller_new slr ON a.upm_id = slr.upm_user_id\n",
        ")\n",
        "\n",
        ", view_dol AS (\n",
        "  SELECT dol.upm_id\n",
        "  , order_nbr\n",
        "  , order_dt\n",
        "  , origl_ordered_qty\n",
        "  , grd_amt_excl_tax_usd\n",
        "  , line_of_business_desc\n",
        "  , univ_div_desc\n",
        "  , first_send_dt\n",
        "  , last_send_dt\n",
        "  FROM DTC_INTEGRATED.DTC_DIGITAL_ORDER_LINE dol\n",
        "  INNER JOIN test_control_table tct ON dol.upm_id = tct.upm_id\n",
        "  WHERE region_key = 1\n",
        "    AND rec_excl_ind = 0\n",
        "    AND ttl_demand_ind = 1\n",
        "    AND univ_cat_desc <> 'Converse'\n",
        "    AND dol.upm_id IS NOT NULL\n",
        "    AND CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,730) AND DATE_ADD(last_send_dt,7)\n",
        "    AND (rtn_qty = 0 or rtn_qty IS NULL)\n",
        ")\n",
        "\n",
        "-- AA period purchase. dynamically based on when the member receives their first communication. Only returns demand for members in the\n",
        ", aa_purchase AS (\n",
        "  SELECT dol.upm_id\n",
        "    ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "    ,COUNT(DISTINCT order_nbr) AS orders\n",
        "    ,SUM(origl_ordered_qty) AS units\n",
        "  FROM view_dol dol\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,31) AND DATE_SUB(first_send_dt,1)\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", demand_bucket AS (\n",
        "  SELECT tct.upm_id\n",
        "  , tct.test_control\n",
        "  , aa.upm_id as upm_id_aa_buyer\n",
        "  , aa.demand\n",
        "  , aa.orders\n",
        "  , aa.units\n",
        "  , CASE\n",
        "    WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "    WHEN aa.demand < 50 THEN '000-050'\n",
        "    WHEN aa.demand < 100 THEN '050-100'\n",
        "    WHEN aa.demand < 200 THEN '100-200'\n",
        "    WHEN aa.demand < 500 THEN '200-500'\n",
        "    WHEN aa.demand < 1000 THEN '500-1000'\n",
        "    WHEN aa.demand >=1000 THEN '1000+'\n",
        "    END AS demand_per_buyer_bucket\n",
        "  , PERCENT_RANK() OVER (\n",
        "    PARTITION BY test_control\n",
        "    , CASE\n",
        "      WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "      WHEN aa.demand < 50 THEN '000-050'\n",
        "      WHEN aa.demand < 100 THEN '050-100'\n",
        "      WHEN aa.demand < 200 THEN '100-200'\n",
        "      WHEN aa.demand < 500 THEN '200-500'\n",
        "      WHEN aa.demand < 1000 THEN '500-1000'\n",
        "      WHEN aa.demand >=1000 THEN '1000+'\n",
        "      END ORDER BY RANDOM(0)\n",
        "    ) AS percent_rank\n",
        "  FROM test_control_table tct\n",
        "  LEFT JOIN aa_purchase aa ON tct.upm_id = aa.upm_id\n",
        ")\n",
        "\n",
        ", balanced_aud_aa as (\n",
        "  SELECT DISTINCT upm_id\n",
        "  FROM demand_bucket db\n",
        "  -- remove outliers\n",
        "  WHERE (demand < 2400 OR demand IS NULL)\n",
        "-- To rebalance, paste text block from template in place of defaults\n",
        "AND ((test_control = 'control' and (demand > 0 and demand <= 50) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand = 0 or demand is null) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand >0 and demand <= 50) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand = 0 or demand is null) and PERCENT_RANK <= 1)\n",
        "\t)\n",
        ")\n",
        "\n",
        ", mhub as (\n",
        " SELECT source_id as upm_id\n",
        " , member_id\n",
        " , preferred_gender\n",
        " FROM MEMBER.MEMBER_HUB\n",
        " WHERE ctry_2_cd = 'US'\n",
        " AND lower(dpa_status) IN ('activate','reactivate')\n",
        ")\n",
        "\n",
        "-- calculate a dynamic member activity window for each communication received by a member\n",
        "-- activity is calculated on a 7 day window, unless the member receives a communication within that timeframe\n",
        "-- When multiple messages are received, activity is only calculated up to the next message_send_dt to avoid double counting\n",
        ", member_activities as (\n",
        "  SELECT mgd.member_id\n",
        "  ,SUM(logged_in_visits_count) AS visits\n",
        "  ,SUM(pdp_favorite_count) AS pdp_favorite_count\n",
        "  ,SUM(add_to_cart_count) AS add_to_cart_count\n",
        "  ,SUM(physical_activity_count) AS physical_activity\n",
        "  ,MAX(CASE WHEN new_user_experience_flag = 'Y' THEN 1 ELSE 0 END) AS new_user_flag\n",
        "  FROM test_control_table tct\n",
        "  INNER JOIN mhub mh ON tct.upm_id = mh.upm_id\n",
        "  LEFT JOIN aud_select_workspace.member_agg_member_growth_daily mgd ON mh.member_id = mgd.member_id\n",
        "  WHERE mgd.activity_dt >= tct.campaign_send_dt\n",
        "    AND mgd.activity_dt < DATE_ADD(campaign_send_dt, INT(LEAST(7,COALESCE(difference_next_send_days,7))))\n",
        "    AND mgd.preferred_retail_geo = 'NA'\n",
        "    AND mgd.experience_name != 'agnostic'\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        "-- for each user, aggregate each order placed within our measurement window. create labels to categorize the type of demand\n",
        ", ab_purchase as (\n",
        "  SELECT dol.upm_id as upm_id_ab_buyer\n",
        "  , order_dt as order_ts\n",
        "  , order_nbr\n",
        "  ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc NOT IN ('Base Inline', 'Clearance', 'Launch') THEN grd_amt_excl_tax_usd ELSE 0 END) AS rest_demand\n",
        "  ,COUNT(DISTINCT order_nbr) AS orders\n",
        "  ,SUM(origl_ordered_qty) AS units\n",
        "  FROM view_dol dol\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN first_send_dt AND DATE_ADD(first_send_dt,7)\n",
        "  GROUP BY 1,2,3\n",
        ")\n",
        "\n",
        "-- For each member, rank each order by how near it was to the last campaign send\n",
        "-- purchases must be after the campaign send and within 7 days of the send to be attributed\n",
        ", aud_order_join as (\n",
        "SELECT a.*\n",
        ", b.*\n",
        ",(bigint(to_timestamp(b.order_ts))-bigint(to_timestamp(a.campaign_send_ts)))/60 as order_send_diff_minutes\n",
        ",RANK() OVER(PARTITION BY a.upm_id, b.order_ts ORDER BY (bigint(to_timestamp(b.order_ts))-bigint(to_timestamp(a.campaign_send_ts)))/60 ASC) as nearest_order\n",
        "FROM test_control_table a\n",
        "LEFT JOIN ab_purchase b ON (a.upm_id = b.upm_id_ab_buyer AND b.order_ts >= a.campaign_send_ts AND b.order_ts <= DATE_ADD(a.campaign_send_ts,7))\n",
        "ORDER BY a.upm_id,a.campaign_send_dt, nearest_order asc\n",
        ")\n",
        "\n",
        "-- Aggregate each distinct send, attributing only the nearest order.\n",
        "-- This prevent double counting of orders when a members receives multiple sends within the measurement window.\n",
        ", aud_order_agg as (\n",
        "SELECT upm_id AS upm_id_ab_buyer\n",
        "  ,SUM(demand) AS demand\n",
        "  ,SUM(base_FW_demand) AS base_FW_demand\n",
        "  ,SUM(base_AP_demand) AS base_AP_demand\n",
        "  ,SUM(base_EQ_demand) AS base_EQ_demand\n",
        "  ,SUM(clr_FW_demand) AS clr_FW_demand\n",
        "  ,SUM(clr_AP_demand) AS clr_AP_demand\n",
        "  ,SUM(clr_EQ_demand) AS clr_EQ_demand\n",
        "  ,SUM(launch_FW_demand) AS launch_FW_demand\n",
        "  ,SUM(launch_AP_demand) AS launch_AP_demand\n",
        "  ,SUM(launch_EQ_demand) AS launch_EQ_demand\n",
        "  ,SUM(rest_demand) AS rest_demand\n",
        "  ,SUM(orders) AS orders\n",
        "  ,SUM(units) AS units\n",
        "FROM aud_order_join a\n",
        "WHERE nearest_order = 1\n",
        "AND demand IS NOT NULL -- additional filter for extraneous records\n",
        "GROUP BY 1\n",
        "ORDER BY 1\n",
        ")\n",
        "\n",
        ", historical_purchase AS (\n",
        "  SELECT aoa.upm_id_ab_buyer as upm_id\n",
        "  , COUNT(DISTINCT order_nbr) as histOrders\n",
        "  FROM aud_order_agg aoa  -- eligible audience\n",
        "  INNER JOIN view_dol dol on aoa.upm_id_ab_buyer = dol.upm_id\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,730) AND DATE_SUB(first_send_dt,1)\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        "-- due to how messages are set up on Bluecore, the campaign_cd is not unique to a particular day. We concat with the message_sent_ts\n",
        "-- only using the date ignores cases where multiple sends occur on the same day, potentially undercounting\n",
        ",email_sent AS (\n",
        "  SELECT fed.upm_id\n",
        "  , COUNT(DISTINCT concat(campaign_cd, LEFT(message_sent_ts,21))) as email_circ\n",
        "  FROM COMMS.FACT_EMAIL_DELIVERY fed\n",
        "  WHERE (campaign_cd LIKE ${hivevar:camp_cd_tst_str} OR campaign_cd LIKE ${hivevar:camp_cd_ctl_str})\n",
        "  AND CAST(message_sent_ts AS DATE) between ${hivevar:msmt_start_dt} and ${hivevar:msmt_end_dt}\n",
        "  AND retail_geo_name = 'NA'\n",
        "  AND fed.upm_id IS NOT NULL\n",
        "  GROUP BY upm_id\n",
        ")\n",
        "\n",
        "-- returns all the instances of the campaign codes for each member along with the response types and revenue if applicable\n",
        ", email_response as (\n",
        "  SELECT DISTINCT fer.upm_id\n",
        "  , LEFT(response_ts,10) AS response_ts\n",
        "  , response_type\n",
        "  , revenue_dollar\n",
        "  FROM comms.fact_email_response fer\n",
        "  INNER JOIN test_control_table tct ON fer.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        "  WHERE (campaign_cd LIKE ${hivevar:camp_cd_tst_str} OR campaign_cd LIKE ${hivevar:camp_cd_ctl_str})\n",
        "  AND CAST(LEFT(response_ts,10) AS DATE) between first_send_dt and DATE_ADD(last_send_dt,7)\n",
        "  AND retail_geo_name = 'NA'\n",
        "  AND fer.upm_id IS NOT NULL\n",
        " )\n",
        "\n",
        "-- calculates the the total number of opens, clicks, purchases and last touch attributed demand for each member for the campaign\n",
        "-- responses are only considered if they occur between when the message was sent and 7 days later.\n",
        ", email_calc AS (\n",
        "  SELECT fer.upm_id\n",
        "  , COUNT(CASE WHEN response_type= 'email open' THEN fer.upm_id ELSE NULL END) AS email_opens\n",
        "  , COUNT(CASE WHEN response_type='link click' THEN fer.upm_id ELSE NULL END) AS email_clicks\n",
        "  , COUNT(CASE WHEN response_type='ecommerce-purchase' THEN fer.upm_id ELSE NULL END) AS email_purch\n",
        "  , SUM(CASE WHEN response_type='ecommerce-purchase' THEN revenue_dollar ELSE 0 END) AS email_demand\n",
        "  FROM email_response fer\n",
        "  INNER JOIN test_control_table tct ON fer.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        "  WHERE CAST(fer.response_ts as DATE) BETWEEN first_send_dt AND DATE_ADD(last_send_dt,7)\n",
        "  GROUP BY 1\n",
        ")\n",
        "-- total number of total messages send for each member during the campaign.\n",
        "-- For a singular campaign, this should be one (1), for triggers it could be more depending on how the trigger is set up\n",
        ", email_delivery_agg as (\n",
        "  SELECT upm_id\n",
        "  ,SUM(email_circ) AS email_circ\n",
        "  FROM email_sent\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", email_response_agg as (\n",
        "  SELECT upm_id\n",
        "  ,SUM(email_opens) AS email_opens\n",
        "  ,SUM(email_clicks) AS email_clicks\n",
        "  ,SUM(email_purch) AS email_purch\n",
        "  ,SUM(email_demand) AS email_demand\n",
        "  FROM email_calc\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", platform_usage as (\n",
        "  SELECT tct.upm_id\n",
        "  , CASE WHEN CAST(first_${hivevar:usage_platform}_activity_dt AS DATE) BETWEEN first_send_dt AND DATE_ADD(last_send_dt,7) THEN 1\n",
        "         ELSE 0\n",
        "    END AS platform_user\n",
        "  FROM aud_select_workspace.gck_first_and_last_touchpoint flt\n",
        "  INNER JOIN test_control_table tct ON flt.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        ")\n",
        "\n",
        "---------------------\n",
        "-- MAIN QUERY\n",
        "----------------------\n",
        "SELECT z.upm_id\n",
        "  , z.test_control\n",
        "\n",
        "  -- primary metrics ON incremental revenue: conversion rate, average demand per purchaser\n",
        "  ,SUM(demand) AS demand\n",
        "\n",
        "  -- secondary metrics ON order double click\n",
        "  ,SUM(orders) AS orders\n",
        "  ,SUM(units) AS units\n",
        "\n",
        "  -- secondary metrics ON emails\n",
        "  , SUM(email_circ) AS email_sent\n",
        "  , SUM(email_opens) AS email_opens\n",
        "  , SUM(email_clicks) AS email_clicks\n",
        "  , SUM(email_purch) AS email_purch\n",
        "  , SUM(email_demand) AS email_demand\n",
        "\n",
        " -- secondary metrics ON site/app engagement\n",
        " , SUM(visits) AS site_app_visits\n",
        " , SUM(PDP_FAVORITE_COUNT) AS PDP_FAVORITE_COUNT\n",
        " , SUM(ADD_TO_CART_COUNT) AS ADD_TO_CART_COUNT\n",
        " , SUM(physical_activity) AS physical_activity\n",
        "\n",
        "  -- secondary metrics ON margin\n",
        "  , SUM(base_FW_demand) AS base_FW_demand\n",
        "  , SUM(base_AP_demand) AS base_AP_demand\n",
        "  , SUM(base_EQ_demand) AS base_EQ_demand\n",
        "  , SUM(clr_FW_demand) AS clr_FW_demand\n",
        "  , SUM(clr_AP_demand) AS clr_AP_demand\n",
        "  , SUM(clr_EQ_demand) AS clr_EQ_demand\n",
        "  , SUM(launch_FW_demand) AS launch_FW_demand\n",
        "  , SUM(launch_AP_demand) AS launch_AP_demand\n",
        "  , SUM(launch_EQ_demand) AS launch_EQ_demand\n",
        "  , SUM(rest_demand) AS rest_demand\n",
        "\n",
        "   -- optional first usage kpi\n",
        "  , SUM(platform_user) AS platform_user\n",
        "\n",
        "    -- secondary metrics ON order history\n",
        "  ,SUM(histOrders) AS histOrders\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END) AS 1_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END) AS 2_priorOrder\n",
        "\n",
        "\n",
        "FROM test_control_table z\n",
        "INNER JOIN balanced_aud_aa b ON z.upm_id = b.upm_id  --inner join our balanced audience to remove members that were distorting the AA period balance\n",
        "\n",
        "INNER JOIN mhub mh ON z.upm_id = mh.upm_id\n",
        "LEFT JOIN member_activities a ON mh.member_id = a.member_id\n",
        "\n",
        "LEFT JOIN aud_order_agg abp ON z.upm_id = abp.upm_id_ab_buyer\n",
        "LEFT JOIN historical_purchase hp ON z.upm_id = hp.upm_id\n",
        "\n",
        "LEFT JOIN email_delivery_agg cda ON z.upm_id = cda.upm_id\n",
        "LEFT JOIN email_response_agg cra ON z.upm_id = cra.upm_id\n",
        "\n",
        "LEFT JOIN platform_usage plt ON z.upm_id = plt.upm_id\n",
        "\n",
        "GROUP BY 1,2\n",
        "ORDER BY 1 DESC;\n",
        "\"\"\"\n",
        "\n",
        "# Load the data into a PySpark DataFrame\n",
        "df_spark = spark.sql(ab_data)\n",
        "\n",
        "# Convert to Pandas DataFrame for further analysis\n",
        "df = df_spark.toPandas()\n",
        "\n",
        "# Specify your personal schema and table name\n",
        "full_table_name = \"your_personal_schema.your_table_name\"\n",
        "\n",
        "# Save the DataFrame as a table in your personal schema\n",
        "df_result.write.mode(\"overwrite\").saveAsTable(full_table_name)\n",
        "\n",
        "\"\"\" new stuff \"\"\"\n",
        "\n",
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create and fetch widget values\n",
        "dbutils.widgets.text(\"s3_bucket_path\", \"s3://default-bucket/personal-schemas\", \"S3 Bucket Path\")\n",
        "dbutils.widgets.text(\"personal_schema\", \"my_personal_schema\", \"Personal Schema\")\n",
        "dbutils.widgets.text(\"table_name\", \"my_table\", \"Table Name\")\n",
        "\n",
        "s3_bucket_path = dbutils.widgets.get(\"s3_bucket_path\")\n",
        "personal_schema = dbutils.widgets.get(\"personal_schema\")\n",
        "table_name = dbutils.widgets.get(\"table_name\")\n",
        "\n",
        "# Assuming 'balanced_df' is your pandas DataFrame, convert it to a Spark DataFrame\n",
        "balanced_spark_df = spark.createDataFrame(balanced_df)\n",
        "\n",
        "# Construct the path for the external table\n",
        "full_path = f\"{s3_bucket_path}/{personal_schema}/{table_name}\"\n",
        "\n",
        "# Save the DataFrame as an external table in your personal schema in S3\n",
        "# Ensure the database (schema) exists\n",
        "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {personal_schema} LOCATION '{s3_bucket_path}/{personal_schema}'\")\n",
        "spark.sql(f\"USE {personal_schema}\")\n",
        "\n",
        "# Save the DataFrame as an external table at the specified path\n",
        "balanced_spark_df.write.mode(\"overwrite\").option(\"path\", full_path).saveAsTable(table_name)\n",
        "\n"
      ],
      "metadata": {
        "id": "eQME19bE0gP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AB Scorecard Metrics"
      ],
      "metadata": {
        "id": "sMZeBqlgGoZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%python\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "## Extracting input parameters for Demand\n",
        "\n",
        "\n",
        "# Calculate average sales (demand) for control and test groups\n",
        "control_avg_demand = control['demand'].mean()\n",
        "test_avg_demand = test['demand'].mean()\n",
        "\n",
        "# Calculate standard deviation of demand for control and test groups\n",
        "control_std_dev = control['demand'].std()\n",
        "test_std_dev = test['demand'].std()\n",
        "\n",
        "# Calculate the # of buying members for the control and test groups\n",
        "control_sample_size = control[control['orders'] > 0]['upm_id'].nunique()\n",
        "test_sample_size = test[test['orders'] > 0]['upm_id'].nunique()\n",
        "\n",
        "\n",
        "## Perform Statistical Analysis for Demand\n",
        "\n",
        "\n",
        "# Calculate delta and z-value\n",
        "\"\"\"\n",
        "Delta, standard error, and z-value, are part of the t-test calculation.\n",
        "\n",
        "We typically wouldn't calculate these separately when using a t-test.\n",
        "\n",
        "If we need to see these, they're more relevant in the context of manual calculations or when not using a t-test directly.\n",
        "\"\"\"\n",
        "\n",
        "# Calculate standard error\n",
        "\"\"\"\n",
        "A note about standard error:\n",
        "\n",
        "* When using stats.ttest_ind with equal_var=False, the test internally computes a version of standard error adjusted for the Welch's t-test.\n",
        "* Welch's t-test doesn't assume equal population variances.\n",
        "* For educational purposes, you can still calculate a simple standard error of the mean (SEM) for each group.\n",
        "* Or, calculate the standard error of the difference between two means if you're comparing them directly.\n",
        "* Code to do so follows below...\n",
        "\n",
        "# Calculate the standard error of the mean for each group\n",
        "sem_control = control['avg_demand'].sem()\n",
        "sem_test = test['avg_demand'].sem()\n",
        "\n",
        "# Calculate the standard error for the difference between two means\n",
        "# Again, this formula is for illustrative purposes; the t-test already accounts for this in its calculation\n",
        "n_control = len(control['avg_demand'])\n",
        "n_test = len(test['avg_demand'])\n",
        "std_error_diff = np.sqrt(sem_control**2 / n_control + sem_test**2 / n_test)\n",
        "\n",
        "print(f\"Standard Error (Control): {sem_control}\")\n",
        "print(f\"Standard Error (Test): {sem_test}\")\n",
        "print(f\"Standard Error of Difference: {std_error_diff}\")\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "## Performing the t-test and ignoring NaN values\n",
        "\n",
        "\n",
        "# Two-tailed t-test for Avg Demand\n",
        "t_stat_two_tailed, p_value_two_tailed = stats.ttest_ind(test['demand'].dropna(), control['demand'].dropna(), equal_var=False, nan_policy='omit')\n",
        "# Calculating confidence levels for two-tailed test\n",
        "confidence_level_two_tailed = 1 - p_value_two_tailed\n",
        "type_i_error_demand_two_tailed = p_value_two_tailed\n",
        "\n",
        "# One-tailed t-test for Avg Demand (assuming we are checking if test > control)\n",
        "\"\"\"\n",
        "Note:\n",
        "We halve the p-value and adjust its interpretation based on the direction of the t-statistic.\n",
        "\n",
        "The one-tailed test results are specifically meant for the hypothesis that the test group mean is greater than the control group mean.\n",
        "\"\"\"\n",
        "if t_stat_two_tailed > 0:\n",
        "    # Halve the p-value for positive t-statistic where test mean is hypothesized to be greater than control mean\n",
        "    p_value_one_tailed = p_value_two_tailed / 2\n",
        "    confidence_level_one_tailed = 1 - p_value_one_tailed\n",
        "    print(\"\\nOne-Tailed Test Results (Test > Control):\")\n",
        "    print(f\"T-statistic: {t_stat_two_tailed}\")  # Same T-statistic as the two-tailed test\n",
        "    print(f\"One-Tailed P-value: {p_value_one_tailed}\")\n",
        "    print(f\"Confidence Level: {confidence_level_one_tailed*100:.2f}%\")\n",
        "else:\n",
        "    # If the t-statistic is negative, indicating the test group mean is not greater than the control mean\n",
        "    print(\"\\nOne-Tailed Test Results (Test > Control):\")\n",
        "    print(f\"T-statistic: {t_stat_two_tailed}\")\n",
        "    print(\"The outcome does not support the hypothesized direction (test > control).\")\n",
        "    print(\"Interpretation of confidence levels or significance is not applicable for this outcome.\")\n",
        "\n",
        "# Output the results for two-tailed test\n",
        "print(\"\\nTwo-Tailed Test Results:\")\n",
        "print(f\"T-statistic: {t_stat_two_tailed}\")\n",
        "print(f\"P-value: {p_value_two_tailed}\")\n",
        "print(f\"Confidence Level: {confidence_level_two_tailed*100:.2f}%\")\n",
        "\n",
        "\"\"\"\n",
        "# Perform t-test for Avg Demand, omitting 'nan' values\n",
        "t_stat_demand, p_value_demand = stats.ttest_ind(test['demand'], control['demand'], equal_var=False, nan_policy='omit')\n",
        "confidence_level_demand = 1 - p_value_demand\n",
        "type_i_error_demand = p_value_demand\n",
        "\n",
        "print(f\"T-statistic for Demand: {t_stat_demand}\")\n",
        "print(f\"P-value for Demand: {p_value_demand}\")\n",
        "print(f\"Confidence Level for Demand: {confidence_level_demand*100:.2f}%\")\n",
        "print(f\"Type I Error for Demand: {type_i_error_demand:.4f}\")\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Demand Sample Size - Control: {control_sample_size}\")\n",
        "print(f\"Demand Sample Size - Test: {test_sample_size}\")\n",
        "print(f\"Average Demand - Control: {control_avg_demand:.2f}\")\n",
        "print(f\"Average Demand - Test: {test_avg_demand:.2f}\")\n",
        "print(f\"Standard Deviation - Test: {test_std_dev:.2f}\")\n",
        "print(f\"Standard Deviation - Control: {control_std_dev:.2f}\")\n",
        "\n",
        "print(f\"T-statistic for Demand: {t_stat_two_tailed}\")\n",
        "print(f\"P-value for Demand: {p_value_two_tailed}\")\n",
        "print(f\"Confidence Level for Demand: {confidence_level_two_tailed*100:.2f}%\")\n",
        "\n",
        "\n",
        "## Extracting input parameters for Conversion\n",
        "\n",
        "\n",
        "# Total count of all unique members in the test and control groups\n",
        "control_conv_sample_size = control['upm_id'].nunique()\n",
        "test_conv_sample_size = test['upm_id'].nunique()\n",
        "\n",
        "# Calculate conversion rate for the test and control groups (total buying members / total members)\n",
        "control_conv_rate = control_sample_size / control_conv_sample_size if control_conv_sample_size > 0 else 0\n",
        "test_conv_rate = test_sample_size / test_conv_sample_size if test_conv_sample_size > 0 else 0\n",
        "\n",
        "# Calculate lift for the conversion rate\n",
        "lift = abs(test_conv_rate - control_conv_rate)\n",
        "\n",
        "# Calculate difference of proportions\n",
        "difference_of_proportions = test_conv_rate - control_conv_rate\n",
        "\n",
        "print(f\"Control Conversion Rate: {control_conv_rate*100:.2f}%\")\n",
        "print(f\"Test Conversion Rate: {test_conv_rate*100:.2f}%\")\n",
        "print(f\"Absolute Lift in Conversion Rate: {lift*100:.4f}%\")\n",
        "print(f\"Difference_of_proportions: {difference_of_proportions:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "## Perform Statistical Analysis for Conversion\n",
        "\n",
        "# Make sure these are not views but standalone DataFrames to avoid SettingWithCopyWarning\n",
        "control = control.copy()\n",
        "test = test.copy()\n",
        "# Add a 'buyer' column to both test and control DataFrames\n",
        "control.loc[:, 'buyer'] = (control['orders'] > 0).astype(int) # 'buyer' = 1 if 'orders' > 0, else 0\n",
        "test.loc[:, 'buyer'] = (test['orders'] > 0).astype(int)\n",
        "\n",
        "## Perform t-test for Conversion Rate\n",
        "t_stat_conversion, p_value_conversion = stats.ttest_ind(test['buyer'], control['buyer'], equal_var=False)\n",
        "\n",
        "# One-tailed test interpretation for Conversion Rate (assuming we are checking if test > control)\n",
        "if t_stat_conversion > 0:\n",
        "    \"\"\" Halve the p-value for positive t-statistic where test conversion rate is hypothesized to be greater than control\"\"\"\n",
        "    p_value_one_tailed_conversion = p_value_conversion / 2\n",
        "    confidence_level_one_tailed_conversion = 1 - p_value_one_tailed_conversion\n",
        "    print(\"\\nOne-Tailed Test Results (Test > Control) for Conversion Rate:\")\n",
        "    print(f\"T-statistic: {t_stat_conversion:.4f}\")\n",
        "    print(f\"One-Tailed P-value: {p_value_one_tailed_conversion:.4f}\")\n",
        "    print(f\"Confidence Level: {confidence_level_one_tailed_conversion*100:.2f}%\")\n",
        "else:\n",
        "    # If the t-statistic is negative, indicating the test group conversion rate is not greater than the control\n",
        "    print(\"\\nOne-Tailed Test Results (Test > Control) for Conversion Rate:\")\n",
        "    print(f\"T-statistic: {t_stat_conversion:.4f}\")\n",
        "    print(\"The outcome does not support the hypothesized direction (test > control) for conversion rate.\")\n",
        "    print(\"Interpretation of confidence levels or significance is not applicable for this outcome.\")\n",
        "\n",
        "\"\"\"\n",
        "# Perform t-test for Conversion Rate\n",
        "t_stat_conversion, p_value_conversion = stats.ttest_ind(test['buyer'], control['buyer'], equal_var=False)\n",
        "\n",
        "# Calculate confidence level and Type I error for Conversion Rate\n",
        "confidence_level_conversion = 1 - p_value_conversion\n",
        "type_i_error_conversion = p_value_conversion\n",
        "\n",
        "print(f\"T-statistic for Conversion: {t_stat_conversion:.4f}\")\n",
        "print(f\"P-value for Conversion: {p_value_conversion:.4f}\")\n",
        "print(f\"Confidence Level for Conversion: {confidence_level_conversion*100:.2f}%\")\n",
        "print(f\"Type I Error for Conversion: {type_i_error_conversion:.4f}\")\n",
        "\"\"\"\n",
        "\n",
        "## Secondary metrics ON order double click\n",
        "\n",
        "\n",
        "# AOV for test & control groups\n",
        "control_AOV = control['demand'].sum() / control['orders'].sum()\n",
        "test_AOV = test['demand'].sum() / test['orders'].sum()\n",
        "\n",
        "# AUR for test & control groups\n",
        "control_AUR = control['demand'].sum() / control['units'].sum()\n",
        "test_AUR = test['demand'].sum() / test['units'].sum()\n",
        "\n",
        "# UPT for test & control groups\n",
        "control_UPT = control['units'].sum() / control['orders'].sum()\n",
        "test_UPT = test['units'].sum() / test['orders'].sum()\n",
        "\n",
        "\n",
        "## Secondary metrics ON emails\n",
        "\n",
        "\n",
        "# Email Open Rate for test & control groups\n",
        "control_email_open_rate = control['email_opens'] / control['email_sent']\n",
        "test_email_open_rate = test['email_opens'] / test['email_sent']\n",
        "\n",
        "# Email Click Rate against sends for test & control groups\n",
        "control_email_click_rate = control['email_clicks'] / control['email_sent']\n",
        "test_email_click_rate = test['email_clicks'] / test['email_sent']\n",
        "\n",
        "# Email Click Rate against opens for test & control groups\n",
        "control_email_click_rate = control['email_clicks'] / control['email_opens']\n",
        "test_email_click_rate = test['email_clicks'] / test['email_opens']\n",
        "\n",
        "# Email Conversion Rate for test & control groups\n",
        "\n",
        "control_email_conversion_rate = control['email_purch'] / control['email_sent']\n",
        "test_email_conversion_rate = control['email_purch'] / control['email_sent']\n",
        "\n",
        "\n",
        "## Secondary metrics ON site/app engagement\n",
        "\n",
        "\n",
        "# Average Site/App Visits for test and control groups\n",
        "control_avg_site_app_visits = control['site_app_visits'].mean()\n",
        "test_avg_site_app_visits = test['site_app_visits'].mean()\n",
        "\n",
        "# Average PDP Favorite Count for test and control groups\n",
        "control_avg_site_app_visits = control['PDP_FAVORITE_COUNT'].mean()\n",
        "test_avg_site_app_visits = test['PDP_FAVORITE_COUNT'].mean()\n",
        "\n",
        "# Average Add to Cart Count for test and control groups\n",
        "control_avg_site_app_visits = control['ADD_TO_CART_COUNT'].mean()\n",
        "test_avg_site_app_visits = test['ADD_TO_CART_COUNT'].mean()\n",
        "\n",
        "# Average Add to Cart Count for test and control groups\n",
        "control_avg_site_app_visits = control['physical_activity'].mean()\n",
        "test_avg_site_app_visits = test['physical_activity'].mean()\n",
        "\n",
        "\n",
        "## Secondary metrics ON margin\n",
        "\n",
        "\n",
        "# Average Margin for test & control groups\n",
        "\n",
        "control_avg_margin = (\n",
        "    control['base_FW_demand'].mean() * 0.38 +\n",
        "    control['base_AP_demand'].mean() * 0.28 +\n",
        "    control['base_EQ_demand'].mean() * 0.32 +\n",
        "    control['clr_FW_demand'].mean() * 0.23 +\n",
        "    control['clr_AP_demand'].mean() * 0.17 +\n",
        "    control['clr_EQ_demand'].mean() * 0.12 +\n",
        "    control['launch_FW_demand'].mean() * 0.25 +\n",
        "    control['launch_AP_demand'].mean() * 0.08 +\n",
        "    control['launch_EQ_demand'].mean() * 0.19 +\n",
        "    control['rest_demand'].mean() * 0.3\n",
        ")\n",
        "\n",
        "test_avg_margin = (\n",
        "    test['base_FW_demand'].mean() * 0.38 +\n",
        "    test['base_AP_demand'].mean() * 0.28 +\n",
        "    test['base_EQ_demand'].mean() * 0.32 +\n",
        "    test['clr_FW_demand'].mean() * 0.23 +\n",
        "    test['clr_AP_demand'].mean() * 0.17 +\n",
        "    test['clr_EQ_demand'].mean() * 0.12 +\n",
        "    test['launch_FW_demand'].mean() * 0.25 +\n",
        "    test['launch_AP_demand'].mean() * 0.08 +\n",
        "    test['launch_EQ_demand'].mean() * 0.19 +\n",
        "    test['rest_demand'].mean() * 0.3\n",
        ")\n",
        "\n",
        "test_base_FW_demand = test['base_FW_demand'].mean()\n",
        "test_base_AP_demand = test['base_AP_demand'].mean()\n",
        "test_base_EQ_demand = test['base_EQ_demand'].mean()\n",
        "test_clr_FW_demand = test['clr_FW_demand'].mean()\n",
        "test_clr_AP_demand = test['clr_AP_demand'].mean()\n",
        "test_clr_EQ_demand = test['clr_EQ_demand'].mean()\n",
        "test_launch_FW_demand = test['launch_FW_demand'].mean()\n",
        "test_launch_AP_demand = test['launch_AP_demand'].mean()\n",
        "test_launch_EQ_demand = test['launch_EQ_demand'].mean()\n",
        "test_rest_demand = test['rest_demand'].mean()\n",
        "\n",
        "control_base_FW_demand = control['base_FW_demand'].mean()\n",
        "control_base_AP_demand = control['base_AP_demand'].mean()\n",
        "control_base_EQ_demand = control['base_EQ_demand'].mean()\n",
        "control_clr_FW_demand = control['clr_FW_demand'].mean()\n",
        "control_clr_AP_demand = control['clr_AP_demand'].mean()\n",
        "control_clr_EQ_demand = control['clr_EQ_demand'].mean()\n",
        "control_launch_FW_demand = control['launch_FW_demand'].mean()\n",
        "control_launch_AP_demand = control['launch_AP_demand'].mean()\n",
        "control_launch_EQ_demand = control['launch_EQ_demand'].mean()\n",
        "control_rest_demand = control['rest_demand'].mean()\n",
        "\n",
        "\n",
        "## Secondary metrics ON order history\n",
        "\n",
        "# For the control group\n",
        "\n",
        "# control_total_members = control['upm_id'].nunique()\n",
        "control_pct_buyers_1_priorOrder = (control['histOrders'] == 1).sum() / control_conv_sample_size\n",
        "control_DPB_1_prior = control[control['histOrders'] == 1]['demand'].sum() / (control['histOrders'] == 1).sum()\n",
        "control_pct_buyers_2_priorOrder = (control['histOrders'] == 2).sum() / control_conv_sample_size\n",
        "control_DPB_2_prior = control[control['histOrders'] == 2]['demand'].sum() / (control['histOrders'] == 2).sum()\n",
        "\n",
        "# For the test group\n",
        "\n",
        "# test_total_members = test['upm_id'].nunique()\n",
        "test_pct_buyers_1_priorOrder = (test['histOrders'] == 1).sum() / test_conv_sample_size\n",
        "test_DPB_1_prior = test[test['histOrders'] == 1]['demand'].sum() / (test['histOrders'] == 1).sum()\n",
        "test_pct_buyers_2_priorOrder = (test['histOrders'] == 2).sum() / test_conv_sample_size\n",
        "test_DPB_2_prior = test[test['histOrders'] == 2]['demand'].sum() / (test['histOrders'] == 2).sum()\n",
        "\n",
        "\n",
        "# Secondary KPI's for the control group\n",
        "\n",
        "control_demand_per_push_sent = control_conv_rate * control_avg_demand\n",
        "control_transactions_per_buyer = control_avg_demand / control_AOV\n",
        "control_margin_per_email_sent = control_avg_margin * control_conv_rate\n",
        "control_footwear_demand_per_member = (control['base_FW_demand'] + control['clr_FW_demand'] + control['launch_FW_demand']) * control_conv_rate\n",
        "control_apparel_demand_per_member = (control['base_AP_demand'] + control['clr_AP_demand'] + control['launch_AP_demand']) * control_conv_rate\n",
        "control_equipment_demand_per_member = (control['base_EQ_demand'] + control['clr_EQ_demand'] + control['launch_EQ_demand']) * control_conv_rate\n",
        "control_regular_product_demand_per_member = (control['base_FW_demand'] + control['base_AP_demand'] + control['base_EQ_demand']) * control_conv_rate\n",
        "control_clearance_demand_per_member = (control['clr_FW_demand'] + control['clr_AP_demand'] + control['clr_EQ_demand']) * control_conv_rate\n",
        "control_launch_demand_per_member = (control['launch_FW_demand'] + control['launch_AP_demand'] + control['launch_EQ_demand']) * control_conv_rate\n",
        "\n",
        "# Secondary KPI's for the test group\n",
        "\n",
        "test_demand_per_push_sent = test_conv_rate * test_avg_demand\n",
        "test_transactions_per_buyer = test_avg_demand / test_AOV\n",
        "test_margin_per_email_sent = test_avg_margin * test_conv_rate\n",
        "test_footwear_demand_per_member = (test['base_FW_demand'] + test['clr_FW_demand'] + test['launch_FW_demand']) * test_conv_rate\n",
        "test_apparel_demand_per_member = (test['base_AP_demand'] + test['clr_AP_demand'] + test['launch_AP_demand']) * test_conv_rate\n",
        "test_equipment_demand_per_member = (test['base_EQ_demand'] + test['clr_EQ_demand'] + test['launch_EQ_demand']) * test_conv_rate\n",
        "test_regular_product_demand_per_member = (test['base_FW_demand'] + test['base_AP_demand'] + test['base_EQ_demand']) * test_conv_rate\n",
        "test_clearance_demand_per_member = (test['clr_FW_demand'] + test['clr_AP_demand'] + test['clr_EQ_demand']) * test_conv_rate\n",
        "test_launch_demand_per_member = (test['launch_FW_demand'] + test['launch_AP_demand'] + test['launch_EQ_demand']) * test_conv_rate\n",
        "\n",
        "# Metrics for the scorecard\n",
        "scorecard_metrics = [\n",
        "    ('Audience Size', 'conv_sample_size'),\n",
        "    ('Demand per push sent', 'demand_per_push_sent'),\n",
        "    ('Conversion rate', 'conversion_rate'),\n",
        "    ('Demand per buyer', 'avg_demand'),\n",
        "    ('AOV', 'AOV'),\n",
        "    ('AUR', 'AUR'),\n",
        "    ('UPT', 'UPT'),\n",
        "    ('# Transactions per buyer', 'transactions_per_buyer'),\n",
        "    ('Members with 1 previous order', '1_priorOrder'),\n",
        "    ('Conversion rate (1x buyers/email receivers)', 'pctBuyers_1_priorOrder'),\n",
        "    ('Demand per Buyer (1 previous order)', 'DPB_1_prior'),\n",
        "    ('Members with 2 previous orders', '2_priorOrder'),\n",
        "    ('Conversion rate (2x buyers/email receivers)', 'pctBuyers_2_priorOrder'),\n",
        "    ('Demand per buyer (2 previous orders)', 'DPB_2_prior'),\n",
        "    ('Margin per buyer', 'avg_margin'),\n",
        "    ('Margin per email sent', 'margin_per_email_sent'),\n",
        "    ('Email open rate (against sends)', 'email_open_rate'),\n",
        "    ('Email click rate (against sends)', 'email_click_rate'),\n",
        "    ('Visits per known member', 'avg_site_app_visits'),\n",
        "    ('PDP favorite per known member', 'avg_PDP_FAVORITE_COUNT'),\n",
        "    ('Add to cart per known member', 'avg_ADD_TO_CART_COUNT'),\n",
        "    ('Workouts per known member', 'avg_physical_activity'),\n",
        "    ('Footwear demand per member', 'footwear_demand_per_member'),\n",
        "    ('Apparel demand per member', 'apparel_demand_per_member'),\n",
        "    ('Equipment demand per member', 'equipment_demand_per_member'),\n",
        "    ('Regular product demand per member', 'regular_product_demand_per_member'),\n",
        "    ('Clearance demand per member', 'clearance_demand_per_member'),\n",
        "    ('Launch demand per member', 'launch_demand_per_member')\n",
        "]\n",
        "\n",
        "\n",
        "# Create an empty DataFrame for the scorecard\n",
        "scorecard_df = pd.DataFrame(columns=['Metric', 'Control', 'Test', 'Lift', 'Confidence Level'])\n",
        "\n",
        "# Iterate through metrics to populate the scorecard\n",
        "for metric_name, variable_name in scorecard_metrics:\n",
        "    # Retrieve scalar values for control and test groups\n",
        "    control_value = locals().get(f'control_{variable_name}', control[variable_name].mean() if variable_name in control.columns else np.nan)\n",
        "    test_value = locals().get(f'test_{variable_name}', test[variable_name].mean() if variable_name in test.columns else np.nan)\n",
        "\n",
        "    # Ensure control_value and test_value are scalars to avoid ambiguous truth value error\n",
        "    control_value = control_value if np.isscalar(control_value) else control_value.mean()\n",
        "    test_value = test_value if np.isscalar(test_value) else test_value.mean()\n",
        "\n",
        "    # Calculate lift with a check to ensure control_value is not zero to avoid division by zero\n",
        "    lift = (test_value - control_value) / control_value * 100 if control_value != 0 else np.nan\n",
        "\n",
        "    # Determine the confidence level to display based on the metric\n",
        "    if metric_name in ['Conversion rate', 'Demand per buyer']:\n",
        "        formatted_confidence_level = f\"{confidence_level_two_tailed * 100:.2f}%\"\n",
        "    else:\n",
        "        formatted_confidence_level = \"\"  # Leave blank for metrics without a specific confidence level\n",
        "\n",
        "    # Append data to the scorecard DataFrame\n",
        "    scorecard_df = scorecard_df.append({\n",
        "        'Metric': metric_name,\n",
        "        'Control': f\"{control_value:.3f}\" if not pd.isna(control_value) else \"N/A\",\n",
        "        'Test': f\"{test_value:.3f}\" if not pd.isna(test_value) else \"N/A\",\n",
        "        'Lift': f\"{lift:.2f}%\" if not np.isnan(lift) else \"N/A\",\n",
        "        'Confidence Level': formatted_confidence_level\n",
        "    }, ignore_index=True)\n",
        "\n",
        "# Display the scorecard\n",
        "scorecard_df"
      ],
      "metadata": {
        "id": "UU1MKkEXGu7Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "806797be-ad1d-4497-816a-731520bc5d2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 392) (<ipython-input-2-bb93f39f4cb1>, line 392)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-bb93f39f4cb1>\"\u001b[0;36m, line \u001b[0;32m392\u001b[0m\n\u001b[0;31m    confidence_level = f\"{confidence_level_two_tailed*100:.2f}% # confidence_level_demand\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 392)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AGG Test"
      ],
      "metadata": {
        "id": "h7U33eFe4ANh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%python\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "## Extracting input parameters for Demand\n",
        "\n",
        "\n",
        "# Calculate average sales (demand) for control and test groups\n",
        "control_avg_demand = control['demand'].mean()\n",
        "test_avg_demand = test['demand'].mean()\n",
        "\n",
        "# Calculate standard deviation of demand for control and test groups\n",
        "control_std_dev = control['demand'].std()\n",
        "test_std_dev = test['demand'].std()\n",
        "\n",
        "# Calculate the # of buying members for the control and test groups\n",
        "control_sample_size = control[control['orders'] > 0]['upm_id'].nunique()\n",
        "test_sample_size = test[test['orders'] > 0]['upm_id'].nunique()\n",
        "\n",
        "\n",
        "## Perform Statistical Analysis for Demand\n",
        "\n",
        "\n",
        "## Performing the t-test and ignoring NaN values\n",
        "\n",
        "# Two-tailed t-test for Avg Demand\n",
        "t_stat_two_tailed, p_value_two_tailed = stats.ttest_ind(test['demand'].dropna(), control['demand'].dropna(), equal_var=False, nan_policy='omit')\n",
        "# Calculating confidence levels for two-tailed test\n",
        "confidence_level_two_tailed = 1 - p_value_two_tailed\n",
        "type_i_error_demand_two_tailed = p_value_two_tailed\n",
        "\n",
        "# One-tailed t-test for Avg Demand (assuming we are checking if test > control)\n",
        "\"\"\"\n",
        "Note:\n",
        "We halve the p-value and adjust its interpretation based on the direction of the t-statistic.\n",
        "\n",
        "The one-tailed test results are specifically meant for the hypothesis that the test group mean is greater than the control group mean.\n",
        "\"\"\"\n",
        "if t_stat_two_tailed > 0:\n",
        "    # Halve the p-value for positive t-statistic where test mean is hypothesized to be greater than control mean\n",
        "    p_value_one_tailed = p_value_two_tailed / 2\n",
        "    confidence_level_one_tailed = 1 - p_value_one_tailed\n",
        "    print(\"\\nOne-Tailed Test Results (Test > Control):\")\n",
        "    print(f\"T-statistic: {t_stat_two_tailed}\")  # Same T-statistic as the two-tailed test\n",
        "    print(f\"One-Tailed P-value: {p_value_one_tailed}\")\n",
        "    print(f\"Confidence Level: {confidence_level_one_tailed*100:.2f}%\")\n",
        "else:\n",
        "    # If the t-statistic is negative, indicating the test group mean is not greater than the control mean\n",
        "    print(\"\\nOne-Tailed Test Results (Test > Control):\")\n",
        "    print(f\"T-statistic: {t_stat_two_tailed}\")\n",
        "    print(\"The outcome does not support the hypothesized direction (test > control).\")\n",
        "    print(\"Interpretation of confidence levels or significance is not applicable for this outcome.\")\n",
        "\n",
        "# Output the results for two-tailed test\n",
        "print(\"\\nTwo-Tailed Test Results:\")\n",
        "print(f\"T-statistic: {t_stat_two_tailed}\")\n",
        "print(f\"P-value: {p_value_two_tailed}\")\n",
        "print(f\"Confidence Level: {confidence_level_two_tailed*100:.2f}%\")\n",
        "\n",
        "print(f\"Demand Sample Size - Control: {control_sample_size}\")\n",
        "print(f\"Demand Sample Size - Test: {test_sample_size}\")\n",
        "print(f\"Average Demand - Control: {control_avg_demand:.2f}\")\n",
        "print(f\"Average Demand - Test: {test_avg_demand:.2f}\")\n",
        "print(f\"Standard Deviation - Test: {test_std_dev:.2f}\")\n",
        "print(f\"Standard Deviation - Control: {control_std_dev:.2f}\")\n",
        "\n",
        "print(f\"T-statistic for Demand: {t_stat_two_tailed}\")\n",
        "print(f\"P-value for Demand: {p_value_two_tailed}\")\n",
        "print(f\"Confidence Level for Demand: {confidence_level_two_tailed*100:.2f}%\")\n",
        "\n",
        "\n",
        "## Extracting input parameters for Conversion\n",
        "\n",
        "\n",
        "# Total count of all unique members in the test and control groups\n",
        "control_conv_sample_size = control['upm_id'].nunique()\n",
        "test_conv_sample_size = test['upm_id'].nunique()\n",
        "\n",
        "# Calculate conversion rate for the test and control groups (total buying members / total members)\n",
        "control_conv_rate = control_sample_size / control_conv_sample_size if control_conv_sample_size > 0 else 0\n",
        "test_conv_rate = test_sample_size / test_conv_sample_size if test_conv_sample_size > 0 else 0\n",
        "\n",
        "# Calculate lift for the conversion rate\n",
        "lift = abs(test_conv_rate - control_conv_rate)\n",
        "\n",
        "# Calculate difference of proportions\n",
        "difference_of_proportions = test_conv_rate - control_conv_rate\n",
        "\n",
        "print(f\"Control Conversion Rate: {control_conv_rate*100:.2f}%\")\n",
        "print(f\"Test Conversion Rate: {test_conv_rate*100:.2f}%\")\n",
        "print(f\"Absolute Lift in Conversion Rate: {lift*100:.4f}%\")\n",
        "print(f\"Difference_of_proportions: {difference_of_proportions:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "## Perform Statistical Analysis for Conversion\n",
        "\n",
        "# Make sure these are not views but standalone DataFrames to avoid SettingWithCopyWarning\n",
        "control = control.copy()\n",
        "test = test.copy()\n",
        "# Add a 'buyer' column to both test and control DataFrames\n",
        "control.loc[:, 'buyer'] = (control['orders'] > 0).astype(int) # 'buyer' = 1 if 'orders' > 0, else 0\n",
        "test.loc[:, 'buyer'] = (test['orders'] > 0).astype(int)\n",
        "\n",
        "## Perform t-test for Conversion Rate\n",
        "t_stat_conversion, p_value_conversion = stats.ttest_ind(test['buyer'], control['buyer'], equal_var=False)\n",
        "\n",
        "# One-tailed test interpretation for Conversion Rate (assuming we are checking if test > control)\n",
        "if t_stat_conversion > 0:\n",
        "    \"\"\" Halve the p-value for positive t-statistic where test conversion rate is hypothesized to be greater than control\"\"\"\n",
        "    p_value_one_tailed_conversion = p_value_conversion / 2\n",
        "    confidence_level_one_tailed_conversion = 1 - p_value_one_tailed_conversion\n",
        "    print(\"\\nOne-Tailed Test Results (Test > Control) for Conversion Rate:\")\n",
        "    print(f\"T-statistic: {t_stat_conversion:.4f}\")\n",
        "    print(f\"One-Tailed P-value: {p_value_one_tailed_conversion:.4f}\")\n",
        "    print(f\"Confidence Level: {confidence_level_one_tailed_conversion*100:.2f}%\")\n",
        "else:\n",
        "    # If the t-statistic is negative, indicating the test group conversion rate is not greater than the control\n",
        "    print(\"\\nOne-Tailed Test Results (Test > Control) for Conversion Rate:\")\n",
        "    print(f\"T-statistic: {t_stat_conversion:.4f}\")\n",
        "    print(\"The outcome does not support the hypothesized direction (test > control) for conversion rate.\")\n",
        "    print(\"Interpretation of confidence levels or significance is not applicable for this outcome.\")\n",
        "\n",
        "\n",
        "## Secondary metrics ON order double click\n",
        "\n",
        "\n",
        "# AOV for test & control groups\n",
        "control_AOV = control['demand'].sum() / control['orders'].sum()\n",
        "test_AOV = test['demand'].sum() / test['orders'].sum()\n",
        "\n",
        "# AUR for test & control groups\n",
        "control_AUR = control['demand'].sum() / control['units'].sum()\n",
        "test_AUR = test['demand'].sum() / test['units'].sum()\n",
        "\n",
        "# UPT for test & control groups\n",
        "control_UPT = control['units'].sum() / control['orders'].sum()\n",
        "test_UPT = test['units'].sum() / test['orders'].sum()\n",
        "\n",
        "\n",
        "## Secondary metrics ON emails\n",
        "\n",
        "\n",
        "# Email Open Rate for test & control groups\n",
        "control_email_open_rate = control['email_opens'] / control['email_sent']\n",
        "test_email_open_rate = test['email_opens'] / test['email_sent']\n",
        "\n",
        "# Email Click Rate against sends for test & control groups\n",
        "control_email_click_rate = control['email_clicks'] / control['email_sent']\n",
        "test_email_click_rate = test['email_clicks'] / test['email_sent']\n",
        "\n",
        "# Email Click Rate against opens for test & control groups\n",
        "control_email_click_rate = control['email_clicks'] / control['email_opens']\n",
        "test_email_click_rate = test['email_clicks'] / test['email_opens']\n",
        "\n",
        "# Email Conversion Rate for test & control groups\n",
        "\n",
        "control_email_conversion_rate = control['email_purch'] / control['email_sent']\n",
        "test_email_conversion_rate = control['email_purch'] / control['email_sent']\n",
        "\n",
        "\n",
        "## Secondary metrics ON site/app engagement\n",
        "\n",
        "\n",
        "# Average Site/App Visits for test and control groups\n",
        "control_avg_site_app_visits = control['site_app_visits'].mean()\n",
        "test_avg_site_app_visits = test['site_app_visits'].mean()\n",
        "\n",
        "# Average PDP Favorite Count for test and control groups\n",
        "control_avg_site_app_visits = control['PDP_FAVORITE_COUNT'].mean()\n",
        "test_avg_site_app_visits = test['PDP_FAVORITE_COUNT'].mean()\n",
        "\n",
        "# Average Add to Cart Count for test and control groups\n",
        "control_avg_site_app_visits = control['ADD_TO_CART_COUNT'].mean()\n",
        "test_avg_site_app_visits = test['ADD_TO_CART_COUNT'].mean()\n",
        "\n",
        "# Average Add to Cart Count for test and control groups\n",
        "control_avg_site_app_visits = control['physical_activity'].mean()\n",
        "test_avg_site_app_visits = test['physical_activity'].mean()\n",
        "\n",
        "\n",
        "## Secondary metrics ON margin\n",
        "\n",
        "\n",
        "# Average Margin for test & control groups\n",
        "\n",
        "control_avg_margin = (\n",
        "    control['base_FW_demand'].mean() * 0.38 +\n",
        "    control['base_AP_demand'].mean() * 0.28 +\n",
        "    control['base_EQ_demand'].mean() * 0.32 +\n",
        "    control['clr_FW_demand'].mean() * 0.23 +\n",
        "    control['clr_AP_demand'].mean() * 0.17 +\n",
        "    control['clr_EQ_demand'].mean() * 0.12 +\n",
        "    control['launch_FW_demand'].mean() * 0.25 +\n",
        "    control['launch_AP_demand'].mean() * 0.08 +\n",
        "    control['launch_EQ_demand'].mean() * 0.19 +\n",
        "    control['rest_demand'].mean() * 0.3\n",
        ")\n",
        "\n",
        "test_avg_margin = (\n",
        "    test['base_FW_demand'].mean() * 0.38 +\n",
        "    test['base_AP_demand'].mean() * 0.28 +\n",
        "    test['base_EQ_demand'].mean() * 0.32 +\n",
        "    test['clr_FW_demand'].mean() * 0.23 +\n",
        "    test['clr_AP_demand'].mean() * 0.17 +\n",
        "    test['clr_EQ_demand'].mean() * 0.12 +\n",
        "    test['launch_FW_demand'].mean() * 0.25 +\n",
        "    test['launch_AP_demand'].mean() * 0.08 +\n",
        "    test['launch_EQ_demand'].mean() * 0.19 +\n",
        "    test['rest_demand'].mean() * 0.3\n",
        ")\n",
        "\n",
        "test_base_FW_demand = test['base_FW_demand'].mean()\n",
        "test_base_AP_demand = test['base_AP_demand'].mean()\n",
        "test_base_EQ_demand = test['base_EQ_demand'].mean()\n",
        "test_clr_FW_demand = test['clr_FW_demand'].mean()\n",
        "test_clr_AP_demand = test['clr_AP_demand'].mean()\n",
        "test_clr_EQ_demand = test['clr_EQ_demand'].mean()\n",
        "test_launch_FW_demand = test['launch_FW_demand'].mean()\n",
        "test_launch_AP_demand = test['launch_AP_demand'].mean()\n",
        "test_launch_EQ_demand = test['launch_EQ_demand'].mean()\n",
        "test_rest_demand = test['rest_demand'].mean()\n",
        "\n",
        "control_base_FW_demand = control['base_FW_demand'].mean()\n",
        "control_base_AP_demand = control['base_AP_demand'].mean()\n",
        "control_base_EQ_demand = control['base_EQ_demand'].mean()\n",
        "control_clr_FW_demand = control['clr_FW_demand'].mean()\n",
        "control_clr_AP_demand = control['clr_AP_demand'].mean()\n",
        "control_clr_EQ_demand = control['clr_EQ_demand'].mean()\n",
        "control_launch_FW_demand = control['launch_FW_demand'].mean()\n",
        "control_launch_AP_demand = control['launch_AP_demand'].mean()\n",
        "control_launch_EQ_demand = control['launch_EQ_demand'].mean()\n",
        "control_rest_demand = control['rest_demand'].mean()\n",
        "\n",
        "\n",
        "## Secondary metrics ON order history\n",
        "\n",
        "# For the control group\n",
        "\n",
        "# control_total_members = control['upm_id'].nunique()\n",
        "control_pct_buyers_1_priorOrder = (control['histOrders'] == 1).sum() / control_conv_sample_size\n",
        "control_DPB_1_prior = control[control['histOrders'] == 1]['demand'].sum() / (control['histOrders'] == 1).sum()\n",
        "control_pct_buyers_2_priorOrder = (control['histOrders'] == 2).sum() / control_conv_sample_size\n",
        "control_DPB_2_prior = control[control['histOrders'] == 2]['demand'].sum() / (control['histOrders'] == 2).sum()\n",
        "\n",
        "# For the test group\n",
        "\n",
        "# test_total_members = test['upm_id'].nunique()\n",
        "test_pct_buyers_1_priorOrder = (test['histOrders'] == 1).sum() / test_conv_sample_size\n",
        "test_DPB_1_prior = test[test['histOrders'] == 1]['demand'].sum() / (test['histOrders'] == 1).sum()\n",
        "test_pct_buyers_2_priorOrder = (test['histOrders'] == 2).sum() / test_conv_sample_size\n",
        "test_DPB_2_prior = test[test['histOrders'] == 2]['demand'].sum() / (test['histOrders'] == 2).sum()\n",
        "\n",
        "\n",
        "# Secondary KPI's for the control group\n",
        "\n",
        "control_demand_per_push_sent = control_conv_rate * control_avg_demand\n",
        "control_transactions_per_buyer = control_avg_demand / control_AOV\n",
        "control_margin_per_email_sent = control_avg_margin * control_conv_rate\n",
        "control_footwear_demand_per_member = (control['base_FW_demand'] + control['clr_FW_demand'] + control['launch_FW_demand']) * control_conv_rate\n",
        "control_apparel_demand_per_member = (control['base_AP_demand'] + control['clr_AP_demand'] + control['launch_AP_demand']) * control_conv_rate\n",
        "control_equipment_demand_per_member = (control['base_EQ_demand'] + control['clr_EQ_demand'] + control['launch_EQ_demand']) * control_conv_rate\n",
        "control_regular_product_demand_per_member = (control['base_FW_demand'] + control['base_AP_demand'] + control['base_EQ_demand']) * control_conv_rate\n",
        "control_clearance_demand_per_member = (control['clr_FW_demand'] + control['clr_AP_demand'] + control['clr_EQ_demand']) * control_conv_rate\n",
        "control_launch_demand_per_member = (control['launch_FW_demand'] + control['launch_AP_demand'] + control['launch_EQ_demand']) * control_conv_rate\n",
        "\n",
        "# Secondary KPI's for the test group\n",
        "\n",
        "test_demand_per_push_sent = test_conv_rate * test_avg_demand\n",
        "test_transactions_per_buyer = test_avg_demand / test_AOV\n",
        "test_margin_per_email_sent = test_avg_margin * test_conv_rate\n",
        "test_footwear_demand_per_member = (test['base_FW_demand'] + test['clr_FW_demand'] + test['launch_FW_demand']) * test_conv_rate\n",
        "test_apparel_demand_per_member = (test['base_AP_demand'] + test['clr_AP_demand'] + test['launch_AP_demand']) * test_conv_rate\n",
        "test_equipment_demand_per_member = (test['base_EQ_demand'] + test['clr_EQ_demand'] + test['launch_EQ_demand']) * test_conv_rate\n",
        "test_regular_product_demand_per_member = (test['base_FW_demand'] + test['base_AP_demand'] + test['base_EQ_demand']) * test_conv_rate\n",
        "test_clearance_demand_per_member = (test['clr_FW_demand'] + test['clr_AP_demand'] + test['clr_EQ_demand']) * test_conv_rate\n",
        "test_launch_demand_per_member = (test['launch_FW_demand'] + test['launch_AP_demand'] + test['launch_EQ_demand']) * test_conv_rate\n",
        "\n",
        "# Metrics for the scorecard\n",
        "scorecard_metrics = [\n",
        "    ('Audience Size', 'conv_sample_size'),\n",
        "    ('Demand per push sent', 'demand_per_push_sent'),\n",
        "    ('Conversion rate', 'conversion_rate'),\n",
        "    ('Demand per buyer', 'avg_demand'),\n",
        "    ('AOV', 'AOV'),\n",
        "    ('AUR', 'AUR'),\n",
        "    ('UPT', 'UPT'),\n",
        "    ('# Transactions per buyer', 'transactions_per_buyer'),\n",
        "    ('Members with 1 previous order', '1_priorOrder'),\n",
        "    ('Conversion rate (1x buyers/email receivers)', 'pctBuyers_1_priorOrder'),\n",
        "    ('Demand per Buyer (1 previous order)', 'DPB_1_prior'),\n",
        "    ('Members with 2 previous orders', '2_priorOrder'),\n",
        "    ('Conversion rate (2x buyers/email receivers)', 'pctBuyers_2_priorOrder'),\n",
        "    ('Demand per buyer (2 previous orders)', 'DPB_2_prior'),\n",
        "    ('Margin per buyer', 'avg_margin'),\n",
        "    ('Margin per email sent', 'margin_per_email_sent'),\n",
        "    ('Email open rate (against sends)', 'email_open_rate'),\n",
        "    ('Email click rate (against sends)', 'email_click_rate'),\n",
        "    ('Visits per known member', 'avg_site_app_visits'),\n",
        "    ('PDP favorite per known member', 'avg_PDP_FAVORITE_COUNT'),\n",
        "    ('Add to cart per known member', 'avg_ADD_TO_CART_COUNT'),\n",
        "    ('Workouts per known member', 'avg_physical_activity'),\n",
        "    ('Footwear demand per member', 'footwear_demand_per_member'),\n",
        "    ('Apparel demand per member', 'apparel_demand_per_member'),\n",
        "    ('Equipment demand per member', 'equipment_demand_per_member'),\n",
        "    ('Regular product demand per member', 'regular_product_demand_per_member'),\n",
        "    ('Clearance demand per member', 'clearance_demand_per_member'),\n",
        "    ('Launch demand per member', 'launch_demand_per_member')\n",
        "]\n",
        "\n",
        "\n",
        "# Create an empty DataFrame for the scorecard\n",
        "scorecard_df = pd.DataFrame(columns=['Metric', 'Control', 'Test', 'Lift', 'Confidence Level'])\n",
        "\n",
        "# Iterate through metrics to populate the scorecard\n",
        "for metric_name, variable_name in scorecard_metrics:\n",
        "    # Retrieve scalar values for control and test groups\n",
        "    control_value = locals().get(f'control_{variable_name}', control[variable_name].mean() if variable_name in control.columns else np.nan)\n",
        "    test_value = locals().get(f'test_{variable_name}', test[variable_name].mean() if variable_name in test.columns else np.nan)\n",
        "\n",
        "    # Ensure control_value and test_value are scalars to avoid ambiguous truth value error\n",
        "    control_value = control_value if np.isscalar(control_value) else control_value.mean()\n",
        "    test_value = test_value if np.isscalar(test_value) else test_value.mean()\n",
        "\n",
        "    # Calculate lift with a check to ensure control_value is not zero to avoid division by zero\n",
        "    lift = (test_value - control_value) / control_value * 100 if control_value != 0 else np.nan\n",
        "\n",
        "    # Determine the confidence level to display based on the metric\n",
        "    if metric_name in ['Conversion rate', 'Demand per buyer']:\n",
        "        formatted_confidence_level = f\"{confidence_level_two_tailed * 100:.2f}%\"\n",
        "    else:\n",
        "        formatted_confidence_level = \"\"  # Leave blank for metrics without a specific confidence level\n",
        "\n",
        "    # Append data to the scorecard DataFrame\n",
        "    scorecard_df = scorecard_df.append({\n",
        "        'Metric': metric_name,\n",
        "        'Control': f\"{control_value:.3f}\" if not pd.isna(control_value) else \"N/A\",\n",
        "        'Test': f\"{test_value:.3f}\" if not pd.isna(test_value) else \"N/A\",\n",
        "        'Lift': f\"{lift:.2f}%\" if not np.isnan(lift) else \"N/A\",\n",
        "        'Confidence Level': formatted_confidence_level\n",
        "    }, ignore_index=True)\n",
        "\n",
        "# Display the scorecard\n",
        "scorecard_df"
      ],
      "metadata": {
        "id": "plyNhfys31EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ERROR"
      ],
      "metadata": {
        "id": "raKAZNGVOkIq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create an empty DataFrame for the scorecard\n",
        "scorecard_df = pd.DataFrame(columns=['Metric', 'Control', 'Test', 'Lift', 'Confidence Level'])\n",
        "\n",
        "# Iterate through metrics to populate the scorecard\n",
        "for metric_name, variable_name in scorecard_metrics:\n",
        "    # Determine the appropriate aggregation function\n",
        "    if variable_name in ['1_priorOrder', '2_priorOrder']:\n",
        "        control_agg_func = 'sum'\n",
        "        test_agg_func = 'sum'\n",
        "    else:\n",
        "        control_agg_func = 'mean'\n",
        "        test_agg_func = 'mean'\n",
        "\n",
        "    # Retrieve scalar values for control and test groups\n",
        "    control_value = locals().get(f'control_{variable_name}', control[variable_name].agg(control_agg_func) if variable_name in control.columns else np.nan)\n",
        "    test_value = locals().get(f'test_{variable_name}', test[variable_name].agg(test_agg_func) if variable_name in test.columns else np.nan)\n",
        "\n",
        "    # Calculate lift, ensuring control_value is not zero to avoid division by zero\n",
        "    lift = (test_value - control_value) / control_value * 100 if control_value != 0 else np.nan\n",
        "\n",
        "    # Determine the confidence level to display based on the metric\n",
        "    formatted_confidence_level = \"\"\n",
        "    if variable_name == 'conversion_rate':\n",
        "        confidence_level_variable = 'confidence_level_one_tailed_conversion' if 'confidence_level_one_tailed_conversion' in locals() else 'confidence_level_two_tailed_conversion'\n",
        "    elif variable_name == 'avg_demand':\n",
        "        confidence_level_variable = 'confidence_level_one_tailed_demand' if 'confidence_level_one_tailed_demand' in locals() else 'confidence_level_two_tailed_demand'\n",
        "    \n",
        "    confidence_level = locals().get(confidence_level_variable, 0)\n",
        "    formatted_confidence_level = f\"{confidence_level:.2f}%\" if confidence_level else \"\"\n",
        "\n",
        "    # Append data to the scorecard DataFrame\n",
        "    scorecard_df = scorecard_df.append({\n",
        "        'Metric': metric_name,\n",
        "        'Control': f\"{control_value:.3f}\" if not pd.isna(control_value) else \"N/A\",\n",
        "        'Test': f\"{test_value:.3f}\" if not pd.isna(test_value) else \"N/A\",\n",
        "        'Lift': f\"{lift:.2f}%\" if not np.isnan(lift) else \"N/A\",\n",
        "        'Confidence Level': formatted_confidence_level\n",
        "    }, ignore_index=True)\n",
        "\n",
        "# Display the scorecard\n",
        "print(scorecard_df)\n"
      ],
      "metadata": {
        "id": "jXicMhASDagM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adding Confidence Level to dataframe"
      ],
      "metadata": {
        "id": "5YAXX7TMoYtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty DataFrame for the scorecard\n",
        "scorecard_df = pd.DataFrame(columns=['Metric', 'Control', 'Test', 'Lift', 'Confidence Level'])\n",
        "\n",
        "# Populating the DataFrame with calculated values\n",
        "metrics = {\n",
        "    'Demand per buyer': ('demand', confidence_level_demand),\n",
        "    'Conversion rate': ('conversion', confidence_level_conversion)\n",
        "}\n",
        "\n",
        "for metric_name, (variable_suffix, confidence_level) in metrics.items():\n",
        "    control_var = 'control_' + variable_suffix\n",
        "    test_var = 'test_' + variable_suffix\n",
        "\n",
        "    control_value = locals()[control_var] if control_var in locals() else control[variable_suffix].mean()\n",
        "    test_value = locals()[test_var] if test_var in locals() else test[variable_suffix].mean()\n",
        "\n",
        "    # Calculate lift directly based on the metric\n",
        "    if metric_name == 'Conversion rate':\n",
        "        lift = abs(test_conv_rate - control_conv_rate)\n",
        "    else:\n",
        "        # If you have a specific way to calculate lift for other metrics, apply here\n",
        "        # For demonstration, using a simple difference for demand, or adjust as needed\n",
        "        lift = test_value - control_value\n",
        "\n",
        "    # Appending values to DataFrame\n",
        "    scorecard_df = scorecard_df.append({\n",
        "        'Metric': metric_name,\n",
        "        'Control': control_value,\n",
        "        'Test': test_value,\n",
        "        'Lift': lift,\n",
        "        'Confidence Level': confidence_level\n",
        "    }, ignore_index=True)\n",
        "\n",
        "# Function to format values\n",
        "def format_values(value, metric_name):\n",
        "    if metric_name in ['Conversion rate', 'Lift', 'Confidence Level']:\n",
        "        return f\"{value * 100:.2f}%\" if isinstance(value, float) else value  # Format as percentage\n",
        "    else:\n",
        "        return f\"{value:.2f}\" if isinstance(value, float) else value  # Format other values to two decimal places\n",
        "\n",
        "# Applying formatting to the scorecard\n",
        "for col in ['Control', 'Test', 'Lift', 'Confidence Level']:\n",
        "    scorecard_df[col] = scorecard_df.apply(lambda row: format_values(row[col], row['Metric']), axis=1)\n",
        "\n",
        "# Display the Formatted Scorecard DataFrame\n",
        "scorecard_df\n"
      ],
      "metadata": {
        "id": "uC4ZdNWtodVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Old Scorecard Code"
      ],
      "metadata": {
        "id": "jzX_IDqW9ogV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty DataFrame for the scorecard\n",
        "scorecard_df = pd.DataFrame(columns=['Metric', 'Control', 'Test', 'Lift', 'Confidence Level'])\n",
        "\n",
        "\n",
        "# Populating the DataFrame with calculated values\n",
        "for metric_name, variable_name in scorecard_metrics:\n",
        "    control_value = locals().get('control_' + variable_name) if 'control_' + variable_name in locals() else control[variable_name].mean()\n",
        "    test_value = locals().get('test_' + variable_name) if 'test_' + variable_name in locals() else test[variable_name].mean()\n",
        "    # lift = calculate_lift(control_value, test_value)\n",
        "    lift = abs(test_value - ontrol_value)\n",
        "    significance = calculate_significance(control, test, variable_name) # confidence_level_demand\n",
        "\n",
        "    scorecard_df = scorecard_df.append({'Metric': metric_name, 'Control': control_value, 'Test': test_value, 'Lift': lift, 'Confidence Level': confidence_level_demand}, ignore_index=True)\n",
        "\n",
        "# Function to format values\n",
        "def format_values(value, metric_name):\n",
        "    if metric_name in ['Conversion rate', 'Lift', 'Confidence Level' ]:\n",
        "        return f\"{value * 100:.3f}%\" if isinstance(value, float) else value  # Format as percentage\n",
        "    else:\n",
        "        return f\"{value:.3f}\" if isinstance(value, float) else value  # Format other values to three decimal places\n",
        "\n",
        "# Applying formatting to the scorecard\n",
        "for col in ['Control', 'Test', 'Lift', 'Confidence Level']:\n",
        "    scorecard_df[col] = scorecard_df.apply(lambda row: format_values(row[col], row['Metric']), axis=1)\n",
        "\n",
        "# Display the Formatted Scorecard DataFrame\n",
        "scorecard_df\n"
      ],
      "metadata": {
        "id": "PpMEd7N3-5qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Old Scorecard Code - V2"
      ],
      "metadata": {
        "id": "wZ9ueCCfBMxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty DataFrame for the scorecard\n",
        "scorecard_df = pd.DataFrame(columns=['Metric', 'Control', 'Test', 'Lift', 'Confidence Level'])\n",
        "\n",
        "# Iterate through the metrics\n",
        "for metric_name, variable_name in scorecard_metrics:\n",
        "    control_value = locals().get('control_' + variable_name, control[variable_name].mean())\n",
        "    test_value = locals().get('test_' + variable_name, test[variable_name].mean())\n",
        "    lift = abs(test_value - control_value)\n",
        "\n",
        "    # Only apply confidence level to conversion and demand\n",
        "    if variable_name in ['conversion_rate', 'avg_demand']:\n",
        "        # Use the pre-calculated 'confidence_level_two_tailed' for conversion rate and avg demand\n",
        "        formatted_confidence_level = f\"{confidence_level_two_tailed*100:.2f}%\"  # Format the confidence level as percentage\n",
        "    else:\n",
        "        formatted_confidence_level =  \"\"  # Leave blank for other metrics\n",
        "\n",
        "    # Append row to DataFrame\n",
        "    scorecard_df = scorecard_df.append({\n",
        "        'Metric': metric_name,\n",
        "        'Control': control_value,\n",
        "        'Test': test_value,\n",
        "        'Lift': lift,  # Use the global 'lift' value for all metrics\n",
        "        'Confidence Level': formatted_confidence_level  # Conditionally added\n",
        "    }, ignore_index=True)\n",
        "\n",
        "# Apply formatting to values\n",
        "def format_values(value, metric_name):\n",
        "    # Assuming this function correctly formats the values based on metric name\n",
        "    if metric_name in ['Conversion rate', 'Lift', 'Confidence Level'] and value != \"N/A\":\n",
        "        return f\"{float(value):.3f}%\"  # Format as percentage if applicable and not \"N/A\"\n",
        "    return value  # Return unmodified value for \"N/A\" or non-floats\n",
        "\n",
        "for col in ['Control', 'Test', 'Lift', 'Confidence Level']:\n",
        "    scorecard_df[col] = scorecard_df.apply(lambda row: format_values(row[col], row['Metric']), axis=1)\n",
        "\n",
        "# Display the formatted scorecard DataFrame\n",
        "print(scorecard_df)"
      ],
      "metadata": {
        "id": "ELvVYb8ABQEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Old Scorecard Code V3"
      ],
      "metadata": {
        "id": "o3OaUIlZSFWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'control' and 'test' DataFrames are already defined with necessary columns\n",
        "\n",
        "# Define your metrics configuration\n",
        "scorecard_metrics = [\n",
        "    # Define your metrics here\n",
        "    # Example: ('Metric Name', 'column_name_in_df'),\n",
        "]\n",
        "\n",
        "# Initialize scorecard DataFrame\n",
        "scorecard_df = pd.DataFrame(columns=['Metric', 'Control', 'Test', 'Lift', 'Confidence Level'])\n",
        "\n",
        "# Iterate through each metric to populate the scorecard\n",
        "for metric_name, variable_name in scorecard_metrics:\n",
        "    # Retrieve or calculate metric values for control and test groups\n",
        "    control_value = control[variable_name].mean() if variable_name in control else 0\n",
        "    test_value = test[variable_name].mean() if variable_name in test else 0\n",
        "\n",
        "    # Calculate lift, handling division by zero\n",
        "    lift = ((test_value - control_value) / control_value * 100) if control_value != 0 else \"N/A\"\n",
        "\n",
        "    # Determine if the metric is eligible for confidence level display\n",
        "    if variable_name in ['conversion_rate', 'avg_demand']:\n",
        "        formatted_confidence_level = f\"{confidence_level_two_tailed * 100:.2f}%\"\n",
        "    else:\n",
        "        formatted_confidence_level = \"\"  # Leave blank for metrics without a specific confidence level\n",
        "\n",
        "    # Append the metric's data to the scorecard\n",
        "    scorecard_df = scorecard_df.append({\n",
        "        'Metric': metric_name,\n",
        "        'Control': f\"{control_value:.3f}\",\n",
        "        'Test': f\"{test_value:.3f}\",\n",
        "        'Lift': f\"{lift}%\" if lift != \"N/A\" else lift,\n",
        "        'Confidence Level': formatted_confidence_level\n",
        "    }, ignore_index=True)\n",
        "\n",
        "# Display the scorecard\n",
        "print(scorecard_df)\n"
      ],
      "metadata": {
        "id": "JfgICipXSEoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Old Scorecard V4"
      ],
      "metadata": {
        "id": "NrlE71_aW_oT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty DataFrame for the scorecard\n",
        "scorecard_df = pd.DataFrame(columns=['Metric', 'Control', 'Test', 'Lift', 'Confidence Level'])\n",
        "\n",
        "# Iterate through metrics to populate the scorecard\n",
        "for metric_name, variable_name in scorecard_metrics:\n",
        "    # Retrieve scalar values for control and test groups\n",
        "    control_value = locals().get(f'control_{variable_name}', control[variable_name].mean() if variable_name in control.columns else np.nan)\n",
        "    test_value = locals().get(f'test_{variable_name}', test[variable_name].mean() if variable_name in test.columns else np.nan)\n",
        "\n",
        "    # Ensure control_value and test_value are scalars to avoid ambiguous truth value error\n",
        "    control_value = control_value if np.isscalar(control_value) else control_value.mean()\n",
        "    test_value = test_value if np.isscalar(test_value) else test_value.mean()\n",
        "\n",
        "    # Calculate lift with a check to ensure control_value is not zero to avoid division by zero\n",
        "    lift = (test_value - control_value) / control_value * 100 if control_value != 0 else np.nan\n",
        "\n",
        "    # Determine the confidence level to display based on the metric\n",
        "    if metric_name in ['Conversion rate', 'Demand per buyer']:\n",
        "        formatted_confidence_level = f\"{confidence_level_two_tailed * 100:.2f}%\"\n",
        "    else:\n",
        "        formatted_confidence_level = \"\"  # Leave blank for metrics without a specific confidence level\n",
        "\n",
        "    # Append data to the scorecard DataFrame\n",
        "    scorecard_df = scorecard_df.append({\n",
        "        'Metric': metric_name,\n",
        "        'Control': f\"{control_value:.3f}\" if not pd.isna(control_value) else \"N/A\",\n",
        "        'Test': f\"{test_value:.3f}\" if not pd.isna(test_value) else \"N/A\",\n",
        "        'Lift': f\"{lift:.2f}%\" if not np.isnan(lift) else \"N/A\",\n",
        "        'Confidence Level': formatted_confidence_level\n",
        "    }, ignore_index=True)\n",
        "\n",
        "# Display the scorecard\n",
        "scorecard_df\n",
        "\n",
        "\n",
        "\"\"\" ALSO this update \"\"\"\n",
        "\n",
        "# Create an empty DataFrame for the scorecard\n",
        "scorecard_df = pd.DataFrame(columns=['Metric', 'Control', 'Test', 'Lift', 'Confidence Level'])\n",
        "\n",
        "# Iterate through metrics to populate the scorecard\n",
        "for metric_name, variable_name in scorecard_metrics:\n",
        "    # Check if the metric needs sum aggregation instead of mean\n",
        "    if variable_name in ['1_priorOrder', '2_priorOrder']:\n",
        "        control_agg_func = 'sum'\n",
        "        test_agg_func = 'sum'\n",
        "    else:\n",
        "        control_agg_func = 'mean'\n",
        "        test_agg_func = 'mean'\n",
        "\n",
        "    # Retrieve or calculate metric values for control and test groups with appropriate aggregation\n",
        "    control_value = locals().get(f'control_{variable_name}', control[variable_name].agg(control_agg_func) if variable_name in control.columns else np.nan)\n",
        "    test_value = locals().get(f'test_{variable_name}', test[variable_name].agg(test_agg_func) if variable_name in test.columns else np.nan)\n",
        "\n",
        "    # Calculate lift with a check to ensure control_value is not zero to avoid division by zero\n",
        "    lift = (test_value - control_value) / control_value * 100 if control_value != 0 else np.nan\n",
        "\n",
        "    # Determine the confidence level to display based on the metric\n",
        "    confidence_level_variable = ''\n",
        "    if variable_name == 'conversion_rate':\n",
        "        confidence_level_variable = 'confidence_level_one_tailed_conversion' if 'confidence_level_one_tailed_conversion' in locals() else 'confidence_level_two_tailed_conversion'\n",
        "    elif variable_name == 'avg_demand':\n",
        "        confidence_level_variable = 'confidence_level_one_tailed_demand' if 'confidence_level_one_tailed_demand' in locals() else 'confidence_level_two_tailed_demand'\n",
        "    confidence_level = locals().get(confidence_level_variable, 0)  # Ensure default value is 0\n",
        "\n",
        "    formatted_confidence_level = f\"{confidence_level * 100:.2f}%\" if confidence_level else \"\"\n",
        "\n",
        "    # Append data to the scorecard DataFrame\n",
        "    scorecard_df = scorecard_df.append({\n",
        "        'Metric': metric_name,\n",
        "        'Control': f\"{control_value:.3f}\" if not pd.isna(control_value) else \"N/A\",\n",
        "        'Test': f\"{test_value:.3f}\" if not pd.isna(test_value) else \"N/A\",\n",
        "        'Lift': f\"{lift:.2f}%\" if not np.isnan(lift) else \"N/A\",\n",
        "        'Confidence Level': formatted_confidence_level\n",
        "    }, ignore_index=True)\n",
        "\n",
        "# Display the scorecard\n",
        "print(scorecard_df)\n",
        "\n",
        "\n",
        "\"\"\" THIS Update too \"\"\"\n",
        "\n",
        "# Create an empty DataFrame for the scorecard\n",
        "scorecard_df = pd.DataFrame(columns=['Metric', 'Control', 'Test', 'Lift', 'Confidence Level'])\n",
        "\n",
        "# Iterate through metrics to populate the scorecard\n",
        "for metric_name, variable_name in scorecard_metrics:\n",
        "    # Attempt to retrieve pre-calculated scalar values for control and test groups or calculate mean\n",
        "    control_value = locals().get(f'control_{variable_name}', control[variable_name].mean() if variable_name in control.columns else np.nan)\n",
        "    test_value = locals().get(f'test_{variable_name}', test[variable_name].mean() if variable_name in test.columns else np.nan)\n",
        "\n",
        "    # Calculate lift, ensuring control_value is not zero to avoid division by zero\n",
        "    lift = \"N/A\" if control_value == 0 else (test_value - control_value) / control_value * 100\n",
        "\n",
        "    # Determine the confidence level to display based on the metric\n",
        "    if metric_name == 'Conversion rate':\n",
        "        formatted_confidence_level = f\"{locals().get('confidence_level_two_tailed_conversion', 0) * 100:.2f}%\" if 'confidence_level_two_tailed_conversion' in locals() else \"\"\n",
        "    elif metric_name == 'Demand per buyer':\n",
        "        formatted_confidence_level = f\"{locals().get('confidence_level_two_tailed_demand', 0) * 100:.2f}%\" if 'confidence_level_two_tailed_demand' in locals() else \"\"\n",
        "    else:\n",
        "        formatted_confidence_level = \"\"  # Leave blank for metrics without a specific confidence level\n",
        "\n",
        "    # Append data to the scorecard DataFrame\n",
        "    scorecard_df = scorecard_df.append({\n",
        "        'Metric': metric_name,\n",
        "        'Control': f\"{control_value:.3f}\" if not pd.isna(control_value) else \"N/A\",\n",
        "        'Test': f\"{test_value:.3f}\" if not pd.isna(test_value) else \"N/A\",\n",
        "        'Lift': f\"{lift:.2f}%\" if lift != \"N/A\" else lift,\n",
        "        'Confidence Level': formatted_confidence_level\n",
        "    }, ignore_index=True)\n",
        "\n",
        "# Display the scorecard\n",
        "print(scorecard_df)\n"
      ],
      "metadata": {
        "id": "lbTJrBURXB_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notes\n",
        "\n",
        "* The Excel spreadsheet managed by the teamm has a '**Statistical Signifigance**' column, which purports to be reporting **1−α**.\n",
        "> Except this column is actually using the Excel formula =NORMSDIST(z-value). Is this formula actually reporting the *p-value*?\n",
        "* There is also a column that purports to be reporting **α**.\n",
        "> Except this column is actually using the Excel formula = 1 - NORMSDIST(z-value). Is this formula actually reporting **1−α** ?\n",
        "\n",
        "The Excel sheet seems to have reversed the p-value and confidence level columns, or am I wrong?\n",
        "\n",
        "\n",
        "#### Confirming the mix up\n",
        "\n",
        "Directly equating the CDF with \"Achieved Confidence Level\" and `1−CDF` with \"Type 1 Error\" is a misinterpretation.\n",
        "\n",
        "The correct approach would be to use the p-value derived from the test statistic (z-value in this case) to compare against **α** to make a decision about the null hypothesis.\n",
        "\n",
        "The *Achieved Confidence Level* could be thought of as\n",
        "`1−p-value`in a loose sense when you're interpreting how confident you are that the observed effect is not due to random chance, but this terminology isn't standard.\n",
        "\n",
        "p"
      ],
      "metadata": {
        "id": "BNEhBKG3FsYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AB Scorecard V6"
      ],
      "metadata": {
        "id": "obLQ5vKQiXIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Full AB SQL"
      ],
      "metadata": {
        "id": "1VhxPp2Ki3Wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%python\n",
        "\n",
        "# Execute SQL Query\n",
        "\n",
        "df_spark = spark.sql(\"\"\"\n",
        "\n",
        "-----------------------------------------------------------------------------------------------------------------------------------\n",
        "-- AB Period\n",
        "-----------------------------------------------------------------------------------------------------------------------------------\n",
        "WITH\n",
        "\n",
        "tc_first_last_send AS (\n",
        "    SELECT *\n",
        "    FROM (\n",
        "         (SELECT distinct tst.upm_id\n",
        "          , 'test' as test_control\n",
        "          , MIN(LEFT(message_sent_ts,10)) AS campaign_send_dt\n",
        "          , MIN(LEFT(message_sent_ts,21)) AS campaign_send_ts\n",
        "          , MIN(LEFT(message_sent_ts,10)) as first_send_dt\n",
        "          , MAX(LEFT(message_sent_ts,10)) as last_send_dt\n",
        "          FROM ${hivevar:aud_table_tst} tst\n",
        "          INNER JOIN comms.fact_email_delivery fed ON tst.upm_id = fed.upm_id AND campaign_geo_name = 'NA' AND campaign_cd LIKE ${hivevar:camp_cd_tst_str} AND CAST(message_sent_ts AS DATE) BETWEEN ${hivevar:msmt_start_dt} AND ${hivevar:msmt_end_dt}\n",
        "          GROUP BY 1,2\n",
        "          )\n",
        "      UNION\n",
        "        (SELECT distinct upm_id\n",
        "         , 'control' as test_control\n",
        "         , ${hivevar:msmt_start_dt} AS campaign_send_dt\n",
        "         , to_timestamp(${hivevar:msmt_start_dt}, 'yyyy-MM-dd') AS campaign_send_ts\n",
        "         , ${hivevar:msmt_start_dt} as first_send_dt\n",
        "         , ${hivevar:msmt_start_dt} as last_send_dt\n",
        "         from ${hivevar:aud_table_ctl}\n",
        "         )\n",
        "    )\n",
        ")\n",
        "\n",
        ", test_control_table AS (\n",
        "    SELECT *\n",
        "    , LEAD(a.campaign_send_dt) OVER (PARTITION BY a.upm_id ORDER BY a.campaign_send_dt) AS next_send_dt\n",
        "    , LAG(a.campaign_send_dt) OVER (PARTITION BY a.upm_id ORDER BY a.campaign_send_dt) AS previous_send_dt\n",
        "    , (int(to_timestamp(a.campaign_send_dt)) - int(to_timestamp(LEAD(a.campaign_send_dt) OVER (PARTITION BY a.upm_id ORDER BY a.campaign_send_dt))))/86400 AS difference_next_send_days\n",
        "    , (int(to_timestamp(a.campaign_send_dt)) - int(to_timestamp(LAG(a.campaign_send_dt) OVER (PARTITION BY a.upm_id ORDER BY a.campaign_send_dt))))/86400 AS difference_previous_send_days\n",
        "    FROM tc_first_last_send a\n",
        "    ANTI JOIN gwan13.bot_master_nike_com bot ON a.upm_id = bot.upm_id\n",
        "    ANTI JOIN aud_select_workspace.reseller_new slr ON a.upm_id = slr.upm_user_id\n",
        ")\n",
        "\n",
        ", view_dol AS (\n",
        "  SELECT dol.upm_id\n",
        "  , order_nbr\n",
        "  , order_dt\n",
        "  , origl_ordered_qty\n",
        "  , grd_amt_excl_tax_usd\n",
        "  , line_of_business_desc\n",
        "  , univ_div_desc\n",
        "  , first_send_dt\n",
        "  , last_send_dt\n",
        "  FROM DTC_INTEGRATED.DTC_DIGITAL_ORDER_LINE dol\n",
        "  INNER JOIN test_control_table tct ON dol.upm_id = tct.upm_id\n",
        "  WHERE region_key = 1\n",
        "    AND rec_excl_ind = 0\n",
        "    AND ttl_demand_ind = 1\n",
        "    AND univ_cat_desc <> 'Converse'\n",
        "    AND dol.upm_id IS NOT NULL\n",
        "    AND CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,730) AND DATE_ADD(last_send_dt,7)\n",
        "    AND (rtn_qty = 0 or rtn_qty IS NULL)\n",
        ")\n",
        "\n",
        "-- AA period purchase. dynamically based on when the member receives their first communication. Only returns demand for members in the\n",
        ", aa_purchase AS (\n",
        "  SELECT dol.upm_id\n",
        "    ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "    ,COUNT(DISTINCT order_nbr) AS orders\n",
        "    ,SUM(origl_ordered_qty) AS units\n",
        "  FROM view_dol dol\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,31) AND DATE_SUB(first_send_dt,1)\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", demand_bucket AS (\n",
        "  SELECT tct.upm_id\n",
        "  , tct.test_control\n",
        "  , aa.upm_id as upm_id_aa_buyer\n",
        "  , aa.demand\n",
        "  , aa.orders\n",
        "  , aa.units\n",
        "  , CASE\n",
        "    WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "    WHEN aa.demand < 50 THEN '000-050'\n",
        "    WHEN aa.demand < 100 THEN '050-100'\n",
        "    WHEN aa.demand < 200 THEN '100-200'\n",
        "    WHEN aa.demand < 500 THEN '200-500'\n",
        "    WHEN aa.demand < 1000 THEN '500-1000'\n",
        "    WHEN aa.demand >=1000 THEN '1000+'\n",
        "    END AS demand_per_buyer_bucket\n",
        "  , PERCENT_RANK() OVER (\n",
        "    PARTITION BY test_control\n",
        "    , CASE\n",
        "      WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "      WHEN aa.demand < 50 THEN '000-050'\n",
        "      WHEN aa.demand < 100 THEN '050-100'\n",
        "      WHEN aa.demand < 200 THEN '100-200'\n",
        "      WHEN aa.demand < 500 THEN '200-500'\n",
        "      WHEN aa.demand < 1000 THEN '500-1000'\n",
        "      WHEN aa.demand >=1000 THEN '1000+'\n",
        "      END ORDER BY RANDOM(0)\n",
        "    ) AS percent_rank\n",
        "  FROM test_control_table tct\n",
        "  LEFT JOIN aa_purchase aa ON tct.upm_id = aa.upm_id\n",
        ")\n",
        "\n",
        ", balanced_aud_aa as (\n",
        "  SELECT DISTINCT upm_id\n",
        "  FROM demand_bucket db\n",
        "  -- remove outliers\n",
        "  WHERE (demand < 2400 OR demand IS NULL)\n",
        "-- To rebalance, paste text block from template in place of defaults\n",
        "AND ((test_control = 'control' and (demand > 0 and demand <= 50) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand > 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'control' and (demand = 0 or demand is null) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand >0 and demand <= 50) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand = 0 or demand is null) and PERCENT_RANK <= 1)\n",
        "\t)\n",
        ")\n",
        "\n",
        ", mhub as (\n",
        " SELECT source_id as upm_id\n",
        " , member_id\n",
        " , preferred_gender\n",
        " FROM MEMBER.MEMBER_HUB\n",
        " WHERE ctry_2_cd = 'US'\n",
        " AND lower(dpa_status) IN ('activate','reactivate')\n",
        ")\n",
        "\n",
        "-- calculate a dynamic member activity window for each communication received by a member\n",
        "-- activity is calculated on a 7 day window, unless the member receives a communication within that timeframe\n",
        "-- When multiple messages are received, activity is only calculated up to the next message_send_dt to avoid double counting\n",
        ", member_activities as (\n",
        "  SELECT mgd.member_id\n",
        "  ,SUM(logged_in_visits_count) AS visits\n",
        "  ,SUM(pdp_favorite_count) AS pdp_favorite_count\n",
        "  ,SUM(add_to_cart_count) AS add_to_cart_count\n",
        "  ,SUM(physical_activity_count) AS physical_activity\n",
        "  ,MAX(CASE WHEN new_user_experience_flag = 'Y' THEN 1 ELSE 0 END) AS new_user_flag\n",
        "  FROM test_control_table tct\n",
        "  INNER JOIN mhub mh ON tct.upm_id = mh.upm_id\n",
        "  LEFT JOIN aud_select_workspace.member_agg_member_growth_daily mgd ON mh.member_id = mgd.member_id\n",
        "  WHERE mgd.activity_dt >= tct.campaign_send_dt\n",
        "    AND mgd.activity_dt < DATE_ADD(campaign_send_dt, INT(LEAST(7,COALESCE(difference_next_send_days,7))))\n",
        "    AND mgd.preferred_retail_geo = 'NA'\n",
        "    AND mgd.experience_name != 'agnostic'\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        "-- for each user, aggregate each order placed within our measurement window. create labels to categorize the type of demand\n",
        ", ab_purchase as (\n",
        "  SELECT dol.upm_id as upm_id_ab_buyer\n",
        "  , order_dt as order_ts\n",
        "  , order_nbr\n",
        "  ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc NOT IN ('Base Inline', 'Clearance', 'Launch') THEN grd_amt_excl_tax_usd ELSE 0 END) AS rest_demand\n",
        "  ,COUNT(DISTINCT order_nbr) AS orders\n",
        "  ,SUM(origl_ordered_qty) AS units\n",
        "  FROM view_dol dol\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN first_send_dt AND DATE_ADD(first_send_dt,7)\n",
        "  GROUP BY 1,2,3\n",
        ")\n",
        "\n",
        "-- For each member, rank each order by how near it was to the last campaign send\n",
        "-- purchases must be after the campaign send and within 7 days of the send to be attributed\n",
        ", aud_order_join as (\n",
        "SELECT a.*\n",
        ", b.*\n",
        ",(bigint(to_timestamp(b.order_ts))-bigint(to_timestamp(a.campaign_send_ts)))/60 as order_send_diff_minutes\n",
        ",RANK() OVER(PARTITION BY a.upm_id, b.order_ts ORDER BY (bigint(to_timestamp(b.order_ts))-bigint(to_timestamp(a.campaign_send_ts)))/60 ASC) as nearest_order\n",
        "FROM test_control_table a\n",
        "LEFT JOIN ab_purchase b ON (a.upm_id = b.upm_id_ab_buyer AND b.order_ts >= a.campaign_send_ts AND b.order_ts <= DATE_ADD(a.campaign_send_ts,7))\n",
        "ORDER BY a.upm_id,a.campaign_send_dt, nearest_order asc\n",
        ")\n",
        "\n",
        "-- Aggregate each distinct send, attributing only the nearest order.\n",
        "-- This prevent double counting of orders when a members receives multiple sends within the measurement window.\n",
        ", aud_order_agg as (\n",
        "SELECT upm_id AS upm_id_ab_buyer\n",
        "  ,SUM(demand) AS demand\n",
        "  ,SUM(base_FW_demand) AS base_FW_demand\n",
        "  ,SUM(base_AP_demand) AS base_AP_demand\n",
        "  ,SUM(base_EQ_demand) AS base_EQ_demand\n",
        "  ,SUM(clr_FW_demand) AS clr_FW_demand\n",
        "  ,SUM(clr_AP_demand) AS clr_AP_demand\n",
        "  ,SUM(clr_EQ_demand) AS clr_EQ_demand\n",
        "  ,SUM(launch_FW_demand) AS launch_FW_demand\n",
        "  ,SUM(launch_AP_demand) AS launch_AP_demand\n",
        "  ,SUM(launch_EQ_demand) AS launch_EQ_demand\n",
        "  ,SUM(rest_demand) AS rest_demand\n",
        "  ,SUM(orders) AS orders\n",
        "  ,SUM(units) AS units\n",
        "FROM aud_order_join a\n",
        "WHERE nearest_order = 1\n",
        "AND demand IS NOT NULL -- additional filter for extraneous records\n",
        "GROUP BY 1\n",
        "ORDER BY 1\n",
        ")\n",
        "\n",
        ", historical_purchase AS (\n",
        "  SELECT aoa.upm_id_ab_buyer as upm_id\n",
        "  , COUNT(DISTINCT order_nbr) as histOrders\n",
        "  FROM aud_order_agg aoa  -- eligible audience\n",
        "  INNER JOIN view_dol dol on aoa.upm_id_ab_buyer = dol.upm_id\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,730) AND DATE_SUB(first_send_dt,1)\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        "-- due to how messages are set up on Bluecore, the campaign_cd is not unique to a particular day. We concat with the message_sent_ts\n",
        "-- only using the date ignores cases where multiple sends occur on the same day, potentially undercounting\n",
        ",email_sent AS (\n",
        "  SELECT fed.upm_id\n",
        "  , COUNT(DISTINCT concat(campaign_cd, LEFT(message_sent_ts,21))) as email_circ\n",
        "  FROM COMMS.FACT_EMAIL_DELIVERY fed\n",
        "  WHERE (campaign_cd LIKE ${hivevar:camp_cd_tst_str} OR campaign_cd LIKE ${hivevar:camp_cd_ctl_str})\n",
        "  AND CAST(message_sent_ts AS DATE) between ${hivevar:msmt_start_dt} and ${hivevar:msmt_end_dt}\n",
        "  AND retail_geo_name = 'NA'\n",
        "  AND fed.upm_id IS NOT NULL\n",
        "  GROUP BY upm_id\n",
        ")\n",
        "\n",
        "-- returns all the instances of the campaign codes for each member along with the response types and revenue if applicable\n",
        ", email_response as (\n",
        "  SELECT DISTINCT fer.upm_id\n",
        "  , LEFT(response_ts,10) AS response_ts\n",
        "  , response_type\n",
        "  , revenue_dollar\n",
        "  FROM comms.fact_email_response fer\n",
        "  INNER JOIN test_control_table tct ON fer.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        "  WHERE (campaign_cd LIKE ${hivevar:camp_cd_tst_str} OR campaign_cd LIKE ${hivevar:camp_cd_ctl_str})\n",
        "  AND CAST(LEFT(response_ts,10) AS DATE) between first_send_dt and DATE_ADD(last_send_dt,7)\n",
        "  AND retail_geo_name = 'NA'\n",
        "  AND fer.upm_id IS NOT NULL\n",
        " )\n",
        "\n",
        "-- calculates the the total number of opens, clicks, purchases and last touch attributed demand for each member for the campaign\n",
        "-- responses are only considered if they occur between when the message was sent and 7 days later.\n",
        ", email_calc AS (\n",
        "  SELECT fer.upm_id\n",
        "  , COUNT(CASE WHEN response_type= 'email open' THEN fer.upm_id ELSE NULL END) AS email_opens\n",
        "  , COUNT(CASE WHEN response_type='link click' THEN fer.upm_id ELSE NULL END) AS email_clicks\n",
        "  , COUNT(CASE WHEN response_type='ecommerce-purchase' THEN fer.upm_id ELSE NULL END) AS email_purch\n",
        "  , SUM(CASE WHEN response_type='ecommerce-purchase' THEN revenue_dollar ELSE 0 END) AS email_demand\n",
        "  FROM email_response fer\n",
        "  INNER JOIN test_control_table tct ON fer.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        "  WHERE CAST(fer.response_ts as DATE) BETWEEN first_send_dt AND DATE_ADD(last_send_dt,7)\n",
        "  GROUP BY 1\n",
        ")\n",
        "-- total number of total messages send for each member during the campaign.\n",
        "-- For a singular campaign, this should be one (1), for triggers it could be more depending on how the trigger is set up\n",
        ", email_delivery_agg as (\n",
        "  SELECT upm_id\n",
        "  ,SUM(email_circ) AS email_circ\n",
        "  FROM email_sent\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", email_response_agg as (\n",
        "  SELECT upm_id\n",
        "  ,SUM(email_opens) AS email_opens\n",
        "  ,SUM(email_clicks) AS email_clicks\n",
        "  ,SUM(email_purch) AS email_purch\n",
        "  ,SUM(email_demand) AS email_demand\n",
        "  FROM email_calc\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", platform_usage as (\n",
        "  SELECT tct.upm_id\n",
        "  , CASE WHEN CAST(first_${hivevar:usage_platform}_activity_dt AS DATE) BETWEEN first_send_dt AND DATE_ADD(last_send_dt,7) THEN 1\n",
        "         ELSE 0\n",
        "    END AS platform_user\n",
        "  FROM aud_select_workspace.gck_first_and_last_touchpoint flt\n",
        "  INNER JOIN test_control_table tct ON flt.upm_id = tct.upm_id  -- Need first/last_send values as activity window\n",
        ")\n",
        "\n",
        "---------------------\n",
        "-- MAIN QUERY\n",
        "----------------------\n",
        "SELECT z.upm_id\n",
        "  , z.test_control\n",
        "\n",
        "  -- primary metrics ON incremental revenue: conversion rate, average demand per purchaser\n",
        "  ,SUM(demand) AS demand\n",
        "\n",
        "  -- secondary metrics ON order double click\n",
        "  ,SUM(orders) AS orders\n",
        "  ,SUM(units) AS units\n",
        "\n",
        "  -- secondary metrics ON emails\n",
        "  , SUM(email_circ) AS email_sent\n",
        "  , SUM(email_opens) AS email_opens\n",
        "  , SUM(email_clicks) AS email_clicks\n",
        "  , SUM(email_purch) AS email_purch\n",
        "  , SUM(email_demand) AS email_demand\n",
        "\n",
        " -- secondary metrics ON site/app engagement\n",
        " , SUM(visits) AS site_app_visits\n",
        " , SUM(PDP_FAVORITE_COUNT) AS PDP_FAVORITE_COUNT\n",
        " , SUM(ADD_TO_CART_COUNT) AS ADD_TO_CART_COUNT\n",
        " , SUM(physical_activity) AS physical_activity\n",
        "\n",
        "  -- secondary metrics ON margin\n",
        "  , SUM(base_FW_demand) AS base_FW_demand\n",
        "  , SUM(base_AP_demand) AS base_AP_demand\n",
        "  , SUM(base_EQ_demand) AS base_EQ_demand\n",
        "  , SUM(clr_FW_demand) AS clr_FW_demand\n",
        "  , SUM(clr_AP_demand) AS clr_AP_demand\n",
        "  , SUM(clr_EQ_demand) AS clr_EQ_demand\n",
        "  , SUM(launch_FW_demand) AS launch_FW_demand\n",
        "  , SUM(launch_AP_demand) AS launch_AP_demand\n",
        "  , SUM(launch_EQ_demand) AS launch_EQ_demand\n",
        "  , SUM(rest_demand) AS rest_demand\n",
        "\n",
        "   -- optional first usage kpi\n",
        "  , SUM(platform_user) AS platform_user\n",
        "\n",
        "    -- secondary metrics ON order history\n",
        "  ,SUM(histOrders) AS histOrders\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END) AS 1_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END) AS 2_priorOrder\n",
        "\n",
        "\n",
        "FROM test_control_table z\n",
        "INNER JOIN balanced_aud_aa b ON z.upm_id = b.upm_id  --inner join our balanced audience to remove members that were distorting the AA period balance\n",
        "\n",
        "INNER JOIN mhub mh ON z.upm_id = mh.upm_id\n",
        "LEFT JOIN member_activities a ON mh.member_id = a.member_id\n",
        "\n",
        "LEFT JOIN aud_order_agg abp ON z.upm_id = abp.upm_id_ab_buyer\n",
        "LEFT JOIN historical_purchase hp ON z.upm_id = hp.upm_id\n",
        "\n",
        "LEFT JOIN email_delivery_agg cda ON z.upm_id = cda.upm_id\n",
        "LEFT JOIN email_response_agg cra ON z.upm_id = cra.upm_id\n",
        "\n",
        "LEFT JOIN platform_usage plt ON z.upm_id = plt.upm_id\n",
        "\n",
        "GROUP BY 1,2\n",
        "ORDER BY 1 DESC;\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# view data in spark df\n",
        "display(df_spark)\n",
        "\n",
        "# Convert to Pandas DataFrame for further analysis\n",
        "df = df_spark.toPandas()\n"
      ],
      "metadata": {
        "id": "ESu0dAcRi2f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Example from Ben Li"
      ],
      "metadata": {
        "id": "JGpUOlPlidD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_aggr = df_clean.groupby('variation').agg({'upm_id': pd.Series.nunique, # visitors\n",
        "                                              'visits': [np.sum],\n",
        "                                              'visits_with_pw_view': [np.count_nonzero], # get ratio pw view rate, unique event - count only once\n",
        "                                              'visits_with_pdp_view': [np.count_nonzero], # get ratio pvr\n",
        "                                              'visits_with_atc': [np.count_nonzero], # get ratio atc\n",
        "                                              'visits_with_cart_view': [np.count_nonzero], # get ratio cart view rate\n",
        "                                              'visits_with_checkout': [np.count_nonzero], # get ratio checkout rate\n",
        "                                              'visits_with_order': [np.count_nonzero], # get ratio cvr\n",
        "                                              'tot_pw_view': [np.sum],\n",
        "                                              'tot_pdp_view': [np.sum],\n",
        "                                              'tot_atc': [np.sum],\n",
        "                                              'tot_cart_view': [np.sum],\n",
        "                                              'tot_checkout': [np.sum],\n",
        "                                              'tot_orders': [np.sum],\n",
        "                                              'tot_units': [np.sum],\n",
        "                                              'tot_demand': [np.sum] # get demand per visitor\n",
        "                                              # 'tot_demand': [np.mean] # demand per visitor\n",
        "                                              }).rename(columns={'upm_id': 'visitors'}).round(2)\n",
        "\n",
        "# display(df_aggr)\n",
        "# all `visits_with_xx` metrics are unique now (as `visitors_with_xx`) because of the count_nonzero() function used.\n",
        "df_aggr"
      ],
      "metadata": {
        "id": "PbLgJAW_ifjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### V6 Draft"
      ],
      "metadata": {
        "id": "Dk-IAAMZmZKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Step 1: Grouping by 'group' and aggregating metrics\n",
        "df_aggr = aggregated_metrics = df.groupby('test_control').agg({'upm_id': pd.Series.nunique,  # Unique members\n",
        "                                                                'orders': 'sum',  # Total orders\n",
        "                                                                'units': 'mean',  # Average units purchased\n",
        "                                                                'demand': 'mean',  # Average sales (demand)\n",
        "\n",
        "                                                            }).rename(columns={'upm_id': 'members', 'demand': 'avg_demand'}).round(2)\n",
        "\n",
        "# Step 2: Calculating lift\n",
        "\n",
        "# Transpose for easier manipulation\n",
        "aggregated_metrics = aggregated_metrics.T\n",
        "\n",
        "# Calculate lift as ((test - control) / control) * 100\n",
        "lift = (aggregated_metrics['test'] - aggregated_metrics['control']) / aggregated_metrics['control'] * 100\n",
        "lift = lift.rename('lift')\n",
        "\n",
        "# Combine the original metrics with the lift calculation\n",
        "result = pd.concat([aggregated_metrics, lift], axis=1)\n",
        "\n",
        "result\n"
      ],
      "metadata": {
        "id": "qJCgR38Libny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### V7 Draft"
      ],
      "metadata": {
        "id": "IlyyG8WyKDec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Step 1: Add a 'buyer' column to indicate whether an order was made\n",
        "df['buyer'] = (df['orders'] > 0).astype(int)\n",
        "\n",
        "# Step 2: aggregate metrics\n",
        "aggregated_metrics = df.groupby('test_control').agg({\n",
        "    'upm_id': pd.Series.nunique,  # Unique members (total_member_cnt)\n",
        "    'buyer': 'sum',  # buying_member_cnt\n",
        "    'orders': 'sum',  # Total orders\n",
        "    'units': 'sum',  # Total units purchased\n",
        "    'demand': ['mean', 'sum'], # Avg sales (demand) and Total Sales\n",
        "}).rename(columns={\n",
        "    'upm_id': 'Total Members',\n",
        "    'demand_mean': 'Demand per Buyer',\n",
        "    'demand_sum': 'Total Demand',\n",
        "    'buyer': 'Buying Members',\n",
        "}).round(2)\n",
        "\n",
        "# Step 3: Calculate additional metrics\n",
        "aggregated_metrics['Conversion Rate'] = (aggregated_metrics['Buying Members'] / aggregated_metrics['Total Members'])\n",
        "#   ,COUNT(DISTINCT abp.upm_id_ab_buyer) / COUNT(DISTINCT z.upm_id) AS conversion_rate\n",
        "aggregated_metrics['Demand per Send'] = (aggregated_metrics['Demand per Buyer'] * aggregated_metrics['Conversion Rate']).round(2)\n",
        "aggregated_metrics['AOV'] = (aggregated_metrics['Total Demand']/aggregated_metrics['orders']).round(2)\n",
        "aggregated_metrics['AUR'] = (aggregated_metrics['units']/aggregated_metrics['orders']).round(2)\n",
        "aggregated_metrics['UPT'] = (aggregated_metrics['units']/aggregated_metrics['orders']).round(2)\n",
        "aggregated_metrics['# Txn per Buyer'] = (aggregated_metrics['units']/aggregated_metrics['orders']).round(2)\n",
        "\n",
        "\n",
        "# Step 4: Perform t-test for conversion rate\n",
        "test_buyers = df[df['test_control'] == 'test']['buyer']\n",
        "control_buyers = df[df['test_control'] == 'control']['buyer']\n",
        "t_stat_conversion, p_value_conversion = stats.ttest_ind(test_buyers, control_buyers, equal_var=False)\n",
        "\n",
        "# Step 5: Add confidence level and p-value to aggregated_metrics\n",
        "aggregated_metrics['t_stat_conversion'] = t_stat_conversion\n",
        "aggregated_metrics['p_value_conversion'] = p_value_conversion\n",
        "\n",
        "# Step 6: Transpose for easier manipulation\n",
        "aggregated_metrics = aggregated_metrics.T\n",
        "\n",
        "# Step 7: Calculate lift\n",
        "# ((test - control) / control) * 100\n",
        "lift = (aggregated_metrics.get('test', 0) - aggregated_metrics.get('control', 0)) / aggregated_metrics.get('control', 1) * 100\n",
        "lift = lift.rename('lift')\n",
        "# Omit lift calculation for metrics that don't apply, like t_stat and p_value\n",
        "lift['t_stat_conversion'] = ''\n",
        "lift['p_value_conversion'] = ''\n",
        "\n",
        "# Step 8: Combine the original metrics with the lift calculation\n",
        "result = pd.concat([aggregated_metrics, lift], axis=1)\n",
        "\n",
        "# Step 9: Display the result\n",
        "result\n"
      ],
      "metadata": {
        "id": "aT6Ywt5cKCYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### V8 Draft"
      ],
      "metadata": {
        "id": "1xyhDBLal4vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Step 1: Add 'buyer' and 'prior_order' columns\n",
        "\n",
        "df['buyer'] = (df['orders'] > 0).astype(int) # 'buyer' = 1 if 'orders' > 0, else 0\n",
        "df['demand_1_prior'] = np.where(df['1_priorOrder'] == 1, df['demand'], 0) # demand for members with 1 prior order\n",
        "df['demand_2_prior'] = np.where(df['2_priorOrder'] == 1, df['demand'], 0) # demand for members with 2 prior orders\n",
        "df['member_1_prior'] = np.where(df['1_priorOrder'] == 1, 1, 0) # Count of members with 1 prior order\n",
        "df['member_2_prior'] = np.where(df['2_priorOrder'] == 1, 1, 0) # Count of members with 2 prior orders\n",
        "\n",
        "# Step 2: Add temporary calculations for members with prior purchase (not to be included in the final DataFrame)\n",
        "\n",
        "temp_calculations = df.groupby('test_control').agg({\n",
        "    'demand_1_prior': 'sum',\n",
        "    'demand_2_prior': 'sum',\n",
        "    'member_1_prior': 'sum',\n",
        "    'member_2_prior': 'sum',\n",
        "}).rename(columns={\n",
        "    'demand_1_prior': 'Total Demand 1 Prior',\n",
        "    'demand_2_prior': 'Total Demand 2 Prior',\n",
        "    'member_1_prior': 'Members 1 Prior',\n",
        "    'member_2_prior': 'Members 2 Prior',\n",
        "})\n",
        "\n",
        "# Step 3: Calculate DPB_1_prior and DPB_2_prior using previously defined temporary calculations\n",
        "\n",
        "dpb_1_prior = temp_calculations['Total Demand 1 Prior'] / temp_calculations['Members 1 Prior']\n",
        "dpb_2_prior = temp_calculations['Total Demand 2 Prior'] / temp_calculations['Members 2 Prior']\n",
        "\n",
        "# Step 4: Aggregate metrics\n",
        "\n",
        "aggregated_metrics = df.groupby('test_control').agg({\n",
        "    'upm_id': pd.Series.nunique,  # Unique members\n",
        "    'buyer': 'sum',  # Buying member count\n",
        "    'orders': 'sum',\n",
        "    'units': 'sum',\n",
        "    'demand': ['sum','mean'],  # Average and total demand\n",
        "    '1_priorOrder': 'sum', # Members with 1 previous order\n",
        "    '2_priorOrder': 'sum', # Members with w previous orders\n",
        "    'demand_1_prior': 'sum',\n",
        "    'demand_2_prior': 'sum',\n",
        "    'member_1_prior': 'sum',\n",
        "    'member_2_prior': 'sum',\n",
        "    'email_opens': 'sum',\n",
        "    'email_sent': 'sum',\n",
        "    'email_clicks': 'sum',\n",
        "    'site_app_visits': 'mean',\n",
        "    'PDP_FAVORITE_COUNT': 'mean',\n",
        "    'ADD_TO_CART_COUNT': 'mean',\n",
        "    'physical_activity': 'mean',\n",
        "    'base_FW_demand': 'mean',\n",
        "    'clr_FW_demand': 'mean',\n",
        "    'launch_FW_demand': 'mean',\n",
        "    'base_AP_demand': 'mean',\n",
        "    'clr_AP_demand': 'mean',\n",
        "    'launch_AP_demand': 'mean',\n",
        "    'base_EQ_demand': 'mean',\n",
        "    'clr_EQ_demand': 'mean',\n",
        "    'launch_EQ_demand': 'mean',\n",
        "})\n",
        "\n",
        "# Step 5: Flatten the MultiIndex columns\n",
        "\n",
        "aggregated_metrics.columns = ['_'.join(col).strip() for col in aggregated_metrics.columns.values]\n",
        "aggregated_metrics = aggregated_metrics.rename(columns={\n",
        "    'upm_id_nunique': 'Total Members',\n",
        "    'buyer_sum': 'Buying Members',\n",
        "    'demand_mean': 'Demand per Buyer',\n",
        "    'demand_sum': 'Total Demand',\n",
        "    'units_sum': 'Total Units',\n",
        "    'orders_sum': 'Total Orders',\n",
        "    # Secondary KPIs - Repeat buyers\n",
        "    '1_priorOrder_sum': 'Members with 1 previous order',\n",
        "    'dpb_1_prior': 'Demand per Buyer (1 previous order)',\n",
        "    '2_priorOrder_sum':'Members with 2 previous orders',\n",
        "    'dpb_2_prior': 'Demand per Buyer (2 previous orders)',\n",
        "    # Secondary KPIs - Site/app Engagement metrics\n",
        "    'site_app_visits_mean':'Visits per known member',\n",
        "    'PDP_FAVORITE_COUNT_mean':'PDP favorite per known member',\n",
        "    'ADD_TO_CART_COUNT_mean': 'Add to cart per known member',\n",
        "    'physical_activity_mean': 'Workouts per known member',\n",
        "})\n",
        "\n",
        "\n",
        "## Step 6: Calculate additional metrics\n",
        "\n",
        "\n",
        "aggregated_metrics['Conversion Rate'] = aggregated_metrics['Buying Members'] / aggregated_metrics['Total Members']\n",
        "aggregated_metrics['Demand per Send'] = aggregated_metrics['Demand per Buyer'] * aggregated_metrics['Conversion Rate']\n",
        "\n",
        "# Secondary KPIs - order deep dive\n",
        "aggregated_metrics['AOV'] = aggregated_metrics['Total Demand'] / aggregated_metrics['Total Orders']\n",
        "aggregated_metrics['AUR'] = aggregated_metrics['Total Demand'] / aggregated_metrics['Total Units']\n",
        "aggregated_metrics['UPT'] = aggregated_metrics['Total Units'] / aggregated_metrics['Total Orders']\n",
        "aggregated_metrics['# Txn per Buyer'] = aggregated_metrics['Demand per Buyer']/aggregated_metrics['AOV']\n",
        "\n",
        "# Secondary KPIs - Repeat buyers\n",
        "aggregated_metrics['Conversion rate (1x buyers/email receivers)'] = aggregated_metrics['Members with 1 previous order'] / aggregated_metrics['Total Members']\n",
        "aggregated_metrics['Conversion rate (2x buyers/email receivers)'] = aggregated_metrics['Members with 2 previous orders'] / aggregated_metrics['Total Members']\n",
        "aggregated_metrics['DPB_1_prior'] = aggregated_metrics['demand_1_prior_sum'] / aggregated_metrics.get('member_1_prior_sum', 1)\n",
        "aggregated_metrics['DPB_2_prior'] = aggregated_metrics['demand_2_prior_sum'] / aggregated_metrics.get('member_2_prior_sum', 1)\n",
        "\n",
        "# Secondary KPIs - Email/Push Engagement metrics (all email/push received during measurement windows)\n",
        "aggregated_metrics['Email Open Rate (against sends)'] = aggregated_metrics['email_opens_sum'] / aggregated_metrics['email_sent_sum']\n",
        "aggregated_metrics['Email Click Rate (against sends)'] = aggregated_metrics['email_clicks_sum'] / aggregated_metrics['email_sent_sum']\n",
        "\n",
        "# Secondary KPIs - product mix\n",
        "aggregated_metrics['Footwear Demand per Member'] = ( aggregated_metrics['base_FW_demand_mean'] + aggregated_metrics['clr_FW_demand_mean'] + aggregated_metrics['launch_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "aggregated_metrics['Apparel Demand per Member'] = ( aggregated_metrics['base_AP_demand_mean'] + aggregated_metrics['clr_AP_demand_mean'] + aggregated_metrics['launch_AP_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "aggregated_metrics['Equipment Demand per Member'] = ( aggregated_metrics['base_EQ_demand_mean'] + aggregated_metrics['clr_EQ_demand_mean'] + aggregated_metrics['launch_EQ_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "\n",
        "# Secondary KPIs - LOB\n",
        "aggregated_metrics['Regular Product Demand per Member'] = ( aggregated_metrics['base_EQ_demand_mean'] + aggregated_metrics['base_AP_demand_mean'] + aggregated_metrics['base_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "\n",
        "aggregated_metrics['Clearance Demand per Member'] = ( aggregated_metrics['clr_EQ_demand_mean'] + aggregated_metrics['clr_AP_demand_mean'] + aggregated_metrics['clr_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "\n",
        "aggregated_metrics['Launch Demand per Member'] = ( aggregated_metrics['launch_EQ_demand_mean'] + aggregated_metrics['launch_AP_demand_mean'] + aggregated_metrics['launch_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "\n",
        "# Step 7: Remove temporary calculation columns as they're not needed in final DataFrame\n",
        "aggregated_metrics.drop(columns=['demand_1_prior_sum', 'demand_2_prior_sum', 'member_1_prior_sum', 'member_2_prior_sum', 'email_opens_sum','email_sent_sum', 'email_clicks_sum', 'base_FW_demand_mean',\n",
        " 'clr_FW_demand_mean', 'launch_FW_demand_mean', 'base_AP_demand_mean', 'clr_AP_demand_mean', 'launch_AP_demand_mean', 'base_EQ_demand_mean', 'clr_EQ_demand_mean','launch_EQ_demand_mean'], inplace=True)\n",
        "\n",
        "# Step 8: Perform t-test for conversion rate\n",
        "test_buyers = df[df['test_control'] == 'test']['buyer']\n",
        "control_buyers = df[df['test_control'] == 'control']['buyer']\n",
        "t_stat_conversion, p_value_conversion = stats.ttest_ind(test_buyers, control_buyers, equal_var=False)\n",
        "\n",
        "# Step 9: Add confidence level and p-value to aggregated_metrics\n",
        "aggregated_metrics['t_stat_conversion'] = t_stat_conversion\n",
        "aggregated_metrics['p_value_conversion'] = p_value_conversion\n",
        "\n",
        "# Step 10: Transpose the table\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "aggregated_metrics = aggregated_metrics.T\n",
        "\n",
        "# Step 11: Calculate lift\n",
        "# ((test - control) / control) * 100\n",
        "lift = (aggregated_metrics.get('test', 0) - aggregated_metrics.get('control', 0)) / aggregated_metrics.get('control', 1) * 100\n",
        "lift = lift.rename('lift')\n",
        "# Omit lift calculation for metrics that don't apply, like t_stat and p_value\n",
        "lift['t_stat_conversion'] = ''\n",
        "lift['p_value_conversion'] = ''\n",
        "\n",
        "# Step 12: Combine the original metrics with the lift calculation\n",
        "result = pd.concat([aggregated_metrics, lift], axis=1)\n",
        "\n",
        "# Step 13: Display the result\n",
        "result"
      ],
      "metadata": {
        "id": "0Ndw2fdDl7W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "futHyP-dLQeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### V11"
      ],
      "metadata": {
        "id": "AZnLciW0jIdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Assuming 'df' is your DataFrame with necessary columns\n",
        "\n",
        "# Step 1: Add 'buyer' and 'prior_order' indicator columns\n",
        "df['buyer'] = (df['orders'] > 0).astype(int)\n",
        "df['demand_1_prior'] = np.where(df['1_priorOrder'] == 1, df['demand'], np.nan)\n",
        "df['demand_2_prior'] = np.where(df['2_priorOrder'] == 2, df['demand'], np.nan)\n",
        "df['member_1_prior'] = np.where(df['1_priorOrder'] == 1, 1, 0)\n",
        "df['member_2_prior'] = np.where(df['2_priorOrder'] == 2, 1, 0)\n",
        "\n",
        "# Step 2: Perform temporary calculations for DPB_1_prior and DPB_2_prior\n",
        "temp_calculations = df.groupby('test_control').agg({\n",
        "    'demand_1_prior': 'sum',\n",
        "    'demand_2_prior': 'sum',\n",
        "    'member_1_prior': 'sum',\n",
        "    'member_2_prior': 'sum',\n",
        "}).rename(columns={\n",
        "    'demand_1_prior': 'Total Demand 1 Prior',\n",
        "    'demand_2_prior': 'Total Demand 2 Prior',\n",
        "    'member_1_prior': 'Members 1 Prior',\n",
        "    'member_2_prior': 'Members 2 Prior',\n",
        "})\n",
        "\n",
        "# Step 3: Calculate DPB_1_prior and DPB_2_prior using temporary calculations\n",
        "dpb_1_prior = temp_calculations['Total Demand 1 Prior'] / temp_calculations['Members 1 Prior']\n",
        "dpb_2_prior = temp_calculations['Total Demand 2 Prior'] / temp_calculations['Members 2 Prior']\n",
        "\n",
        "# Step 4: Aggregate metrics\n",
        "aggregated_metrics = df.groupby('test_control').agg({\n",
        "    'upm_id': 'nunique',\n",
        "    'buyer': 'sum',\n",
        "    'orders': 'sum',\n",
        "    'units': 'sum',\n",
        "    'demand': ['sum', 'mean'],\n",
        "    '1_priorOrder': 'sum',\n",
        "    '2_priorOrder': 'sum',\n",
        "    'demand_1_prior': 'sum',\n",
        "    'demand_2_prior': 'sum',\n",
        "    'member_1_prior': 'sum',\n",
        "    'member_2_prior': 'sum',\n",
        "    'email_opens': 'sum',\n",
        "    'email_sent': 'sum',\n",
        "    'email_clicks': 'sum',\n",
        "    'site_app_visits': 'mean',\n",
        "    'PDP_FAVORITE_COUNT': 'mean',\n",
        "    'ADD_TO_CART_COUNT': 'mean',\n",
        "    'physical_activity': 'mean',\n",
        "    'base_FW_demand': 'mean',\n",
        "    'clr_FW_demand': 'mean',\n",
        "    'launch_FW_demand': 'mean',\n",
        "    'base_AP_demand': 'mean',\n",
        "    'clr_AP_demand': 'mean',\n",
        "    'launch_AP_demand': 'mean',\n",
        "    'base_EQ_demand': 'mean',\n",
        "    'clr_EQ_demand': 'mean',\n",
        "    'launch_EQ_demand': 'mean',\n",
        "})\n",
        "\n",
        "# Step 5: Correct the MultiIndex handling by flattening and renaming\n",
        "aggregated_metrics.columns = ['_'.join(col).strip() for col in aggregated_metrics.columns.values]\n",
        "aggregated_metrics = aggregated_metrics.rename(columns={\n",
        "    'upm_id_nunique': 'Total Members',\n",
        "    'buyer_sum': 'Buying Members',\n",
        "    'demand_mean': 'Demand per Buyer',\n",
        "    'demand_sum': 'Total Demand',\n",
        "    'units_sum': 'Total Units',\n",
        "    'orders_sum': 'Total Orders',\n",
        "    # Secondary KPIs - Repeat buyers\n",
        "    '1_priorOrder_sum': 'Members with 1 previous order',\n",
        "    'dpb_1_prior': 'Demand per Buyer (1 previous order)',\n",
        "    '2_priorOrder_sum':'Members with 2 previous orders',\n",
        "    'dpb_2_prior': 'Demand per Buyer (2 previous orders)',\n",
        "    # Secondary KPIs - Site/app Engagement metrics\n",
        "    'site_app_visits_mean':'Visits per known member',\n",
        "    'PDP_FAVORITE_COUNT_mean':'PDP favorite per known member',\n",
        "    'ADD_TO_CART_COUNT_mean': 'Add to cart per known member',\n",
        "    'physical_activity_mean': 'Workouts per known member',\n",
        "})\n",
        "\n",
        "# Step 6: Calculate DPB_1_prior and DPB_2_prior based on the temporary calculations and integrate directly into the DataFrame\n",
        "# aggregated_metrics['DPB_1_prior'] = aggregated_metrics['demand_1_prior'] / aggregated_metrics['member_1_prior']\n",
        "# aggregated_metrics['DPB_2_prior'] = aggregated_metrics['demand_2_prior'] / aggregated_metrics['member_2_prior']\n",
        "\n",
        "# Step 7: Calculate additional metrics\n",
        "\n",
        "aggregated_metrics['Conversion Rate'] = aggregated_metrics['Buying Members'] / aggregated_metrics['Total Members']\n",
        "aggregated_metrics['Demand per Send'] = aggregated_metrics['Demand per Buyer'] * aggregated_metrics['Conversion Rate']\n",
        "# Secondary KPIs - order deep dive\n",
        "aggregated_metrics['AOV'] = aggregated_metrics['Total Demand'] / aggregated_metrics['Total Orders']\n",
        "aggregated_metrics['AUR'] = aggregated_metrics['Total Demand'] / aggregated_metrics['Total Units']\n",
        "aggregated_metrics['UPT'] = aggregated_metrics['Total Units'] / aggregated_metrics['Total Orders']\n",
        "aggregated_metrics['# Txn per Buyer'] = aggregated_metrics['Demand per Buyer']/aggregated_metrics['AOV']\n",
        "# Secondary KPIs - Repeat buyers\n",
        "aggregated_metrics['Conversion rate (1x buyers/email receivers)'] = aggregated_metrics['Members with 1 previous order'] / aggregated_metrics['Total Members']\n",
        "aggregated_metrics['Conversion rate (2x buyers/email receivers)'] = aggregated_metrics['Members with 2 previous orders'] / aggregated_metrics['Total Members']\n",
        "aggregated_metrics['DPB_1_prior'] = aggregated_metrics['demand_1_prior_sum'] / aggregated_metrics.get('member_1_prior_sum', 1)\n",
        "aggregated_metrics['DPB_2_prior'] = aggregated_metrics['demand_2_prior_sum'] / aggregated_metrics.get('member_2_prior_sum', 1)\n",
        "# Secondary KPIs - Email/Push Engagement metrics (all email/push received during measurement windows)\n",
        "aggregated_metrics['Email Open Rate (against sends)'] = aggregated_metrics['email_opens_sum'] / aggregated_metrics['email_sent_sum']\n",
        "aggregated_metrics['Email Click Rate (against sends)'] = aggregated_metrics['email_clicks_sum'] / aggregated_metrics['email_sent_sum']\n",
        "# Secondary KPIs - product mix\n",
        "aggregated_metrics['Footwear Demand per Member'] = ( aggregated_metrics['base_FW_demand_mean'] + aggregated_metrics['clr_FW_demand_mean'] + aggregated_metrics['launch_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "aggregated_metrics['Apparel Demand per Member'] = ( aggregated_metrics['base_AP_demand_mean'] + aggregated_metrics['clr_AP_demand_mean'] + aggregated_metrics['launch_AP_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "aggregated_metrics['Equipment Demand per Member'] = ( aggregated_metrics['base_EQ_demand_mean'] + aggregated_metrics['clr_EQ_demand_mean'] + aggregated_metrics['launch_EQ_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "# Secondary KPIs - LOB\n",
        "aggregated_metrics['Regular Product Demand per Member'] = ( aggregated_metrics['base_EQ_demand_mean'] + aggregated_metrics['base_AP_demand_mean'] + aggregated_metrics['base_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "\n",
        "aggregated_metrics['Clearance Demand per Member'] = ( aggregated_metrics['clr_EQ_demand_mean'] + aggregated_metrics['clr_AP_demand_mean'] + aggregated_metrics['clr_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "\n",
        "aggregated_metrics['Launch Demand per Member'] = ( aggregated_metrics['launch_EQ_demand_mean'] + aggregated_metrics['launch_AP_demand_mean'] + aggregated_metrics['launch_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "\n",
        "\n",
        "# Step 7: Remove temporary calculation columns as they're not needed in final DataFrame\n",
        "aggregated_metrics.drop(columns=['demand_1_prior_sum', 'demand_2_prior_sum', 'member_1_prior_sum', 'member_2_prior_sum', 'email_opens_sum','email_sent_sum', 'email_clicks_sum', 'base_FW_demand_mean',\n",
        " 'clr_FW_demand_mean', 'launch_FW_demand_mean', 'base_AP_demand_mean', 'clr_AP_demand_mean', 'launch_AP_demand_mean', 'base_EQ_demand_mean', 'clr_EQ_demand_mean','launch_EQ_demand_mean'], inplace=True)\n",
        "\n",
        "# Step 8: Perform t-test for 'Conversion Rate' and 'Demand per Buyer'\n",
        "\n",
        "# Separate the test and control groups to perform the t-tests correctly.\n",
        "test_group = df[df['test_control'] == 'test']\n",
        "control_group = df[df['test_control'] == 'control']\n",
        "\n",
        "# Conversion Rate t-test\n",
        "conversion_rate_test = test_group['buyer'].sum() / test_group['upm_id'].nunique()\n",
        "conversion_rate_control = control_group['buyer'].sum() / control_group['upm_id'].nunique()\n",
        "\n",
        "# Demand per Buyer t-test\n",
        "demand_per_buyer_test = test_group[test_group['buyer'] > 0]['demand'].sum() / test_group['buyer'].sum()\n",
        "demand_per_buyer_control = control_group[control_group['buyer'] > 0]['demand'].sum() / control_group['buyer'].sum()\n",
        "\n",
        "t_stat_conversion, p_value_conversion = stats.ttest_ind(\n",
        "    test_group[test_group['buyer'] > 0]['demand'],\n",
        "    control_group[control_group['buyer'] > 0]['demand'],\n",
        "    equal_var=False, nan_policy='omit'\n",
        ")\n",
        "\n",
        "t_stat_demand_per_buyer, p_value_demand_per_buyer = stats.ttest_ind(\n",
        "    test_group[test_group['buyer'] > 0]['demand'] / test_group['buyer'],\n",
        "    control_group[control_group['buyer'] > 0]['demand'] / control_group['buyer'],\n",
        "    equal_var=False, nan_policy='omit'\n",
        ")\n",
        "\n",
        "# Step 9: Transpose the table\n",
        "aggregated_metrics = aggregated_metrics.T\n",
        "\n",
        "# Step 10: Calculate lift for all metrics except 'Conversion Rate' and 'Demand per Buyer'\n",
        "lift_columns = ['Metric', 'Test', 'Control', 'Lift', 't-stat', 'p-value']\n",
        "scorecard = pd.DataFrame(columns=lift_columns)\n",
        "\n",
        "for metric in aggregated_metrics.index:  # Iterating over the index after transpose\n",
        "    if metric not in ['Conversion Rate', 'Demand per Buyer']:\n",
        "        test_val = aggregated_metrics.loc[metric, 'test']\n",
        "        control_val = aggregated_metrics.loc[metric, 'control']\n",
        "        lift = ((test_val - control_val) / control_val) * 100 if control_val != 0 else np.nan\n",
        "        # Append each metric row to the scorecard DataFrame\n",
        "        scorecard = scorecard.append({\n",
        "            'Metric': metric,\n",
        "            'Test': test_val,\n",
        "            'Control': control_val,\n",
        "            'Lift': lift,\n",
        "            't-stat': '',\n",
        "            'p-value': ''\n",
        "        }, ignore_index=True)\n",
        "\n",
        "# Make sure to set 'Metric' column as index if needed to match your scorecard structure\n",
        "# scorecard.set_index('Metric', inplace=True)\n",
        "\n",
        "# Step 11: Add t_stat and p_value for 'Conversion Rate' and 'Demand per Buyer' to the scorecard\n",
        "scorecard.loc[scorecard['Metric'] == 'Conversion Rate', ['t-stat', 'p-value']] = [t_stat_conversion, p_value_conversion]\n",
        "scorecard.loc[scorecard['Metric'] == 'Demand per Buyer', ['t-stat', 'p-value']] = [t_stat_demand_per_buyer, p_value_demand_per_buyer]\n",
        "\n",
        "# Step 12: Exclude 'Conversion Rate' and 'Demand per Buyer' from lift calculation if not already done\n",
        "scorecard.loc[scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 'Lift'] = np.nan # Set Lift to NaN for 'Conversion Rate' and 'Demand per Buyer' to denote exclusion\n",
        "\n",
        "# Step 13: Display the scorecard\n",
        "scorecard\n"
      ],
      "metadata": {
        "id": "FqOJ6HwOtsr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### V12"
      ],
      "metadata": {
        "id": "VUQAHxgWF3H-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Assuming 'df' is your DataFrame with necessary columns\n",
        "\n",
        "# Step 1: Add 'buyer' and 'prior_order' indicator columns\n",
        "df['buyer'] = (df['orders'] > 0).astype(int)\n",
        "df['demand_1_prior'] = np.where(df['1_priorOrder'] == 1, df['demand'], np.nan)\n",
        "df['demand_2_prior'] = np.where(df['2_priorOrder'] == 2, df['demand'], np.nan)\n",
        "df['member_1_prior'] = np.where(df['1_priorOrder'] == 1, 1, 0)\n",
        "df['member_2_prior'] = np.where(df['2_priorOrder'] == 2, 1, 0)\n",
        "\n",
        "# Step 2: Perform temporary calculations for DPB_1_prior and DPB_2_prior\n",
        "temp_calculations = df.groupby('test_control').agg({\n",
        "    'demand_1_prior': 'sum',\n",
        "    'demand_2_prior': 'sum',\n",
        "    'member_1_prior': 'sum',\n",
        "    'member_2_prior': 'sum',\n",
        "}).rename(columns={\n",
        "    'demand_1_prior': 'Total Demand 1 Prior',\n",
        "    'demand_2_prior': 'Total Demand 2 Prior',\n",
        "    'member_1_prior': 'Members 1 Prior',\n",
        "    'member_2_prior': 'Members 2 Prior',\n",
        "})\n",
        "\n",
        "# Step 3: Calculate DPB_1_prior and DPB_2_prior using temporary calculations\n",
        "dpb_1_prior = temp_calculations['Total Demand 1 Prior'] / temp_calculations['Members 1 Prior']\n",
        "dpb_2_prior = temp_calculations['Total Demand 2 Prior'] / temp_calculations['Members 2 Prior']\n",
        "\n",
        "# Step 4: Aggregate metrics\n",
        "aggregated_metrics = df.groupby('test_control').agg({\n",
        "    'upm_id': 'nunique',\n",
        "    'buyer': 'sum',\n",
        "    'orders': 'sum',\n",
        "    'units': 'sum',\n",
        "    'demand': ['sum', 'mean'],\n",
        "    '1_priorOrder': 'sum',\n",
        "    '2_priorOrder': 'sum',\n",
        "    'demand_1_prior': 'sum',\n",
        "    'demand_2_prior': 'sum',\n",
        "    'member_1_prior': 'sum',\n",
        "    'member_2_prior': 'sum',\n",
        "    'email_opens': 'sum',\n",
        "    'email_sent': 'sum',\n",
        "    'email_clicks': 'sum',\n",
        "    'site_app_visits': 'mean',\n",
        "    'PDP_FAVORITE_COUNT': 'mean',\n",
        "    'ADD_TO_CART_COUNT': 'mean',\n",
        "    'physical_activity': 'mean',\n",
        "    'base_FW_demand': 'mean',\n",
        "    'clr_FW_demand': 'mean',\n",
        "    'launch_FW_demand': 'mean',\n",
        "    'base_AP_demand': 'mean',\n",
        "    'clr_AP_demand': 'mean',\n",
        "    'launch_AP_demand': 'mean',\n",
        "    'base_EQ_demand': 'mean',\n",
        "    'clr_EQ_demand': 'mean',\n",
        "    'launch_EQ_demand': 'mean',\n",
        "})\n",
        "\n",
        "# Step 5: Correct the MultiIndex handling by flattening and renaming\n",
        "aggregated_metrics.columns = ['_'.join(col).strip() for col in aggregated_metrics.columns.values]\n",
        "aggregated_metrics = aggregated_metrics.rename(columns={\n",
        "    'upm_id_nunique': 'Total Members',\n",
        "    'buyer_sum': 'Buying Members',\n",
        "    'demand_mean': 'Demand per Buyer',\n",
        "    'demand_sum': 'Total Demand',\n",
        "    'units_sum': 'Total Units',\n",
        "    'orders_sum': 'Total Orders',\n",
        "    # Secondary KPIs - Repeat buyers\n",
        "    '1_priorOrder_sum': 'Members with 1 previous order',\n",
        "    '2_priorOrder_sum':'Members with 2 previous orders',\n",
        "    # Secondary KPIs - Site/app Engagement metrics\n",
        "    'site_app_visits_mean':'Visits per known member',\n",
        "    'PDP_FAVORITE_COUNT_mean':'PDP favorite per known member',\n",
        "    'ADD_TO_CART_COUNT_mean': 'Add to cart per known member',\n",
        "    'physical_activity_mean': 'Workouts per known member',\n",
        "})\n",
        "\n",
        "# Step 6: Calculate DPB_1_prior and DPB_2_prior based on the temporary calculations and integrate directly into the DataFrame\n",
        "# aggregated_metrics['DPB_1_prior'] = aggregated_metrics['demand_1_prior'] / aggregated_metrics['member_1_prior']\n",
        "# aggregated_metrics['DPB_2_prior'] = aggregated_metrics['demand_2_prior'] / aggregated_metrics['member_2_prior']\n",
        "\n",
        "# Step 7: Calculate additional metrics\n",
        "\n",
        "aggregated_metrics['Conversion Rate'] = aggregated_metrics['Buying Members'] / aggregated_metrics['Total Members']\n",
        "aggregated_metrics['Demand per Send'] = aggregated_metrics['Demand per Buyer'] * aggregated_metrics['Conversion Rate']\n",
        "# Secondary KPIs - order deep dive\n",
        "aggregated_metrics['AOV'] = aggregated_metrics['Total Demand'] / aggregated_metrics['Total Orders']\n",
        "aggregated_metrics['AUR'] = aggregated_metrics['Total Demand'] / aggregated_metrics['Total Units']\n",
        "aggregated_metrics['UPT'] = aggregated_metrics['Total Units'] / aggregated_metrics['Total Orders']\n",
        "aggregated_metrics['# Txn per Buyer'] = aggregated_metrics['Demand per Buyer']/aggregated_metrics['AOV']\n",
        "# Secondary KPIs - Repeat buyers\n",
        "aggregated_metrics['Conversion rate (1x buyers/email receivers)'] = aggregated_metrics['Members with 1 previous order'] / aggregated_metrics['Total Members']\n",
        "aggregated_metrics['Conversion rate (2x buyers/email receivers)'] = aggregated_metrics['Members with 2 previous orders'] / aggregated_metrics['Total Members']\n",
        "aggregated_metrics['Demand per Buyer (1 previous order)'] = aggregated_metrics['demand_1_prior_sum'] / aggregated_metrics.get('member_1_prior_sum', 1)\n",
        "aggregated_metrics['Demand per Buyer (2 previous order)'] = aggregated_metrics['demand_2_prior_sum'] / aggregated_metrics.get('member_2_prior_sum', 1)\n",
        "# Secondary KPIs - Email/Push Engagement metrics (all email/push received during measurement windows)\n",
        "aggregated_metrics['Email Open Rate (against sends)'] = aggregated_metrics['email_opens_sum'] / aggregated_metrics['email_sent_sum']\n",
        "aggregated_metrics['Email Click Rate (against sends)'] = aggregated_metrics['email_clicks_sum'] / aggregated_metrics['email_sent_sum']\n",
        "# Secondary KPIs - product mix\n",
        "aggregated_metrics['Footwear Demand per Member'] = ( aggregated_metrics['base_FW_demand_mean'] + aggregated_metrics['clr_FW_demand_mean'] + aggregated_metrics['launch_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "aggregated_metrics['Apparel Demand per Member'] = ( aggregated_metrics['base_AP_demand_mean'] + aggregated_metrics['clr_AP_demand_mean'] + aggregated_metrics['launch_AP_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "aggregated_metrics['Equipment Demand per Member'] = ( aggregated_metrics['base_EQ_demand_mean'] + aggregated_metrics['clr_EQ_demand_mean'] + aggregated_metrics['launch_EQ_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "# Secondary KPIs - LOB\n",
        "aggregated_metrics['Regular Product Demand per Member'] = ( aggregated_metrics['base_EQ_demand_mean'] + aggregated_metrics['base_AP_demand_mean'] + aggregated_metrics['base_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "\n",
        "aggregated_metrics['Clearance Demand per Member'] = ( aggregated_metrics['clr_EQ_demand_mean'] + aggregated_metrics['clr_AP_demand_mean'] + aggregated_metrics['clr_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "\n",
        "aggregated_metrics['Launch Demand per Member'] = ( aggregated_metrics['launch_EQ_demand_mean'] + aggregated_metrics['launch_AP_demand_mean'] + aggregated_metrics['launch_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "\n",
        "\n",
        "# Step 7: Remove temporary calculation columns as they're not needed in final DataFrame\n",
        "aggregated_metrics.drop(columns=['demand_1_prior_sum', 'demand_2_prior_sum', 'member_1_prior_sum', 'member_2_prior_sum', 'email_opens_sum','email_sent_sum', 'email_clicks_sum', 'base_FW_demand_mean',\n",
        " 'clr_FW_demand_mean', 'launch_FW_demand_mean', 'base_AP_demand_mean', 'clr_AP_demand_mean', 'launch_AP_demand_mean', 'base_EQ_demand_mean', 'clr_EQ_demand_mean','launch_EQ_demand_mean'], inplace=True)\n",
        "\n",
        "# Step 8: Perform t-test for 'Conversion Rate' and 'Demand per Buyer'\n",
        "\n",
        "# Separate the test and control groups to perform the t-tests correctly.\n",
        "test_group = df[df['test_control'] == 'test']\n",
        "control_group = df[df['test_control'] == 'control']\n",
        "\n",
        "# Conversion Rate t-test\n",
        "t_stat_conversion, p_value_conversion = stats.ttest_ind(test_group['buyer'], control_group['buyer'], equal_var=False)\n",
        "confidence_level_conversion = 1 - p_value_conversion\n",
        "type_i_error_conversion = p_value_conversion\n",
        "\n",
        "# Demand per Buyer t-test\n",
        "# 'nan' values imputed with 0\n",
        "\"\"\"\n",
        "t_stat_demand, p_value_demand = stats.ttest_ind(test_group['demand'].fillna(0), control_group['demand'].fillna(0), equal_var=False)\n",
        "confidence_level_demand = 1 - p_value_demand\n",
        "type_i_error_demand = p_value_demand\n",
        "\"\"\"\n",
        "# 'nan' values omitted\n",
        "t_stat_demand, p_value_demand = stats.ttest_ind(test_group['demand'], control_group['demand'], equal_var=False, nan_policy='omit')\n",
        "confidence_level_demand = 1 - p_value_demand\n",
        "type_i_error_demand = p_value_demand\n",
        "\"\"\" Note, results for demand ttest are the same with 'nan' values omitted or imputed to 0 \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "print(f\"T-statistic for Demand: {t_stat_demand}\")\n",
        "print(f\"P-value for Demand: {p_value_demand}\")\n",
        "print(f\"Confidence Level for Demand: {confidence_level_demand*100:.2f}%\")\n",
        "print(f\"Type I Error for Demand: {type_i_error_demand:.4f}\")\n",
        "\n",
        "\n",
        "print(f\"T-statistic for Demand: {t_stat_conversion}\")\n",
        "print(f\"P-value for Demand: {p_value_conversion}\")\n",
        "print(f\"Confidence Level for Demand: {confidence_level_conversion*100:.2f}%\")\n",
        "print(f\"Type I Error for Demand: {type_i_error_conversion:.4f}\")\n",
        "\"\"\"\n",
        "\n",
        "# Step 9: Transpose the table\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "aggregated_metrics = aggregated_metrics.T\n",
        "\n",
        "# Step 10: Calculate lift for all metrics except 'Conversion Rate' and 'Demand per Buyer'\n",
        "scorecard = pd.DataFrame(index=aggregated_metrics.index, columns=['Metric', 'Test', 'Control', 'Lift', 't-stat', 'p-value'])\n",
        "\n",
        "\"\"\"\n",
        "# Step 10: Calculate lift for all metrics except 'Conversion Rate' and 'Demand per Buyer'\n",
        "lift_columns = ['Metric', 'Test', 'Control', 'Lift', 't-stat', 'p-value']\n",
        "scorecard = pd.DataFrame(columns=lift_columns)\n",
        "\n",
        "for metric in aggregated_metrics.columns:\n",
        "    for metric in aggregated_metrics.index:  # Assuming metrics are now index after transpose\n",
        "      if metric not in ['Conversion Rate', 'Demand per Buyer']:\n",
        "            test_val = aggregated_metrics.loc[metric, 'test']\n",
        "            control_val = aggregated_metrics.loc[metric, 'control']\n",
        "            lift = ((test_val - control_val) / control_val) * 100 if control_val != 0 else np.nan\n",
        "      scorecard = scorecard.append({\n",
        "            'Metric': metric,\n",
        "            'Test': aggregated_metrics.loc[metric, 'test'],\n",
        "            'Control': aggregated_metrics.loc[metric, 'control'],\n",
        "            'Lift': lift,\n",
        "            't-stat': '',\n",
        "            'p-value': ''\n",
        "      }, ignore_index=True)\n",
        "\"\"\"\n",
        "\n",
        "# Populate Test and Control values from aggregated_metrics\n",
        "scorecard['Test'] = aggregated_metrics['test']\n",
        "scorecard['Control'] = aggregated_metrics['control']\n",
        "\n",
        "# Calculate Lift\n",
        "for metric in aggregated_metrics.index:\n",
        "    if metric not in ['Conversion Rate', 'Demand per Buyer']: # excluding Conversion_Rate and Demand_per_Buyer\n",
        "        test_val = scorecard.at[metric, 'Test']\n",
        "        control_val = scorecard.at[metric, 'Control']\n",
        "        lift = ((test_val - control_val) / control_val) * 100 if control_val != 0 else np.nan\n",
        "        scorecard.at[metric, 'Lift'] = lift\n",
        "\n",
        "# Step 11: Add t_stat and p_value for 'Conversion Rate' and 'Demand per Buyer'\n",
        "scorecard.at['Conversion Rate', 't-stat'] = t_stat_conversion\n",
        "scorecard.at['Conversion Rate', 'p-value'] = p_value_conversion\n",
        "scorecard.at['Demand per Buyer', 't-stat'] = t_stat_demand\n",
        "scorecard.at['Demand per Buyer', 'p-value'] = p_value_demand\n",
        "\n",
        "# Step 12: Ensure 'Metric' column is correctly populated\n",
        "# A byproduct of transposing the dataframe with some calculations only being applied selectively\n",
        "scorecard['Metric'] = scorecard.index\n",
        "scorecard.reset_index(drop=False, inplace=True)\n",
        "scorecard.drop(columns=['index'], inplace=True)\n",
        "\n",
        "# Step 13: Exclude 'Conversion Rate' and 'Demand per Buyer' from lift calculation\n",
        "scorecard.loc[scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 'Lift'] = ''\n",
        "scorecard.loc[~scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 't-stat'] = ''\n",
        "scorecard.loc[~scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 'p-value'] = ''\n",
        "\n",
        "# Step 14: Display the scorecard\n",
        "scorecard\n",
        "\n"
      ],
      "metadata": {
        "id": "pJankKgeF2o5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### V13 - Latest Working Version"
      ],
      "metadata": {
        "id": "WzG7pQpWbWO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Assuming 'df' is your DataFrame with necessary columns\n",
        "\n",
        "# Step 1: Add 'buyer' and 'prior_order' indicator columns\n",
        "df_ab['buyer'] = (df_ab['orders'] > 0).astype(int)\n",
        "df_ab['demand_1_prior'] = np.where(df_ab['1_priorOrder'] == 1, df_ab['demand'], np.nan)\n",
        "df_ab['demand_2_prior'] = np.where(df_ab['2_priorOrder'] == 1, df_ab['demand'], np.nan)\n",
        "df_ab['member_1_prior'] = np.where(df_ab['1_priorOrder'] == 1, 1, 0)\n",
        "df_ab['member_2_prior'] = np.where(df_ab['2_priorOrder'] == 1, 1, 0)\n",
        "\n",
        "# Step 2: Perform temporary calculations for DPB_1_prior and DPB_2_prior\n",
        "temp_calculations = df_ab.groupby('test_control').agg({\n",
        "    'demand_1_prior': 'sum',\n",
        "    'demand_2_prior': 'sum',\n",
        "    'member_1_prior': 'sum',\n",
        "    'member_2_prior': 'sum',\n",
        "}).rename(columns={\n",
        "    'demand_1_prior': 'Total Demand 1 Prior',\n",
        "    'demand_2_prior': 'Total Demand 2 Prior',\n",
        "    'member_1_prior': 'Members 1 Prior',\n",
        "    'member_2_prior': 'Members 2 Prior',\n",
        "})\n",
        "\n",
        "# Step 3: Calculate DPB_1_prior and DPB_2_prior using temporary calculations\n",
        "dpb_1_prior = temp_calculations['Total Demand 1 Prior'] / temp_calculations['Members 1 Prior']\n",
        "dpb_2_prior = temp_calculations['Total Demand 2 Prior'] / temp_calculations['Members 2 Prior']\n",
        "\n",
        "# Step 4: Aggregate metrics\n",
        "aggregated_metrics = df_ab.groupby('test_control').agg({\n",
        "    'upm_id': 'nunique',\n",
        "    'buyer': 'sum',\n",
        "    'orders': 'sum',\n",
        "    'units': 'sum',\n",
        "    'demand': ['sum', 'mean'],\n",
        "    '1_priorOrder': 'sum',\n",
        "    '2_priorOrder': 'sum',\n",
        "    'demand_1_prior': 'sum',\n",
        "    'demand_2_prior': 'sum',\n",
        "    'member_1_prior': 'sum',\n",
        "    'member_2_prior': 'sum',\n",
        "    'email_opens': 'sum',\n",
        "    'email_sent': 'sum',\n",
        "    'email_clicks': 'sum',\n",
        "    'site_app_visits': 'mean',\n",
        "    'PDP_FAVORITE_COUNT': 'mean',\n",
        "    'ADD_TO_CART_COUNT': 'mean',\n",
        "    'physical_activity': 'mean',\n",
        "    'base_FW_demand': 'mean',\n",
        "    'clr_FW_demand': 'mean',\n",
        "    'launch_FW_demand': 'mean',\n",
        "    'base_AP_demand': 'mean',\n",
        "    'clr_AP_demand': 'mean',\n",
        "    'launch_AP_demand': 'mean',\n",
        "    'base_EQ_demand': 'mean',\n",
        "    'clr_EQ_demand': 'mean',\n",
        "    'launch_EQ_demand': 'mean',\n",
        "})\n",
        "\n",
        "# Step 5: Correct the MultiIndex handling by flattening and renaming\n",
        "aggregated_metrics.columns = ['_'.join(col).strip() for col in aggregated_metrics.columns.values]\n",
        "aggregated_metrics = aggregated_metrics.rename(columns={\n",
        "    'upm_id_nunique': 'Total Members',\n",
        "    'buyer_sum': 'Buying Members',\n",
        "    'demand_mean': 'Demand per Buyer',\n",
        "    'demand_sum': 'Total Demand',\n",
        "    'units_sum': 'Total Units',\n",
        "    'orders_sum': 'Total Orders',\n",
        "    # Secondary KPIs - Repeat buyers\n",
        "    '1_priorOrder_sum': 'Members with 1 previous order',\n",
        "    '2_priorOrder_sum':'Members with 2 previous orders',\n",
        "    # Secondary KPIs - Site/app Engagement metrics\n",
        "    'site_app_visits_mean':'Visits per known member',\n",
        "    'PDP_FAVORITE_COUNT_mean':'PDP favorite per known member',\n",
        "    'ADD_TO_CART_COUNT_mean': 'Add to cart per known member',\n",
        "    'physical_activity_mean': 'Workouts per known member',\n",
        "})\n",
        "\n",
        "# Step 6: Calculate DPB_1_prior and DPB_2_prior based on the temporary calculations and integrate directly into the DataFrame\n",
        "# aggregated_metrics['DPB_1_prior'] = aggregated_metrics['demand_1_prior'] / aggregated_metrics['member_1_prior']\n",
        "# aggregated_metrics['DPB_2_prior'] = aggregated_metrics['demand_2_prior'] / aggregated_metrics['member_2_prior']\n",
        "\n",
        "# Step 7: Calculate additional metrics\n",
        "\n",
        "aggregated_metrics['Conversion Rate'] = aggregated_metrics['Buying Members'] / aggregated_metrics['Total Members']\n",
        "aggregated_metrics['Demand per Send'] = aggregated_metrics['Demand per Buyer'] * aggregated_metrics['Conversion Rate']\n",
        "# Secondary KPIs - order deep dive\n",
        "aggregated_metrics['AOV'] = aggregated_metrics['Total Demand'] / aggregated_metrics['Total Orders']\n",
        "aggregated_metrics['AUR'] = aggregated_metrics['Total Demand'] / aggregated_metrics['Total Units']\n",
        "aggregated_metrics['UPT'] = aggregated_metrics['Total Units'] / aggregated_metrics['Total Orders']\n",
        "aggregated_metrics['# Txn per Buyer'] = aggregated_metrics['Demand per Buyer']/aggregated_metrics['AOV']\n",
        "# Secondary KPIs - Repeat buyers\n",
        "aggregated_metrics['Conversion rate (1x buyers/email receivers)'] = aggregated_metrics['Members with 1 previous order'] / aggregated_metrics['Total Members']\n",
        "aggregated_metrics['Conversion rate (2x buyers/email receivers)'] = aggregated_metrics['Members with 2 previous orders'] / aggregated_metrics['Total Members']\n",
        "aggregated_metrics['Demand per Buyer (1 previous order)'] = aggregated_metrics['demand_1_prior_sum'] / aggregated_metrics.get('member_1_prior_sum', 1)\n",
        "aggregated_metrics['Demand per Buyer (2 previous order)'] = aggregated_metrics['demand_2_prior_sum'] / aggregated_metrics.get('member_2_prior_sum', 1)\n",
        "# Secondary KPIs - Email/Push Engagement metrics (all email/push received during measurement windows)\n",
        "aggregated_metrics['Email Open Rate (against sends)'] = aggregated_metrics['email_opens_sum'] / aggregated_metrics['email_sent_sum']\n",
        "aggregated_metrics['Email Click Rate (against sends)'] = aggregated_metrics['email_clicks_sum'] / aggregated_metrics['email_sent_sum']\n",
        "# Secondary KPIs - product mix\n",
        "aggregated_metrics['Footwear Demand per Member'] = ( aggregated_metrics['base_FW_demand_mean'] + aggregated_metrics['clr_FW_demand_mean'] + aggregated_metrics['launch_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "aggregated_metrics['Apparel Demand per Member'] = ( aggregated_metrics['base_AP_demand_mean'] + aggregated_metrics['clr_AP_demand_mean'] + aggregated_metrics['launch_AP_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "aggregated_metrics['Equipment Demand per Member'] = ( aggregated_metrics['base_EQ_demand_mean'] + aggregated_metrics['clr_EQ_demand_mean'] + aggregated_metrics['launch_EQ_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "# Secondary KPIs - LOB\n",
        "aggregated_metrics['Regular Product Demand per Member'] = ( aggregated_metrics['base_EQ_demand_mean'] + aggregated_metrics['base_AP_demand_mean'] + aggregated_metrics['base_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "\n",
        "aggregated_metrics['Clearance Demand per Member'] = ( aggregated_metrics['clr_EQ_demand_mean'] + aggregated_metrics['clr_AP_demand_mean'] + aggregated_metrics['clr_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "\n",
        "aggregated_metrics['Launch Demand per Member'] = ( aggregated_metrics['launch_EQ_demand_mean'] + aggregated_metrics['launch_AP_demand_mean'] + aggregated_metrics['launch_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "\n",
        "\n",
        "# Step 7: Remove temporary calculation columns as they're not needed in final DataFrame\n",
        "aggregated_metrics.drop(columns=['demand_1_prior_sum', 'demand_2_prior_sum', 'member_1_prior_sum', 'member_2_prior_sum', 'email_opens_sum','email_sent_sum', 'email_clicks_sum', 'base_FW_demand_mean',\n",
        " 'clr_FW_demand_mean', 'launch_FW_demand_mean', 'base_AP_demand_mean', 'clr_AP_demand_mean', 'launch_AP_demand_mean', 'base_EQ_demand_mean', 'clr_EQ_demand_mean','launch_EQ_demand_mean'], inplace=True)\n",
        "\n",
        "# Step 8: Perform t-test for 'Conversion Rate' and 'Demand per Buyer'\n",
        "\n",
        "# Separate the test and control groups to perform the t-tests correctly.\n",
        "test_group = df_ab[df_ab['test_control'] == 'test']\n",
        "control_group = df_ab[df_ab['test_control'] == 'control']\n",
        "\n",
        "# Conversion Rate Chi-square\n",
        "\n",
        "# Create a contingency table\n",
        "conv_contingency_table = pd.crosstab(df_ab['test_control'], df_ab['buyer']) # Count num of buyers and non-buyers in each group\n",
        "\n",
        "# Perform the Chi-square test\n",
        "chi2_stat_conversion, p_value_conversion, dof_conversion, expected_cnt_conversion = chi2_contingency(conv_contingency_table)\n",
        "\n",
        "# Demand per Buyer T-test\n",
        "\"\"\"\n",
        "# 'nan' values imputed with 0\n",
        "t_stat_demand, p_value_demand = stats.ttest_ind(test_group['demand'].fillna(0), control_group['demand'].fillna(0), equal_var=False)\n",
        "\"\"\"\n",
        "# 'nan' values omitted\n",
        "t_stat_demand, p_value_demand = stats.ttest_ind(test_group['demand'], control_group['demand'], equal_var=False, nan_policy='omit')\n",
        "\"\"\" Note, results for demand ttest are the same with 'nan' values omitted or imputed to 0 \"\"\"\n",
        "\n",
        "# Step 9: Transpose the table\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "aggregated_metrics = aggregated_metrics.T\n",
        "\n",
        "# Step 10: Calculate lift for all metrics except 'Conversion Rate' and 'Demand per Buyer'\n",
        "scorecard = pd.DataFrame(index=aggregated_metrics.index, columns=['Metric', 'Test', 'Control', 'Lift','p-value','T or Chi2 stat'])\n",
        "\n",
        "# Populate Test and Control values from aggregated_metrics\n",
        "scorecard['Test'] = aggregated_metrics['test']\n",
        "scorecard['Control'] = aggregated_metrics['control']\n",
        "\n",
        "# Calculate Lift\n",
        "for metric in aggregated_metrics.index:\n",
        "    if metric not in ['Conversion Rate', 'Demand per Buyer']: # excluding Conversion_Rate and Demand_per_Buyer\n",
        "        test_val = scorecard.at[metric, 'Test']\n",
        "        control_val = scorecard.at[metric, 'Control']\n",
        "        lift = ((test_val - control_val) / control_val) * 100 if control_val != 0 else np.nan\n",
        "        scorecard.at[metric, 'Lift'] = lift\n",
        "\n",
        "# Step 11: Add t_stat and p_value for 'Conversion Rate' and 'Demand per Buyer' to dataframe\n",
        "scorecard.at['Conversion Rate', 'T or Chi2 stat'] = chi2_stat_conversion\n",
        "scorecard.at['Conversion Rate', 'p-value'] = p_value_conversion\n",
        "scorecard.at['Demand per Buyer', 'T or Chi2 stat'] = t_stat_demand\n",
        "scorecard.at['Demand per Buyer', 'p-value'] = p_value_demand\n",
        "\n",
        "# Step 12: Ensure 'Metric' column is correctly populated\n",
        "# A byproduct of transposing the dataframe with some calculations only being applied selectively\n",
        "scorecard['Metric'] = scorecard.index\n",
        "scorecard.reset_index(drop=False, inplace=True)\n",
        "scorecard.drop(columns=['index'], inplace=True)\n",
        "\n",
        "# Step 13: Exclude 'Conversion Rate' and 'Demand per Buyer' from lift calculation\n",
        "scorecard.loc[scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 'Lift'] = ''\n",
        "scorecard.loc[~scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 'T or Chi2 stat'] = ''\n",
        "scorecard.loc[~scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 'p-value'] = ''\n",
        "\n",
        "# Step 14: Display the scorecard\n",
        "scorecard\n"
      ],
      "metadata": {
        "id": "2QSvPkDdbZjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%python\n",
        "\n",
        "alpha = 0.1\n",
        "\n",
        "print(f\"P-value for Demand: {p_value_demand:.4f}\")\n",
        "print(f\"P-value for Conversion: {p_value_conversion:.4f}\")\n",
        "\n",
        "# Check for statistical significance for Demand\n",
        "if p_value_demand < alpha:\n",
        "    print(\"Demand is Significant\")\n",
        "else:\n",
        "    print(\"Demand Not Significant\")\n",
        "\n",
        "# Check for statistical significance for Conversion\n",
        "if p_value_conversion < alpha:\n",
        "    print(\"Conversion is Significant\")\n",
        "else:\n",
        "    print(\"Conversion Not Significant\")"
      ],
      "metadata": {
        "id": "ncl9FDzfq4v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Mann Whitney U (Wilcox) Test - V1"
      ],
      "metadata": {
        "id": "Rqdr-JYRo_7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%python\n",
        "\n",
        "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html\n",
        "\n",
        "\"\"\"\n",
        "The Mann-Whitney U test is a nonparametric test of the null hypothesis that the distribution underlying sample x is the same as the distribution underlying sample y.\n",
        "\n",
        "It is often used as a test of difference in location between distributions.\n",
        "\"\"\"\n",
        "\n",
        "alpha = 0.1\n",
        "\n",
        "# result_rev = stats.mannwhitneyu(test_group['demand'], control_group['demand'])\n",
        "\n",
        "# print('p_value = ', result_rev[1].round(10))\n",
        "\n",
        "result_rev = stats.mannwhitneyu(test_group['demand'], control_group['demand'])\n",
        "\n",
        "print('p_value = ', result_rev[1].round(10))\n",
        "\n",
        "# Check for statistical significance\n",
        "if result_rev[1] < alpha:\n",
        "    print(\"Significant\")\n",
        "else:\n",
        "    print(\"Not Significant\")"
      ],
      "metadata": {
        "id": "XHb_-v4O4oCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Mann Whitney U (Wilcox) Test - V2"
      ],
      "metadata": {
        "id": "cx8eYyvupEbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html\n",
        "\n",
        "\"\"\"\n",
        "The Mann-Whitney U test is a nonparametric test of the null hypothesis that the distribution underlying sample x is the same as the distribution underlying sample y.\n",
        "\n",
        "It is often used as a test of difference in location between distributions.\n",
        "\"\"\"\n",
        "\n",
        "# Separate the test and control groups\n",
        "test_group_ab = df_ab[df_ab['test_control'] == 'test']\n",
        "control_group_ab = df_ab[df_ab['test_control'] == 'control']\n",
        "\n",
        "# Perform the Mann-Whitney U test on 'demand', omitting NaN values\n",
        "\"\"\"Note: It's important to handle NaN values. Depending on the data, we might want to exclude them.\"\"\"\n",
        "\n",
        "# u_statistic, p_value = stats.mannwhitneyu(test_group_ab['demand'], control_group_ab['demand'], alternative='two-sided', nan_policy='omit')\n",
        "# Returns a TypeError: mannwhitneyu() got an unexpected keyword argument 'nan_policy'\n",
        "\n",
        "mw_u_statistic_ab, mw_p_value_ab = stats.mannwhitneyu(test_group_ab['demand'].dropna(), control_group_ab['demand'].dropna(), alternative='two-sided')\n",
        "\n",
        "# Interpretation\n",
        "if mw_p_value_ab > alpha:\n",
        "    print(f\"\\nMann Whitney U Statistic: {mw_u_statistic_ab}\")\n",
        "    print(f\"P-value: {mw_p_value_ab}\")\n",
        "    print(f\"\\nP-value at: {mw_p_value_ab}. This is above alpha at {alpha}.\")\n",
        "    print(f\"\\nTherefore, we do not reject the null hypothesis (that there is no difference between the distributions of the two indepdent samples being compared).\")\n",
        "else:\n",
        "    print(f\"\\nMann Whitney U Statistic: {mw_u_statistic_ab}\")\n",
        "    print(f\"\\nP-value: {mw_p_value_ab}\")\n",
        "    print(f\"\\nP-value at: {mw_p_value_ab}. This is below alpha at {alpha}.\")\n",
        "    print(f\"\\nTherefore, we reject the null hypothesis (that there is no difference between the distributions of the two indepdent samples being compared).\")"
      ],
      "metadata": {
        "id": "52XfzPt0pE_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results**\n",
        "\n",
        "* Mann Whitney U Statistic: 2910485.0\n",
        "* P-value: 0.7567487973086111"
      ],
      "metadata": {
        "id": "3w7uqArG_vR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TLDR Summary\n",
        "\n",
        "When interpreting these values:\n",
        "- A low **p-value** (<0.10) across either metric suggests the test group's performance is statistically significantly different from the control group's, indicating our test has likely influenced buying behavior.\n",
        "- The $\\chi$<sup>2</sup> and **t-statistics** provide a measure of the *magnitude* of this difference, but the p-value helps us understand if the difference is statistically significant."
      ],
      "metadata": {
        "id": "uZmOcA_lmDfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding the Chi-square Statistic and p-value for Conversion\n",
        "\n",
        "**Conversion Analysis:**\n",
        "\n",
        "We created a binary column called 'buyer' which contains a 1 for members who made a purchase, and a 0 for members who did not.\n",
        "\n",
        "We then compared the proportion of buyers to non-buyers between the test and control groups.\n",
        "\n",
        "We used the **Chi-square (\\(\\chi^2\\)) $\\chi$<sup>2</sup> statistic** and **p-value** for this purpose.\n",
        "\n",
        "- **Chi-square Statistic $\\chi$<sup>2</sup>**: This number tells us how much the observed conversion rates deviate from what we would expect if there were no difference between the test and control groups.\n",
        "\n",
        "  > A higher **$\\chi$<sup>2</sup>**  value indicates a larger deviation from the expected, no-effect scenario.\n",
        "\n",
        "- **p-value**: This value tells us the probability of observing our results (or more extreme) if there were truly no difference in conversion rates between the test and control groups.\n",
        "\n",
        "  > A low p-value (<0.10 in our case) suggests that the observed differences are statistically significant, meaning it's unlikely they occurred by chance.\n",
        "\n",
        "**Interpretation**:\n",
        "- If the p-value is below our predefined threshold (e.g., $\\alpha$ is 0.10), we conclude that there is a statistically significant difference in conversion rates between the test and control groups.\n",
        "- This suggests that the test has had a measurable impact on conversion.\n",
        "- Conversely, a p-value above $\\alpha$ suggests that any observed differences in conversion rates could likely be due to random variation, and we do not have sufficient evidence to say the test affected conversion rates."
      ],
      "metadata": {
        "id": "hBaZ77mNgre2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding the u-statistic and p-value for Demand\n",
        "\n",
        "\n",
        "#### Mann-Whitney U Test: Detailed Explanation\n",
        "\n",
        "* The Mann-Whitney U test assesses whether one of two groups of independent observations tends to have larger values than the other.\n",
        "* Unlike the t-test, it does not require the data to follow a normal distribution or have equal variances, making it a robust alternative for ordinal data or non-normally distributed interval data.\n",
        "\n",
        "#### U-Statistic Explained\n",
        "\n",
        "The U-statistic quantifies the difference between the distributions of the two groups.\n",
        "* It's calculated by ranking all observations from both groups together and then summing the ranks for each group.\n",
        "* The actual U statistic value is the number of times observations in one group precede observations in the other group in this ranking.\n",
        "* It reflects the magnitude of overlap between the two groups; **smaller values indicate less overlap and a greater difference between the groups**.\n",
        "\n",
        "#### High U-Statistic and High P-Value: Interpretation\n",
        "\n",
        "In our case:\n",
        "1.  A high U-statistic (2910485.0)\n",
        "2.  A high p-value (0.7567487973086111)\n",
        "\n",
        "**High U-Statistic:**\n",
        "  * This indicates a significant overlap in the ranks of the two groups.\n",
        "  * In practical terms, it suggests that the distributions of the two groups are similar.\n",
        "  > No group consistently ranks higher than the other across all observations.\n",
        "\n",
        "**High P-Value:**\n",
        "  * A high p-value suggests that the observed data are highly compatible with the null hypothesis of no difference between the groups.\n",
        "  > In this context, the p-value indicates that any observed differences in the central tendencies of the two groups could easily occur by random chance alone.\n",
        "\n",
        "#### Practical Interpretation\n",
        "\n",
        "The combination of a high U-statistic and a high p-value in the context of a Mann-Whitney U test suggests that there is no statistically significant difference in the distribution of demand between the test and control groups.\n",
        "\n",
        "The data do not provide sufficient evidence to reject the null hypothesis of equality in distributions.\n",
        "\n",
        "The intervention or change tested does not have a statistically significant effect on demand, and any observed differences might be due to chance variation in the sample data.\n"
      ],
      "metadata": {
        "id": "K7wsYVHGl_rw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding the t-statistic and p-value for Demand\n",
        "\n",
        "**Demand Analysis**\n",
        "\n",
        "Looks at whether the average amount spent (demand) per buyer differs significantly between the test and control groups. We utilized the **t-statistic** and **p-value** for assessing this.\n",
        "\n",
        "- **t-statistic**: This value measures the size of the difference between the group means relative to the variation observed in the groups.\n",
        "  > A larger absolute value of the t-statistic indicates a greater difference between the groups.\n",
        "- **p-value**: Similar to the Chi-square test, the p-value here indicates the probability of observing our results (or more extreme) if there were truly no difference in average demand between the test and control groups.\n",
        "  > A low p-value (<0.10 in our case) indicates that the differences in demand are statistically significant.\n",
        "\n",
        "**Interpretation**:\n",
        "-  If the p-value is below our predefined threshold (e.g., $\\alpha$ is 0.10), it means there is a statistically significant difference in the average demand between the test and control groups, suggesting that the test has likely influenced buying behavior.\n",
        "- A p-value above $\\alpha$ indicates that any observed differences in average demand could be due to chance, and we do not have enough evidence to claim an effect of the test on demand.\n"
      ],
      "metadata": {
        "id": "1ihPGvAbFnPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing the Underlying Distribution"
      ],
      "metadata": {
        "id": "md5jCCMdDhtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%python\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plotting Demand Distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(test_group['demand'].dropna(), color=\"skyblue\", label=\"Test Group Demand\", kde=True)\n",
        "sns.histplot(control_group['demand'].dropna(), color=\"red\", label=\"Control Group Demand\", kde=True)\n",
        "plt.title('Demand Distribution - Test vs Control')\n",
        "plt.xlabel('Demand')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plotting the buyer status in both test and control groups.\n",
        "# Assumes a binary 'buyer' column where 1 indicates a buyer and 0 indicates a non-buyer.\n",
        "%python\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.countplot(x='buyer', data=test_group)\n",
        "plt.title('Buyer Status Counts - Test Group')\n",
        "plt.xlabel('Buyer Status')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.countplot(x='buyer', data=control_group)\n",
        "plt.title('Buyer Status Counts - Control Group')\n",
        "plt.xlabel('Buyer Status')\n",
        "plt.ylabel('Count')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Conversion through Confidence Intervals\n",
        "%python\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate the confidence interval\n",
        "def proportion_confint(count, nobs, alpha=0.05, method='normal'):\n",
        "    quantile = stats.norm.ppf(1 - alpha / 2.)\n",
        "    center = count / nobs\n",
        "    error = quantile * np.sqrt(center * (1 - center) / nobs)\n",
        "    return center - error, center + error\n",
        "\n",
        "# Calculating conversion rate and confidence intervals\n",
        "test_success = test_group['buyer'].sum()\n",
        "test_total = test_group['buyer'].count()\n",
        "test_cr, test_ci_lower, test_ci_upper = test_success / test_total, *proportion_confint(test_success, test_total)\n",
        "\n",
        "control_success = control_group['buyer'].sum()\n",
        "control_total = control_group['buyer'].count()\n",
        "control_cr, control_ci_lower, control_ci_upper = control_success / control_total, *proportion_confint(control_success, control_total)\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(['Test', 'Control'], [test_cr, control_cr], yerr=[[test_cr - test_ci_lower, control_cr - control_ci_lower],[test_ci_upper - test_cr, control_ci_upper - control_cr]], capsize=10)\n",
        "ax.set_ylabel('Conversion Rate')\n",
        "ax.set_title('Conversion Rate with 95% Confidence Intervals')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\"\"\" The proportion_confint function calculates confidence intervals for each group's conversion rate.\n",
        "These intervals are then plotted as error bars on a bar chart, giving a clearer picture of the data's variability\n",
        "and how the groups compare. \"\"\""
      ],
      "metadata": {
        "id": "Wi2kRnhSDo9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TBTLPp-eDhj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Error\n",
        "\n",
        "KeyError: 'test'\n",
        "\n",
        "Code: `lift = ((aggregated_metrics.loc['test', metric] - aggregated_metrics.loc['control', metric]) /`\n",
        "\n",
        "Here is my code:\n",
        "\n",
        "\n",
        "**Fix:**\n",
        "\n",
        "*Adjust Access in Step 10: If 'test' and 'control' are indeed column names after transposition, your current loop should work. If they are not, ensure they are correctly set during the transposition.*"
      ],
      "metadata": {
        "id": "6GxSszbGyZA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming aggregated_metrics is transposed correctly and 'test', 'control' are column names\n",
        "if 'test' in aggregated_metrics.columns and 'control' in aggregated_metrics.columns:\n",
        "    for metric in aggregated_metrics.index:  # Assuming metrics are now index after transpose\n",
        "        if metric not in ['Conversion Rate', 'Demand per Buyer']:\n",
        "            test_val = aggregated_metrics.loc[metric, 'test']\n",
        "            control_val = aggregated_metrics.loc[metric, 'control']\n",
        "            lift = ((test_val - control_val) / control_val) * 100 if control_val != 0 else np.nan\n",
        "            print(metric, lift)  # For demonstration, replace with how you want to store this lift value\n",
        "else:\n",
        "    print(\"Column names 'test' and 'control' not found. Please check DataFrame structure.\")\n"
      ],
      "metadata": {
        "id": "jqV4X7Lv7kVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0: Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Assuming 'df' is your DataFrame with necessary columns\n",
        "\n",
        "# Step 1 through Step 7 remain unchanged as you've described\n",
        "\n",
        "# Step 8: Perform t-test for 'Conversion Rate' and 'Demand per Buyer'\n",
        "# This step also remains as you've described, performing t-tests for the specific metrics\n",
        "\n",
        "# Step 9: Transpose the aggregated metrics for easier handling\n",
        "# This step remains as you've described\n",
        "\n",
        "# Integrating Steps 10 and 11 with modifications for Step 12\n",
        "\n",
        "# Step 10: Calculate lift for all metrics except 'Conversion Rate' and 'Demand per Buyer'\n",
        "# Adjustments to include the calculation of t_stat and p_value directly in the DataFrame for specific metrics\n",
        "aggregated_metrics['Lift'] = np.nan\n",
        "aggregated_metrics.loc['Conversion Rate', 't_stat'] = t_stat_conversion\n",
        "aggregated_metrics.loc['Conversion Rate', 'p_value'] = p_value_conversion\n",
        "aggregated_metrics.loc['Demand per Buyer', 't_stat'] = t_stat_demand_per_buyer\n",
        "aggregated_metrics.loc['Demand per Buyer', 'p_value'] = p_value_demand_per_buyer\n",
        "\n",
        "# Calculate lift only for metrics other than specified\n",
        "for metric in aggregated_metrics.index:\n",
        "    if metric not in ['Conversion Rate', 'Demand per Buyer']:\n",
        "        control = aggregated_metrics.loc[metric, 'Control']\n",
        "        test = aggregated_metrics.loc[metric, 'Test']\n",
        "        if control != 0:  # Prevent division by zero\n",
        "            aggregated_metrics.loc[metric, 'Lift'] = ((test - control) / control) * 100\n",
        "\n",
        "# Step 12: Final preparation of the scorecard\n",
        "# Since 'Lift', 't_stat', and 'p_value' are now part of aggregated_metrics, there's no need for a separate action here\n",
        "\n",
        "# Step 13: Display the final scorecard with adjustments\n",
        "# Convert aggregated metrics to a format suitable for display\n",
        "scorecard = aggregated_metrics[['Test', 'Control', 'Lift', 't_stat', 'p_value']].reset_index().rename(columns={'index': 'Metric'})\n",
        "scorecard.fillna('', inplace=True)  # Replace NaN with empty strings for cleanliness\n",
        "\n",
        "print(scorecard)\n"
      ],
      "metadata": {
        "id": "-KjIzGEHpeai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simple calculation to replicate their results in Excel\n",
        "\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Given values\n",
        "z_score = 0.43\n",
        "alpha = 0.10\n",
        "\n",
        "# Calculating the p-value for the right-tailed test\n",
        "p_value = 1 - norm.cdf(z_score)\n",
        "\n",
        "# Check if the result is statistically significant\n",
        "is_significant = p_value < alpha\n",
        "\n",
        "p_value, is_significant"
      ],
      "metadata": {
        "id": "ot-uiv_d8r-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistical Power Analysis"
      ],
      "metadata": {
        "id": "EIJGJBMAdDLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Minimum Detectable Effect"
      ],
      "metadata": {
        "id": "HXx3UcLLm9Qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.power import NormalIndPower\n",
        "from statsmodels.stats.proportion import proportion_effectsize\n",
        "\n",
        "# Example data points\n",
        "baseline_conversion_rate = 0.10  # Control group conversion rate\n",
        "mde = 0.02  # Minimum detectable effect (a 2% increase in conversion rate)\n",
        "alpha = 0.05  # Significance level\n",
        "power = 0.8  # Desired power\n",
        "current_sample_size_per_group = 500  # Number of observations in each group\n",
        "\n",
        "# Calculate the effect size based on baseline and MDE\n",
        "effect_size = proportion_effectsize(baseline_conversion_rate, baseline_conversion_rate + mde)\n",
        "\n",
        "# Initialize the power analysis object for a z-test\n",
        "power_analysis = NormalIndPower()\n",
        "\n",
        "# Calculate required sample size\n",
        "required_sample_size = power_analysis.solve_power(effect_size=effect_size, power=power, alpha=alpha, ratio=1, alternative='larger')\n",
        "\n",
        "print(f\"Required Sample Size per Group: {required_sample_size}\")\n",
        "print(f\"Current Sample Size per Group: {current_sample_size_per_group}\")\n",
        "\n",
        "if current_sample_size_per_group >= required_sample_size:\n",
        "    print(\"You have collected enough data to reliably detect the MDE with the desired power and significance level.\")\n",
        "else:\n",
        "    remaining_sample_size = required_sample_size - current_sample_size_per_group\n",
        "    print(f\"More data needed. You need approximately {remaining_sample_size} more observations per group to reach the desired power level.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrT9jR7CdCZv",
        "outputId": "6b5816d7-9b65-4104-9aa0-552092435f1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required Sample Size per Group: [10.]\n",
            "Current Sample Size per Group: 500\n",
            "You have collected enough data to reliably detect the MDE with the desired power and significance level.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/stats/power.py:133: RuntimeWarning: invalid value encountered in sqrt\n",
            "  pow_ = stats.norm.sf(crit - d*np.sqrt(nobs)/sigma)\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/stats/power.py:525: ConvergenceWarning: \n",
            "Failed to converge on a solution.\n",
            "\n",
            "  warnings.warn(convergence_doc, ConvergenceWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### w/ Confidence Interval"
      ],
      "metadata": {
        "id": "fx8fkCkmm05U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Observed means, standard deviations, and sample sizes from your test and control groups\n",
        "mean_test = 5.1  # Mean of the test group\n",
        "mean_control = 5.0  # Mean of the control group\n",
        "std_test = 1.5  # Standard deviation of the test group\n",
        "std_control = 1.4  # Standard deviation of the control group\n",
        "n_test = 150  # Sample size of the test group\n",
        "n_control = 150  # Sample size of the control group\n",
        "\n",
        "# Calculate pooled standard deviation\n",
        "std_pooled = np.sqrt(((n_test - 1) * std_test ** 2 + (n_control - 1) * std_control ** 2) / (n_test + n_control - 2))\n",
        "\n",
        "# Calculate the standard error of the difference in means\n",
        "se_diff = std_pooled * np.sqrt(1/n_test + 1/n_control)\n",
        "\n",
        "# Determine the Z-score for a 90% CI\n",
        "z_score = stats.norm.ppf(0.95)  # This is for a 90% CI, using the 95th percentile\n",
        "\n",
        "# Calculate the confidence intervals\n",
        "ci_lower = (mean_test - mean_control) - z_score * se_diff\n",
        "ci_upper = (mean_test - mean_control) + z_score * se_diff\n",
        "\n",
        "# Display the confidence interval\n",
        "print(f\"90% Confidence Interval for the difference in means: [{ci_lower:.3f}, {ci_upper:.3f}]\")\n",
        "\n",
        "# Continue with MDE calculations\n",
        "effect_size = (mean_test - mean_control) / std_pooled\n",
        "alpha = 0.05  # Significance level for a one-tailed test\n",
        "power = 0.8  # Desired power\n",
        "\n",
        "# Calculate the sample size required for the given MDE\n",
        "sample_size_per_group = stats.norm.ppf(1 - alpha) ** 2 * (std_pooled ** 2) * ((1 / effect_size) ** 2)\n",
        "print(f\"Sample size per group for MDE ({effect_size*100:.2f}% effect size) with 80% power and 5% significance level: {np.ceil(sample_size_per_group)}\")\n"
      ],
      "metadata": {
        "id": "Wo3bdWdbm3wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Things to look for in the Confidence Interval\n",
        "\n",
        "\n",
        "1.   **Width of the CI:** The wider the confidence interval, the more uncertainty there is about the precise effect of the treatment.  \n",
        "> A narrow CI indicates more precision about the treatment's effect size.\n",
        "2. **If CI Includes 0:** If the confidence interval includes 0 (*e.g., [-0.2, 0.3]*), it means there's a possibility that there is no real difference between the test and control groups.\n",
        "> The treatment effect could be positive, negative, or non-existent, and we cannot be confident that the treatment had a significant effect.\n",
        "3. **Effect Size:** Even if a result is statistically significant (e.g., the CI does not include 0), the effect size matters for practical decisions.\n",
        "> If the CI shows a very small effect size that is not meaningful for our business objectives, we might decide not to implement the change even though it showed a statistically significant difference.\n",
        "\n"
      ],
      "metadata": {
        "id": "YRpRU__3nFMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate a one-tailed t-test in python\n",
        "\n",
        "from scipy.stats import ttest_1samp\n",
        "\n",
        "# Sample data\n",
        "data = [10, 12, 14, 15, 16, 17, 18, 19, 20]\n",
        "\n",
        "# Set the null hypothesis mean\n",
        "null_mean = 15\n",
        "\n",
        "# Perform the one-tailed t-test\n",
        "t_statistic, p_value = ttest_1samp(data, null_mean, alternative=\"greater\")\n",
        "\n",
        "# Print the results\n",
        "print(\"t-statistic:\", t_statistic)\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "# Interpret the results\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis. The sample mean is significantly greater than the null mean.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. The sample mean is not significantly greater than the null mean.\")\n"
      ],
      "metadata": {
        "id": "uKzFzPbRn2w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Code for AB Period Measurment w/ Combined Platforms"
      ],
      "metadata": {
        "id": "B4AyztJgMeyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Correct Results\n",
        "\n"
      ],
      "metadata": {
        "id": "WDhBsIofCWSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "-----------------------------------------------------------------------------------------------------------------------------------\n",
        "-- AB Period\n",
        "-----------------------------------------------------------------------------------------------------------------------------------\n",
        "WITH\n",
        "\n",
        "-- use audience file to determine send dates when holdout audience is used\n",
        "tc_app_sends AS (\n",
        "    SELECT upm_id\n",
        "    , CASE WHEN final_exposure_or_holdout like ${hivevar:node_tst_str} THEN 'test'\n",
        "        -- WHEN final_exposure_or_holdout like ${hivevar:node_ctl_str_2} THEN 'holdout'\n",
        "        WHEN final_exposure_or_holdout like ${hivevar:node_ctl_str} THEN 'control'\n",
        "      ELSE 'na' END AS test_control\n",
        "    , LEFT(timestamp,10) AS campaign_send_dt\n",
        "    , CAST(LEFT(timestamp,19) AS TIMESTAMP) AS campaign_send_ts\n",
        "    FROM ${hivevar:aud_table} aud\n",
        "    ANTI JOIN contam_base c ON aud.upm_id = c.upm_id\n",
        "    ANTI JOIN gwan13.bot_master_nike_com bot ON aud.upm_id = bot.upm_id\n",
        "    ANTI JOIN aud_select_workspace.reseller_new slr ON aud.upm_id = slr.upm_user_id\n",
        "    WHERE CAST(LEFT(timestamp,10) AS DATE) BETWEEN ${hivevar:msmt_start_dt} AND ${hivevar:msmt_end_dt}\n",
        ")\n",
        "\n",
        ", tc_first_last_send AS (\n",
        "  SELECT upm_id\n",
        "  , MIN(campaign_send_dt) as first_send_dt\n",
        "  , MAX(campaign_send_dt) as last_send_dt\n",
        "  FROM tc_app_sends\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", test_control_table AS (\n",
        "    SELECT aud.*\n",
        "    , fls.first_send_dt\n",
        "    , fls.last_send_dt\n",
        "    , LEAD(campaign_send_dt) OVER (PARTITION BY aud.upm_id ORDER BY campaign_send_dt) AS next_send_dt\n",
        "    , LAG(campaign_send_dt) OVER (PARTITION BY aud.upm_id ORDER BY campaign_send_dt) AS previous_send_dt\n",
        "    , (int(to_timestamp(campaign_send_dt)) - int(to_timestamp(LEAD(campaign_send_dt) OVER (PARTITION BY aud.upm_id ORDER BY campaign_send_dt))))/86400 AS difference_next_send_days\n",
        "    , (int(to_timestamp(campaign_send_dt)) - int(to_timestamp(LAG(campaign_send_dt) OVER (PARTITION BY aud.upm_id ORDER BY campaign_send_dt))))/86400 AS difference_previous_send_days\n",
        "    FROM tc_app_sends aud\n",
        "    INNER JOIN tc_first_last_send fls ON aud.upm_id = fls.upm_id\n",
        ")\n",
        "\n",
        ", view_dol AS (\n",
        "  SELECT dol.upm_id\n",
        "  , order_nbr\n",
        "  , order_dt\n",
        "  , origl_ordered_qty\n",
        "  , grd_amt_excl_tax_usd\n",
        "  , line_of_business_desc\n",
        "  , univ_div_desc\n",
        "  , first_send_dt\n",
        "  , last_send_dt\n",
        "  FROM awight.na_digital_order_line_snapshot dol\n",
        "  INNER JOIN tc_first_last_send fls ON dol.upm_id = fls.upm_id\n",
        "  WHERE rec_excl_ind = 0\n",
        "    AND ttl_demand_ind = 1\n",
        "    AND univ_cat_desc <> 'Converse'\n",
        "    AND dol.upm_id IS NOT NULL\n",
        "    AND order_dt BETWEEN DATE_SUB(first_send_dt,730) AND DATE_ADD(last_send_dt,7)\n",
        "    AND (rtn_qty = 0 or rtn_qty IS NULL)\n",
        ")\n",
        "\n",
        "-- AA period purchase. dynamically based on when the member receives their first communication. Only returns demand for members in the\n",
        ", aa_purchase AS (\n",
        "  SELECT dol.upm_id\n",
        "    ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "    ,COUNT(DISTINCT order_nbr) AS orders\n",
        "    ,SUM(origl_ordered_qty) AS units\n",
        "  FROM view_dol dol\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,31) AND DATE_SUB(first_send_dt,1)\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", demand_bucket AS (\n",
        "  SELECT tct.upm_id\n",
        "  , tct.test_control\n",
        "  , aa.upm_id as upm_id_aa_buyer\n",
        "  , aa.demand\n",
        "  , aa.orders\n",
        "  , aa.units\n",
        "  , CASE\n",
        "    WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "    WHEN aa.demand < 50 THEN '000-050'\n",
        "    WHEN aa.demand < 100 THEN '050-100'\n",
        "    WHEN aa.demand < 200 THEN '100-200'\n",
        "    WHEN aa.demand < 500 THEN '200-500'\n",
        "    WHEN aa.demand < 1000 THEN '500-1000'\n",
        "    WHEN aa.demand >=1000 THEN '1000+'\n",
        "    END AS demand_per_buyer_bucket\n",
        "  , PERCENT_RANK() OVER (\n",
        "    PARTITION BY test_control\n",
        "    , CASE\n",
        "      WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "      WHEN aa.demand < 50 THEN '000-050'\n",
        "      WHEN aa.demand < 100 THEN '050-100'\n",
        "      WHEN aa.demand < 200 THEN '100-200'\n",
        "      WHEN aa.demand < 500 THEN '200-500'\n",
        "      WHEN aa.demand < 1000 THEN '500-1000'\n",
        "      WHEN aa.demand >=1000 THEN '1000+'\n",
        "      END ORDER BY RANDOM(0)\n",
        "    ) AS percent_rank\n",
        "  FROM test_control_table tct\n",
        "  LEFT JOIN aa_purchase aa ON tct.upm_id = aa.upm_id\n",
        ")\n",
        "\n",
        ", balanced_aud_aa as (\n",
        "  SELECT DISTINCT upm_id\n",
        "  FROM demand_bucket db\n",
        "  -- remove outliers\n",
        "  WHERE (demand < 2400 OR demand IS NULL)\n",
        "-- To rebalance, paste text block from template in place of defaults\n",
        "AND ((test_control = 'control' and (demand > 0 and demand <= 50) and PERCENT_RANK <= 0.97632 )\n",
        "OR (test_control = 'control' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 1 )\n",
        "OR (test_control = 'control' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 0.99509 )\n",
        "OR (test_control = 'control' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 1 )\n",
        "OR (test_control = 'control' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 0.90741 )\n",
        "OR (test_control = 'control' and (demand > 1000) and PERCENT_RANK <= 0.98161 )\n",
        "OR (test_control = 'control' and (demand = 0 or demand is null) and PERCENT_RANK <= 0.99998 )\n",
        "OR (test_control = 'test' and (demand >0 and demand <= 50) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 0.98934 )\n",
        "OR (test_control = 'test' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 0.98953 )\n",
        "OR (test_control = 'test' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand = 0 or demand is null) and PERCENT_RANK <= 1)\n",
        "))\n",
        "\n",
        ", mhub as (\n",
        " SELECT source_id as upm_id\n",
        " , nuid\n",
        " , member_id\n",
        " , preferred_gender\n",
        " FROM MEMBER.MEMBER_HUB\n",
        " WHERE ctry_2_cd = 'US'\n",
        " AND lower(dpa_status) IN ('activate','reactivate')\n",
        ")\n",
        "\n",
        "-- calculate a dynamic member activity window for each communication received by a member\n",
        "-- activity is calculated on a 7 day window, unless the member receives a communication within that timeframe\n",
        "-- When multiple messages are received, activity is only calculated up to the next message_send_dt to avoid double counting\n",
        ", member_activities as (\n",
        "  SELECT mgd.member_id\n",
        "  ,SUM(logged_in_visits_count) AS visits\n",
        "  ,SUM(pdp_favorite_count) AS pdp_favorite_count\n",
        "  ,SUM(add_to_cart_count) AS add_to_cart_count\n",
        "  ,SUM(physical_activity_count) AS physical_activity\n",
        "  ,MAX(CASE WHEN new_user_experience_flag = 'Y' THEN 1 ELSE 0 END) AS new_user_flag\n",
        "  ,SUM(eod_chat_count) AS NEOD_Session\n",
        "  FROM member_agd mgd --aud_select_workspace.member_agg_member_growth_daily mgd\n",
        "  INNER JOIN mhub mh ON mgd.member_id = mh.member_id\n",
        "  INNER JOIN test_control_table tct ON mh.upm_id = tct.upm_id\n",
        "  WHERE mgd.activity_dt >= tct.campaign_send_dt\n",
        "    AND mgd.activity_dt < DATE_ADD(campaign_send_dt, INT(LEAST(7,COALESCE(difference_next_send_days,7))))\n",
        "    AND mgd.preferred_retail_geo = 'NA'\n",
        "    AND mgd.experience_name != 'agnostic'\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        "-- for each user, aggregate each order placed within our measurement window. create labels to categorize the type of demand\n",
        ", ab_purchase as (\n",
        "  SELECT dol.upm_id as upm_id_ab_buyer\n",
        "  , order_dt as order_ts\n",
        "  , order_nbr\n",
        "  ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc NOT IN ('Base Inline', 'Clearance', 'Launch') THEN grd_amt_excl_tax_usd ELSE 0 END) AS rest_demand\n",
        "  ,COUNT(DISTINCT order_nbr) AS orders\n",
        "  ,SUM(origl_ordered_qty) AS units\n",
        "  FROM view_dol dol\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN last_send_dt AND DATE_ADD(last_send_dt,7)\n",
        "  GROUP BY 1,2,3\n",
        ")\n",
        "\n",
        "-- counting the first order demand after each send\n",
        ", reset_per_campaign_sent AS (\n",
        " SELECT a.*\n",
        "  ,CONCAT(a.upm_id, ' ', a.campaign_send_dt ) AS touchpoint_per_member\n",
        "  FROM test_control_table a -- should it be created based on buyers/\n",
        "  WHERE upm_id IN (SELECT DISTINCT upm_id_ab_buyer FROM ab_purchase )\n",
        "  )\n",
        "\n",
        "\n",
        ", aud_order_join AS    (\n",
        "  SELECT a.*\n",
        "  , b.*\n",
        "  ,round(((bigint(to_timestamp(a.campaign_send_ts)))-(bigint(to_timestamp(b.order_ts))))/(60),3) AS order_send_diff_min\n",
        "  ,RANK() OVER(PARTITION BY  a.touchpoint_per_member  ORDER BY (((bigint(to_timestamp(a.campaign_send_ts)))-(bigint(to_timestamp(b.order_ts))))/(60)) DESC, a.campaign_send_dt ) AS nearest_order\n",
        "  FROM reset_per_campaign_sent a\n",
        "  LEFT JOIN ab_purchase b\n",
        "  ON (a.upm_id = b.upm_id_ab_buyer AND b.order_ts >= a.campaign_send_ts AND b.order_ts <= DATE_ADD(CAST(a.`campaign_send_dt` AS DATE), CAST(LEAST(7, COALESCE(a.`difference_next_send_days`, 7)) AS INT)))\n",
        "  ORDER BY a.upm_id, a.campaign_send_dt ASC\n",
        ")\n",
        "\n",
        "\n",
        "-- Aggregate each distinct send, attributing only the nearest order.\n",
        "-- This prevent double counting of orders when a members receives multiple sends within the measurement window.\n",
        ", aud_order_agg as (\n",
        "SELECT upm_id AS upm_id_ab_buyer\n",
        "  ,SUM(demand) AS demand\n",
        "  ,SUM(base_FW_demand) AS base_FW_demand\n",
        "  ,SUM(base_AP_demand) AS base_AP_demand\n",
        "  ,SUM(base_EQ_demand) AS base_EQ_demand\n",
        "  ,SUM(clr_FW_demand) AS clr_FW_demand\n",
        "  ,SUM(clr_AP_demand) AS clr_AP_demand\n",
        "  ,SUM(clr_EQ_demand) AS clr_EQ_demand\n",
        "  ,SUM(launch_FW_demand) AS launch_FW_demand\n",
        "  ,SUM(launch_AP_demand) AS launch_AP_demand\n",
        "  ,SUM(launch_EQ_demand) AS launch_EQ_demand\n",
        "  ,SUM(rest_demand) AS rest_demand\n",
        "  ,SUM(orders) AS orders\n",
        "  ,SUM(units) AS units\n",
        "FROM aud_order_join a\n",
        "WHERE nearest_order = 1\n",
        "AND demand IS NOT NULL -- additional filter for extraneous records\n",
        "GROUP BY 1\n",
        "ORDER BY 1\n",
        ")\n",
        "\n",
        ", historical_purchase AS (\n",
        "  SELECT aoa.upm_id_ab_buyer as upm_id\n",
        "  , COUNT(DISTINCT order_nbr) as histOrders\n",
        "  FROM aud_order_agg aoa  -- eligible audience\n",
        "  INNER JOIN view_dol dol on aoa.upm_id_ab_buyer = dol.upm_id\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,730) AND DATE_SUB(first_send_dt,1)\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        "/*\n",
        "--set the correct usage platform variable at the top of the notebook\n",
        ", platform_usage as (\n",
        "  SELECT DISTINCT tct.upm_id\n",
        "  , CASE WHEN CAST(first_${hivevar:usage_platform}_activity_dt AS DATE) BETWEEN first_send_dt AND DATE_ADD(last_send_dt,7) THEN 1\n",
        "         ELSE 0\n",
        "    END AS platform_user\n",
        "  FROM aud_select_workspace.gck_first_and_last_touchpoint flt\n",
        "  INNER JOIN tc_first_last_send fls ON flt.upm_id = fls.upm_id --need first/last send values as activity window\n",
        ")\n",
        "*/\n",
        "---------------------\n",
        "-- MAIN QUERY\n",
        "----------------------\n",
        "SELECT z.test_control\n",
        "  -- primary metrics ON incremental revenue: conversion rate, average demand per purchaser\n",
        "  ,COUNT(DISTINCT z.upm_id) AS total_members\n",
        "  ,COUNT(DISTINCT abp.upm_id_ab_buyer) AS buying_members\n",
        "  ,COUNT(DISTINCT abp.upm_id_ab_buyer) / COUNT(DISTINCT z.upm_id) AS conversion_rate\n",
        "  ,AVG(demand) AS avg_demand\n",
        "  ,stddev(demand) AS std_demand\n",
        "  -- secondary metrics ON order double click\n",
        "  ,SUM(demand) / SUM(orders) AS AOV\n",
        "  ,SUM(demand) / SUM(units) AS AUR\n",
        "  ,SUM(units) / SUM(orders) AS UPT\n",
        "\n",
        "\n",
        "  -- secondary metrics ON site/app engagement\n",
        "  ,AVG(visits) AS avg_site_app_visits\n",
        "  ,AVG(PDP_FAVORITE_COUNT) AS avg_PDP_FAVORITE_COUNT\n",
        "  ,AVG(ADD_TO_CART_COUNT) AS avg_ADD_TO_CART_COUNT\n",
        "  ,AVG(physical_activity) AS avg_physical_activity\n",
        "\n",
        "  -- secondary metrics ON margin\n",
        "  ,(AVG(base_FW_demand) * 0.38 + AVG(base_AP_demand) * 0.28 + AVG(base_EQ_demand) * 0.32\n",
        "    + AVG(clr_FW_demand) * 0.23 + AVG(base_AP_demand) * 0.17 + AVG(clr_EQ_demand) * 0.12\n",
        "    + AVG(launch_FW_demand) * 0.25 + AVG(launch_AP_demand) * 0.08 + AVG(launch_EQ_demand) * 0.19\n",
        "    + AVG(rest_demand) * 0.3\n",
        "  ) AS avg_margin\n",
        "  ,AVG(base_FW_demand) AS base_FW_demand\n",
        "  ,AVG(base_AP_demand) AS base_AP_demand\n",
        "  ,AVG(base_EQ_demand) AS base_EQ_demand\n",
        "  ,AVG(clr_FW_demand) AS clr_FW_demand\n",
        "  ,AVG(clr_AP_demand) AS clr_AP_demand\n",
        "  ,AVG(clr_EQ_demand) AS clr_EQ_demand\n",
        "  ,AVG(launch_FW_demand) AS launch_FW_demand\n",
        "  ,AVG(launch_AP_demand) AS launch_AP_demand\n",
        "  ,AVG(launch_EQ_demand) AS launch_EQ_demand\n",
        "  ,AVG(rest_demand) AS rest_demand\n",
        "    -- secondary metrics ON order history\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END) AS 1_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END)/COUNT(DISTINCT z.upm_id) AS pctBuyers_1_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN demand ELSE 0 END)/SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END) AS DPB_1_prior\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END) AS 2_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END)/COUNT(DISTINCT z.upm_id) AS pctBuyers_2_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN demand ELSE 0 END)/SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END) AS DPB_2_prior\n",
        "\n",
        "\n",
        "FROM test_control_table z\n",
        "INNER JOIN balanced_aud_aa b ON z.upm_id = b.upm_id  --inner join our balanced audience to remove members that were distorting the AA period balance\n",
        "\n",
        "INNER JOIN mhub mh ON z.upm_id = mh.upm_id\n",
        "LEFT JOIN member_activities a ON mh.member_id = a.member_id\n",
        "\n",
        "LEFT JOIN aud_order_agg abp ON z.upm_id = abp.upm_id_ab_buyer\n",
        "LEFT JOIN historical_purchase hp ON z.upm_id = hp.upm_id\n",
        "\n",
        "--LEFT JOIN platform_usage plt ON z.upm_id = plt.upm_id\n",
        "\n",
        "WHERE (demand < 600 OR demand IS NULL)\n",
        "AND test_control != 'na'\n",
        "\n",
        "GROUP BY 1\n",
        "ORDER BY 1 DESC;"
      ],
      "metadata": {
        "id": "wh9JfZPuMkHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Incorrect Results\n"
      ],
      "metadata": {
        "id": "-YVUdQDRCerC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WITH\n",
        "\n",
        "    -- use audience file to determine send dates when holdout audience is used\n",
        "    tc_app_sends AS (\n",
        "      SELECT upm_id\n",
        "      , mhub.nuid\n",
        "      , CASE WHEN final_exposure_or_holdout like ${hivevar:node_tst_str} THEN 'test'\n",
        "            WHEN final_exposure_or_holdout like ${hivevar:node_ctl_str} THEN 'control'\n",
        "        ELSE 'na' END AS test_control\n",
        "      , LEFT(timestamp,10) AS campaign_send_dt\n",
        "      , CAST(LEFT(timestamp,19) AS TIMESTAMP) AS campaign_send_ts\n",
        "      FROM ${hivevar:aud_table} aud\n",
        "      INNER JOIN member.member_hub mhub ON aud.upm_id = mhub.source_id\n",
        "      ANTI JOIN contam_base c ON aud.upm_id = c.upm_id\n",
        "      ANTI JOIN gwan13.bot_master_nike_com bot ON aud.upm_id = bot.upm_id\n",
        "      ANTI JOIN aud_select_workspace.reseller_new slr ON aud.upm_id = slr.upm_user_id\n",
        "      WHERE CAST(LEFT(timestamp,10) AS DATE) BETWEEN ${hivevar:msmt_start_dt} AND ${hivevar:msmt_end_dt}\n",
        "      GROUP BY 1,2,3,4,5\n",
        "  )\n",
        "\n",
        "    , tc_first_last_send AS (\n",
        "      SELECT upm_id\n",
        "      , nuid\n",
        "      , MIN(campaign_send_dt) as first_send_dt\n",
        "      , MAX(campaign_send_dt) as last_send_dt\n",
        "      FROM tc_app_sends\n",
        "      GROUP BY 1,2\n",
        "    )\n",
        "\n",
        "    , test_control_table AS (\n",
        "        SELECT aud.*\n",
        "        , fls.first_send_dt\n",
        "        , fls.last_send_dt\n",
        "        , LEAD(campaign_send_dt) OVER (PARTITION BY aud.upm_id ORDER BY campaign_send_dt) AS next_send_dt\n",
        "        , LAG(campaign_send_dt) OVER (PARTITION BY aud.upm_id ORDER BY campaign_send_dt) AS previous_send_dt\n",
        "        , (int(to_timestamp(campaign_send_dt)) - int(to_timestamp(LEAD(campaign_send_dt) OVER (PARTITION BY aud.upm_id ORDER BY campaign_send_dt))))/86400 AS difference_next_send_days\n",
        "        , (int(to_timestamp(campaign_send_dt)) - int(to_timestamp(LAG(campaign_send_dt) OVER (PARTITION BY aud.upm_id ORDER BY campaign_send_dt))))/86400 AS difference_previous_send_days\n",
        "        FROM tc_app_sends aud\n",
        "        INNER JOIN tc_first_last_send fls ON aud.upm_id = fls.upm_id\n",
        "    )\n",
        "\n",
        "    , view_dol AS (\n",
        "      SELECT dol.upm_id\n",
        "      , order_nbr\n",
        "      , order_dt\n",
        "      , origl_ordered_qty\n",
        "      , grd_amt_excl_tax_usd\n",
        "      , line_of_business_desc\n",
        "      , univ_div_desc\n",
        "      , first_send_dt\n",
        "      , last_send_dt\n",
        "      FROM awight.na_digital_order_line_snapshot dol\n",
        "      INNER JOIN tc_first_last_send fls ON dol.upm_id = fls.upm_id\n",
        "      WHERE rec_excl_ind = 0\n",
        "        AND ttl_demand_ind = 1\n",
        "        AND univ_cat_desc <> 'Converse'\n",
        "        AND dol.upm_id IS NOT NULL\n",
        "        AND order_dt BETWEEN DATE_SUB(first_send_dt,730) AND DATE_ADD(last_send_dt,7)\n",
        "        AND (rtn_qty = 0 or rtn_qty IS NULL)\n",
        "    )\n",
        "\n",
        "    -- AA period purchase. dynamically based on when the member receives their first communication. Only returns demand for members in the\n",
        "    , aa_purchase AS (\n",
        "      SELECT dol.upm_id\n",
        "        ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "        ,COUNT(DISTINCT order_nbr) AS orders\n",
        "        ,SUM(origl_ordered_qty) AS units\n",
        "      FROM view_dol dol\n",
        "      WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,31) AND DATE_SUB(first_send_dt,1)\n",
        "      GROUP BY 1\n",
        "    )\n",
        "\n",
        "    , demand_bucket AS (\n",
        "      SELECT tct.upm_id\n",
        "      , tct.test_control\n",
        "      , aa.upm_id as upm_id_aa_buyer\n",
        "      , aa.demand\n",
        "      , aa.orders\n",
        "      , aa.units\n",
        "      , CASE\n",
        "        WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "        WHEN aa.demand < 50 THEN '000-050'\n",
        "        WHEN aa.demand < 100 THEN '050-100'\n",
        "        WHEN aa.demand < 200 THEN '100-200'\n",
        "        WHEN aa.demand < 500 THEN '200-500'\n",
        "        WHEN aa.demand < 1000 THEN '500-1000'\n",
        "        WHEN aa.demand >=1000 THEN '1000+'\n",
        "        END AS demand_per_buyer_bucket\n",
        "      , PERCENT_RANK() OVER (\n",
        "        PARTITION BY test_control\n",
        "        , CASE\n",
        "          WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "          WHEN aa.demand < 50 THEN '000-050'\n",
        "          WHEN aa.demand < 100 THEN '050-100'\n",
        "          WHEN aa.demand < 200 THEN '100-200'\n",
        "          WHEN aa.demand < 500 THEN '200-500'\n",
        "          WHEN aa.demand < 1000 THEN '500-1000'\n",
        "          WHEN aa.demand >=1000 THEN '1000+'\n",
        "          END ORDER BY RANDOM(0)\n",
        "        ) AS percent_rank\n",
        "      FROM test_control_table tct\n",
        "      LEFT JOIN aa_purchase aa ON tct.upm_id = aa.upm_id\n",
        "    )\n",
        "\n",
        "    , balanced_aud_aa as (\n",
        "      SELECT DISTINCT upm_id\n",
        "      FROM demand_bucket db\n",
        "      -- remove outliers\n",
        "      WHERE (demand < 2400 OR demand IS NULL)\n",
        "    -- To rebalance, paste text block from template in place of defaults\n",
        "    AND ((test_control = 'control' and (demand > 0 and demand <= 50) and PERCENT_RANK <= 0.97632 )\n",
        "    OR (test_control = 'control' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 1 )\n",
        "    OR (test_control = 'control' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 0.99509 )\n",
        "    OR (test_control = 'control' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 1 )\n",
        "    OR (test_control = 'control' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 0.90741 )\n",
        "    OR (test_control = 'control' and (demand > 1000) and PERCENT_RANK <= 0.98161 )\n",
        "    OR (test_control = 'control' and (demand = 0 or demand is null) and PERCENT_RANK <= 0.99998 )\n",
        "    OR (test_control = 'test' and (demand >0 and demand <= 50) and PERCENT_RANK <= 1)\n",
        "    OR (test_control = 'test' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 0.98934 )\n",
        "    OR (test_control = 'test' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 1)\n",
        "    OR (test_control = 'test' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 0.98953 )\n",
        "    OR (test_control = 'test' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 1)\n",
        "    OR (test_control = 'test' and (demand > 1000) and PERCENT_RANK <= 1)\n",
        "    OR (test_control = 'test' and (demand = 0 or demand is null) and PERCENT_RANK <= 1)\n",
        "    ))\n",
        "\n",
        "    , mhub AS (\n",
        "      SELECT source_id as upm_id\n",
        "      , nuid\n",
        "      , member_id\n",
        "      , preferred_gender\n",
        "      FROM MEMBER.MEMBER_HUB\n",
        "      WHERE ctry_2_cd = 'US'\n",
        "      AND lower(dpa_status) IN ('activate','reactivate')\n",
        "    )\n",
        "\n",
        "    -- calculate a dynamic member activity window for each communication received by a member\n",
        "    -- activity is calculated on a 7 day window, unless the member receives a communication within that timeframe\n",
        "    -- When multiple messages are received, activity is only calculated up to the next message_send_dt to avoid double counting\n",
        "    , member_activities as (\n",
        "      SELECT mgd.member_id\n",
        "      ,SUM(logged_in_visits_count) AS visits\n",
        "      ,SUM(pdp_favorite_count) AS pdp_favorite_count\n",
        "      ,SUM(add_to_cart_count) AS add_to_cart_count\n",
        "      ,SUM(physical_activity_count) AS physical_activity\n",
        "      ,MAX(CASE WHEN new_user_experience_flag = 'Y' THEN 1 ELSE 0 END) AS new_user_flag\n",
        "      ,SUM(eod_chat_count) AS NEOD_Session\n",
        "      FROM member_agd mgd --aud_select_workspace.member_agg_member_growth_daily mgd\n",
        "      INNER JOIN mhub mh ON mgd.member_id = mh.member_id\n",
        "      INNER JOIN test_control_table tct ON mh.upm_id = tct.upm_id\n",
        "      WHERE mgd.activity_dt >= tct.campaign_send_dt\n",
        "        AND mgd.activity_dt < DATE_ADD(campaign_send_dt, INT(LEAST(7,COALESCE(difference_next_send_days,7))))\n",
        "        AND mgd.preferred_retail_geo = 'NA'\n",
        "        AND mgd.experience_name != 'agnostic'\n",
        "      GROUP BY 1\n",
        "    )\n",
        "\n",
        "    -- for each user, aggregate each order placed within our measurement window. create labels to categorize the type of demand\n",
        "    , ab_purchase as (\n",
        "      SELECT dol.upm_id as upm_id_ab_buyer\n",
        "      , order_dt as order_ts\n",
        "      , order_nbr\n",
        "      ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "      ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_FW_demand\n",
        "      ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_AP_demand\n",
        "      ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_EQ_demand\n",
        "      ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_FW_demand\n",
        "      ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_AP_demand\n",
        "      ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_EQ_demand\n",
        "      ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_FW_demand\n",
        "      ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_AP_demand\n",
        "      ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_EQ_demand\n",
        "      ,SUM(CASE WHEN line_of_business_desc NOT IN ('Base Inline', 'Clearance', 'Launch') THEN grd_amt_excl_tax_usd ELSE 0 END) AS rest_demand\n",
        "      ,COUNT(DISTINCT order_nbr) AS orders\n",
        "      ,SUM(origl_ordered_qty) AS units\n",
        "      FROM view_dol dol\n",
        "      WHERE CAST(order_dt AS DATE) BETWEEN last_send_dt AND DATE_ADD(last_send_dt,7)\n",
        "      GROUP BY 1,2,3\n",
        "    )\n",
        "\n",
        "    -- counting the first order demand after each send\n",
        "    , reset_per_campaign_sent AS (\n",
        "    SELECT a.*\n",
        "      ,CONCAT(a.upm_id, ' ', a.campaign_send_dt ) AS touchpoint_per_member\n",
        "      FROM test_control_table a -- should it be created based on buyers/\n",
        "      WHERE upm_id IN (SELECT DISTINCT upm_id_ab_buyer FROM ab_purchase )\n",
        "      )\n",
        "\n",
        "\n",
        "    , aud_order_join AS    (\n",
        "      SELECT a.*\n",
        "      , b.*\n",
        "      ,round(((bigint(to_timestamp(a.campaign_send_ts)))-(bigint(to_timestamp(b.order_ts))))/(60),3) AS order_send_diff_min\n",
        "      ,RANK() OVER(PARTITION BY  a.touchpoint_per_member  ORDER BY (((bigint(to_timestamp(a.campaign_send_ts)))-(bigint(to_timestamp(b.order_ts))))/(60)) DESC, a.campaign_send_dt ) AS nearest_order\n",
        "      FROM reset_per_campaign_sent a\n",
        "      LEFT JOIN ab_purchase b\n",
        "      ON (a.upm_id = b.upm_id_ab_buyer AND b.order_ts >= a.campaign_send_ts AND b.order_ts <= DATE_ADD(CAST(a.`campaign_send_dt` AS DATE), CAST(LEAST(7, COALESCE(a.`difference_next_send_days`, 7)) AS INT)))\n",
        "      ORDER BY a.upm_id, a.campaign_send_dt ASC\n",
        "    )\n",
        "\n",
        "\n",
        "    -- Aggregate each distinct send, attributing only the nearest order.\n",
        "    -- This prevents double counting of orders when a members receives multiple sends within the measurement window.\n",
        "    , aud_order_agg as (\n",
        "    SELECT upm_id AS upm_id_ab_buyer\n",
        "      ,SUM(demand) AS demand\n",
        "      ,SUM(base_FW_demand) AS base_FW_demand\n",
        "      ,SUM(base_AP_demand) AS base_AP_demand\n",
        "      ,SUM(base_EQ_demand) AS base_EQ_demand\n",
        "      ,SUM(clr_FW_demand) AS clr_FW_demand\n",
        "      ,SUM(clr_AP_demand) AS clr_AP_demand\n",
        "      ,SUM(clr_EQ_demand) AS clr_EQ_demand\n",
        "      ,SUM(launch_FW_demand) AS launch_FW_demand\n",
        "      ,SUM(launch_AP_demand) AS launch_AP_demand\n",
        "      ,SUM(launch_EQ_demand) AS launch_EQ_demand\n",
        "      ,SUM(rest_demand) AS rest_demand\n",
        "      ,SUM(orders) AS orders\n",
        "      ,SUM(units) AS units\n",
        "    FROM aud_order_join a\n",
        "    WHERE nearest_order = 1\n",
        "    AND demand IS NOT NULL -- additional filter for extraneous records\n",
        "    GROUP BY 1\n",
        "    ORDER BY 1\n",
        "    )\n",
        "\n",
        "    , historical_purchase AS (\n",
        "      SELECT aoa.upm_id_ab_buyer as upm_id\n",
        "      , COUNT(DISTINCT order_nbr) as histOrders\n",
        "      FROM aud_order_agg aoa  -- eligible audience\n",
        "      INNER JOIN view_dol dol on aoa.upm_id_ab_buyer = dol.upm_id\n",
        "      WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,730) AND DATE_SUB(first_send_dt,1)\n",
        "      GROUP BY 1\n",
        "    )\n",
        "\n",
        "    -- returns all the instances of the campaign codes for each member along with the action types and revenue if applicable\n",
        "    , email_engage as (\n",
        "      SELECT DISTINCT fer.upm_id\n",
        "      , LEFT(event_dttm,10) AS engage_dt\n",
        "      , action_type\n",
        "      , revenue_dollar\n",
        "      , sent_ind\n",
        "      , open_ind\n",
        "      , click_ind\n",
        "      , bounce_ind\n",
        "      , optout_ind\n",
        "      , purchase_ind\n",
        "      FROM awight.na_email_userengagement_snapshot_1 fer\n",
        "      INNER JOIN test_control_table tct ON fer.upm_id = tct.upm_id\n",
        "      WHERE record_dt BETWEEN ${hivevar:msmt_start_dt} AND DATE_ADD(${hivevar:msmt_end_dt},7)\n",
        "      AND action_type IN ('open','click','send','bounce','optout')\n",
        "      AND comm_id IN (${hivevar:email_commID_str})\n",
        "      AND CAST(LEFT(event_dttm,10) AS DATE) BETWEEN last_send_dt AND DATE_ADD(last_send_dt,7)\n",
        "      AND fer.upm_id IS NOT NULL\n",
        "      GROUP BY 1,2,3,4,5,6,7,8,9,10\n",
        "      )\n",
        "\n",
        "\n",
        "    -- calculates the the total number of opens, clicks, purchases and last touch attributed demand for each member for the campaign\n",
        "    -- responses are only considered if they occur between when the message was sent and 7 days later.\n",
        "    , email_calc AS (\n",
        "        SELECT fer.upm_id\n",
        "        , MAX(sent_ind) AS email_sends\n",
        "        , MAX(open_ind) AS email_opens\n",
        "        , MAX(click_ind) AS email_clicks\n",
        "        , MAX(bounce_ind) AS email_bounces\n",
        "        , MAX(optout_ind) AS email_optouts\n",
        "        , MAX(purchase_ind) AS email_purch\n",
        "        , SUM(CASE WHEN action_type = 'ecommerce-purchase' THEN revenue_dollar ELSE 0 END) AS email_demand\n",
        "        FROM email_engage fer\n",
        "        INNER JOIN test_control_table tct ON fer.upm_id = tct.upm_id\n",
        "        GROUP BY 1\n",
        "    )\n",
        "\n",
        "    , email_engage_agg as (\n",
        "      SELECT upm_id\n",
        "      , SUM(email_sends) AS email_sends\n",
        "      , SUM(email_opens) AS email_opens\n",
        "      , SUM(email_clicks) AS email_clicks\n",
        "      , SUM(email_bounces) AS email_bounces\n",
        "      , SUM(email_optouts) AS email_optouts\n",
        "      , SUM(email_purch) AS email_purch\n",
        "      , SUM(email_demand) AS email_demand\n",
        "      FROM email_calc\n",
        "      GROUP BY 1\n",
        "    )\n",
        "\n",
        "    , push_engagement AS (\n",
        "        SELECT -- fpr.upm_id\n",
        "          fpr.nuid\n",
        "        , tc.last_send_dt\n",
        "        , COUNT(DISTINCT CASE WHEN event_dttm IS NOT NULL AND action_type = 'send' THEN cp_cd END) AS pushes\n",
        "        , COUNT(DISTINCT CASE WHEN event_dttm IS NOT NULL AND action_type = 'open' OR action_type = 'inferred_open' THEN cp_cd\n",
        "                              -- WHEN evemt_dttm IS NOT NULL AND action_type = 'inferred_open' THEN cp_cd\n",
        "                          END) AS open_pushes\n",
        "        FROM awight.na_push_userengagement_snapshot_1 AS fpr\n",
        "        INNER JOIN test_control_table tc ON tc.nuid = fpr.nuid\n",
        "        -- INNER JOIN test_control_table tc ON tc.upm_id = fpr.upm_id\n",
        "        WHERE CAST(event_dttm AS DATE) BETWEEN last_send_dt AND DATE_ADD(last_send_dt,7)\n",
        "        AND comm_id IN (${hivevar:push_commID_str1}, ${hivevar:push_commID_str2})\n",
        "        AND action_type IN ('send','open','inferred_open')\n",
        "        -- AND fpr.upm_id IS NOT NULL\n",
        "        AND retail_geo = 'NA' -- optional\n",
        "        GROUP BY 1,2\n",
        "    )\n",
        "\n",
        "/*\n",
        "\n",
        ", app_metrics_feed AS (\n",
        "    SELECT upm_id\n",
        "  , SUM(tot_editorial_hub_card_shown_count\n",
        "                + tot_stream_card_shown_count\n",
        "                + tot_editorial_card_shown_count\n",
        "                + tot_mhome_product_marketing_card_shown_count\n",
        "                + tot_mhome_recommended_product_card_shown_count\n",
        "        ) as total_card_views\n",
        "  , \tSUM(tot_editorial_hub_card_clicked_count\n",
        "                + tot_stream_card_clicked_count\n",
        "                + tot_editorial_card_clicked_count\n",
        "                + tot_mhome_product_marketing_card_clicked_count\n",
        "                + tot_mhome_recommended_product_card_clicked_count\n",
        "        ) as total_card_taps\n",
        "    FROM content.agg_user_session_content_activity_daily\n",
        "    WHERE thread_id IN (${hivevar:feedID_str})\n",
        "      AND CAST(session_start_date AS DATE) BETWEEN ${hivevar:msmt_start_dt} AND DATE_ADD(${hivevar:msmt_end_dt},7)\n",
        "      AND app_version IS NOT NULL\n",
        "      AND card_key != 'UNKNOWN'\n",
        "    GROUP BY 1\n",
        "  )\n",
        "\n",
        "\n",
        ", app_metrics_thread AS (\n",
        "    SELECT upm_id\n",
        "    ,SUM(tot_thread_clicks) as thread_taps\n",
        "    ,SUM(tot_thread_viewed_count) as thread_views\n",
        "    ,SUM(tot_thread_comment_added_count) as thread_comment_add\n",
        "    FROM content.agg_user_session_content_activity_daily\n",
        "    WHERE thread_id IN (${hivevar:threadID_str})\n",
        "      AND CAST(session_start_date AS DATE) BETWEEN ${hivevar:msmt_start_dt} AND DATE_ADD(${hivevar:msmt_end_dt},7)\n",
        "      AND app_version IS NOT NULL\n",
        "      AND card_key != 'UNKNOWN'\n",
        "    GROUP BY 1\n",
        ")\n",
        "*/\n",
        "\n",
        "    ---------------------\n",
        "    -- MAIN QUERY\n",
        "    ----------------------\n",
        "    SELECT  z.upm_id\n",
        "          , z.test_control\n",
        "\n",
        "          -- primary metrics ON incremental revenue: conversion rate, average demand per purchaser\n",
        "          , SUM(demand) AS demand\n",
        "\n",
        "          -- secondary metrics ON order double click\n",
        "          , SUM(orders) AS orders\n",
        "          , SUM(units) AS units\n",
        "\n",
        "          -- secondary metrics ON emails\n",
        "          , SUM(email_sends) AS email_sent\n",
        "          , SUM(email_opens) AS email_opens\n",
        "          , SUM(email_clicks) AS email_clicks\n",
        "          , SUM(email_purch) AS email_purch\n",
        "          , SUM(email_demand) AS email_demand\n",
        "\n",
        "          -- secondary metrics ON app push/engagement\n",
        "          , (CASE WHEN SUM(pushes) = 0 THEN 0 ELSE SUM(pushes) END) AS push_sent\n",
        "          , (CASE WHEN SUM(pushes) = 0 THEN 0 ELSE SUM(open_pushes) END) AS push_opens\n",
        "          , (CASE WHEN SUM(pushes) = 0 THEN 0 ELSE SUM(open_pushes) / SUM(pushes) END) AS push_open_rate\n",
        "          /*\n",
        "          , SUM(total_card_views) as card_views\n",
        "          , SUM(total_card_taps) as card_taps\n",
        "          , SUM(thread_views) as thread_views\n",
        "          , SUM(thread_taps) as thread_taps\n",
        "          , SUM(thread_comment_add) as thread_comment_add\n",
        "          */\n",
        "\n",
        "          -- secondary metrics ON site/app engagement\n",
        "          , SUM(visits) AS site_app_visits\n",
        "          , SUM(PDP_FAVORITE_COUNT) AS PDP_FAVORITE_COUNT\n",
        "          , SUM(ADD_TO_CART_COUNT) AS ADD_TO_CART_COUNT\n",
        "          , SUM(physical_activity) AS physical_activity\n",
        "\n",
        "          -- secondary metrics ON margin\n",
        "          , SUM(base_FW_demand) AS base_FW_demand\n",
        "          , SUM(base_AP_demand) AS base_AP_demand\n",
        "          , SUM(base_EQ_demand) AS base_EQ_demand\n",
        "          , SUM(clr_FW_demand) AS clr_FW_demand\n",
        "          , SUM(clr_AP_demand) AS clr_AP_demand\n",
        "          , SUM(clr_EQ_demand) AS clr_EQ_demand\n",
        "          , SUM(launch_FW_demand) AS launch_FW_demand\n",
        "          , SUM(launch_AP_demand) AS launch_AP_demand\n",
        "          , SUM(launch_EQ_demand) AS launch_EQ_demand\n",
        "          , SUM(rest_demand) AS rest_demand\n",
        "\n",
        "          -- secondary metrics ON order history\n",
        "          , SUM(histOrders) AS histOrders\n",
        "          ,SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END) AS 1_priorOrder\n",
        "          ,SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END) AS 2_priorOrder\n",
        "\n",
        "          -- optional first usage kpi\n",
        "          --  ,SUM(platform_user) AS first_usage\n",
        "\n",
        "    FROM test_control_table z\n",
        "    INNER JOIN balanced_aud_aa b ON z.upm_id = b.upm_id  --inner join our balanced audience to remove members that were distorting the AA period balance\n",
        "\n",
        "    INNER JOIN mhub mh ON z.upm_id = mh.upm_id\n",
        "    LEFT JOIN member_activities a ON mh.member_id = a.member_id\n",
        "\n",
        "    LEFT JOIN aud_order_agg abp ON z.upm_id = abp.upm_id_ab_buyer\n",
        "    LEFT JOIN historical_purchase hp ON z.upm_id = hp.upm_id\n",
        "\n",
        "    LEFT JOIN email_engage_agg cra ON z.upm_id = cra.upm_id\n",
        "/*\n",
        "    LEFT JOIN email_delivery_agg cda ON z.upm_id = cda.upm_id\n",
        "    LEFT JOIN email_response_agg cra ON z.upm_id = cra.upm_id\n",
        "*/\n",
        "\n",
        "    LEFT JOIN push_engagement pe ON mh.nuid = pe.nuid\n",
        "/*\n",
        "    LEFT JOIN app_metrics_feed amf ON z.upm_id = amf.upm_id\n",
        "    LEFT JOIN app_metrics_thread amt ON z.upm_id = amt.upm_id\n",
        "*/\n",
        "    WHERE (demand < 600 OR demand IS NULL)\n",
        "\n",
        "    GROUP BY 1,2\n",
        "    ORDER BY 1 DESC"
      ],
      "metadata": {
        "id": "BR48mJ1jCgMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Updated Code"
      ],
      "metadata": {
        "id": "18JNw94pNaI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "co\n",
        "  WITH\n",
        "\n",
        "    -- use audience file to determine send dates when holdout audience is used\n",
        "    tc_app_sends AS (\n",
        "      SELECT upm_id\n",
        "      , mhub.nuid\n",
        "      , CASE WHEN final_exposure_or_holdout like ${hivevar:node_tst_str} THEN 'test'\n",
        "            WHEN final_exposure_or_holdout like ${hivevar:node_ctl_str} THEN 'control'\n",
        "        ELSE 'na' END AS test_control\n",
        "      , LEFT(timestamp,10) AS campaign_send_dt\n",
        "      , CAST(LEFT(timestamp,19) AS TIMESTAMP) AS campaign_send_ts\n",
        "      FROM ${hivevar:aud_table} aud\n",
        "      INNER JOIN member.member_hub mhub ON aud.upm_id = mhub.source_id\n",
        "      ANTI JOIN contam_base c ON aud.upm_id = c.upm_id\n",
        "      ANTI JOIN gwan13.bot_master_nike_com bot ON aud.upm_id = bot.upm_id\n",
        "      ANTI JOIN aud_select_workspace.reseller_new slr ON aud.upm_id = slr.upm_user_id\n",
        "      WHERE CAST(LEFT(timestamp,10) AS DATE) BETWEEN ${hivevar:msmt_start_dt} AND ${hivevar:msmt_end_dt}\n",
        "      GROUP BY 1,2,3,4,5\n",
        "  )\n",
        "\n",
        "    , tc_first_last_send AS (\n",
        "      SELECT upm_id\n",
        "      , nuid\n",
        "      , MIN(campaign_send_dt) as first_send_dt\n",
        "      , MAX(campaign_send_dt) as last_send_dt\n",
        "      FROM tc_app_sends\n",
        "      GROUP BY 1,2\n",
        "    )\n",
        "\n",
        "    , test_control_table AS (\n",
        "        SELECT aud.*\n",
        "        , fls.first_send_dt\n",
        "        , fls.last_send_dt\n",
        "        , LEAD(campaign_send_dt) OVER (PARTITION BY aud.upm_id ORDER BY campaign_send_dt) AS next_send_dt\n",
        "        , LAG(campaign_send_dt) OVER (PARTITION BY aud.upm_id ORDER BY campaign_send_dt) AS previous_send_dt\n",
        "        , (int(to_timestamp(campaign_send_dt)) - int(to_timestamp(LEAD(campaign_send_dt) OVER (PARTITION BY aud.upm_id ORDER BY campaign_send_dt))))/86400 AS difference_next_send_days\n",
        "        , (int(to_timestamp(campaign_send_dt)) - int(to_timestamp(LAG(campaign_send_dt) OVER (PARTITION BY aud.upm_id ORDER BY campaign_send_dt))))/86400 AS difference_previous_send_days\n",
        "        FROM tc_app_sends aud\n",
        "        INNER JOIN tc_first_last_send fls ON aud.upm_id = fls.upm_id\n",
        "    )\n",
        "\n",
        "    , view_dol AS (\n",
        "      SELECT dol.upm_id\n",
        "      , order_nbr\n",
        "      , order_dt\n",
        "      , origl_ordered_qty\n",
        "      , grd_amt_excl_tax_usd\n",
        "      , line_of_business_desc\n",
        "      , univ_div_desc\n",
        "      , first_send_dt\n",
        "      , last_send_dt\n",
        "      FROM awight.na_digital_order_line_snapshot dol\n",
        "      INNER JOIN tc_first_last_send fls ON dol.upm_id = fls.upm_id\n",
        "      WHERE rec_excl_ind = 0\n",
        "        AND ttl_demand_ind = 1\n",
        "        AND univ_cat_desc <> 'Converse'\n",
        "        AND dol.upm_id IS NOT NULL\n",
        "        AND order_dt BETWEEN DATE_SUB(first_send_dt,730) AND DATE_ADD(last_send_dt,7)\n",
        "        AND (rtn_qty = 0 or rtn_qty IS NULL)\n",
        "    )\n",
        "\n",
        "    -- AA period purchase. dynamically based on when the member receives their first communication. Only returns demand for members in the\n",
        "    , aa_purchase AS (\n",
        "      SELECT dol.upm_id\n",
        "        ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "        ,COUNT(DISTINCT order_nbr) AS orders\n",
        "        ,SUM(origl_ordered_qty) AS units\n",
        "      FROM view_dol dol\n",
        "      WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,31) AND DATE_SUB(first_send_dt,1)\n",
        "      GROUP BY 1\n",
        "    )\n",
        "\n",
        "    , demand_bucket AS (\n",
        "      SELECT tct.upm_id\n",
        "      , tct.test_control\n",
        "      , aa.upm_id as upm_id_aa_buyer\n",
        "      , aa.demand\n",
        "      , aa.orders\n",
        "      , aa.units\n",
        "      , CASE\n",
        "        WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "        WHEN aa.demand < 50 THEN '000-050'\n",
        "        WHEN aa.demand < 100 THEN '050-100'\n",
        "        WHEN aa.demand < 200 THEN '100-200'\n",
        "        WHEN aa.demand < 500 THEN '200-500'\n",
        "        WHEN aa.demand < 1000 THEN '500-1000'\n",
        "        WHEN aa.demand >=1000 THEN '1000+'\n",
        "        END AS demand_per_buyer_bucket\n",
        "      , PERCENT_RANK() OVER (\n",
        "        PARTITION BY test_control\n",
        "        , CASE\n",
        "          WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "          WHEN aa.demand < 50 THEN '000-050'\n",
        "          WHEN aa.demand < 100 THEN '050-100'\n",
        "          WHEN aa.demand < 200 THEN '100-200'\n",
        "          WHEN aa.demand < 500 THEN '200-500'\n",
        "          WHEN aa.demand < 1000 THEN '500-1000'\n",
        "          WHEN aa.demand >=1000 THEN '1000+'\n",
        "          END ORDER BY RANDOM(0)\n",
        "        ) AS percent_rank\n",
        "      FROM test_control_table tct\n",
        "      LEFT JOIN aa_purchase aa ON tct.upm_id = aa.upm_id\n",
        "    )\n",
        "\n",
        "    , balanced_aud_aa as (\n",
        "      SELECT DISTINCT upm_id\n",
        "      FROM demand_bucket db\n",
        "      -- remove outliers\n",
        "      WHERE (demand < 2400 OR demand IS NULL)\n",
        "    -- To rebalance, paste text block from template in place of defaults\n",
        "    AND ((test_control = 'control' and (demand > 0 and demand <= 50) and PERCENT_RANK <= 0.97632 )\n",
        "    OR (test_control = 'control' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 1 )\n",
        "    OR (test_control = 'control' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 0.99509 )\n",
        "    OR (test_control = 'control' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 1 )\n",
        "    OR (test_control = 'control' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 0.90741 )\n",
        "    OR (test_control = 'control' and (demand > 1000) and PERCENT_RANK <= 0.98161 )\n",
        "    OR (test_control = 'control' and (demand = 0 or demand is null) and PERCENT_RANK <= 0.99998 )\n",
        "    OR (test_control = 'test' and (demand >0 and demand <= 50) and PERCENT_RANK <= 1)\n",
        "    OR (test_control = 'test' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 0.98934 )\n",
        "    OR (test_control = 'test' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 1)\n",
        "    OR (test_control = 'test' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 0.98953 )\n",
        "    OR (test_control = 'test' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 1)\n",
        "    OR (test_control = 'test' and (demand > 1000) and PERCENT_RANK <= 1)\n",
        "    OR (test_control = 'test' and (demand = 0 or demand is null) and PERCENT_RANK <= 1)\n",
        "    ))\n",
        "\n",
        "    , mhub AS (\n",
        "      SELECT source_id as upm_id\n",
        "      , nuid\n",
        "      , member_id\n",
        "      , preferred_gender\n",
        "      FROM MEMBER.MEMBER_HUB\n",
        "      WHERE ctry_2_cd = 'US'\n",
        "      AND lower(dpa_status) IN ('activate','reactivate')\n",
        "    )\n",
        "\n",
        "    -- calculate a dynamic member activity window for each communication received by a member\n",
        "    -- activity is calculated on a 7 day window, unless the member receives a communication within that timeframe\n",
        "    -- When multiple messages are received, activity is only calculated up to the next message_send_dt to avoid double counting\n",
        "    , member_activities as (\n",
        "      SELECT mgd.member_id\n",
        "      ,SUM(logged_in_visits_count) AS visits\n",
        "      ,SUM(pdp_favorite_count) AS pdp_favorite_count\n",
        "      ,SUM(add_to_cart_count) AS add_to_cart_count\n",
        "      ,SUM(physical_activity_count) AS physical_activity\n",
        "      ,MAX(CASE WHEN new_user_experience_flag = 'Y' THEN 1 ELSE 0 END) AS new_user_flag\n",
        "      ,SUM(eod_chat_count) AS NEOD_Session\n",
        "      FROM member_agd mgd --aud_select_workspace.member_agg_member_growth_daily mgd\n",
        "      INNER JOIN mhub mh ON mgd.member_id = mh.member_id\n",
        "      INNER JOIN test_control_table tct ON mh.upm_id = tct.upm_id\n",
        "      WHERE mgd.activity_dt >= tct.campaign_send_dt\n",
        "        AND mgd.activity_dt < DATE_ADD(campaign_send_dt, INT(LEAST(7,COALESCE(difference_next_send_days,7))))\n",
        "        AND mgd.preferred_retail_geo = 'NA'\n",
        "        AND mgd.experience_name != 'agnostic'\n",
        "      GROUP BY 1\n",
        "    )\n",
        "\n",
        "    -- for each user, aggregate each order placed within our measurement window. create labels to categorize the type of demand\n",
        "    , ab_purchase as (\n",
        "      SELECT dol.upm_id as upm_id_ab_buyer\n",
        "      , order_dt as order_ts\n",
        "      , order_nbr\n",
        "      ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "      ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_FW_demand\n",
        "      ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_AP_demand\n",
        "      ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_EQ_demand\n",
        "      ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_FW_demand\n",
        "      ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_AP_demand\n",
        "      ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_EQ_demand\n",
        "      ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_FW_demand\n",
        "      ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_AP_demand\n",
        "      ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_EQ_demand\n",
        "      ,SUM(CASE WHEN line_of_business_desc NOT IN ('Base Inline', 'Clearance', 'Launch') THEN grd_amt_excl_tax_usd ELSE 0 END) AS rest_demand\n",
        "      ,COUNT(DISTINCT order_nbr) AS orders\n",
        "      ,SUM(origl_ordered_qty) AS units\n",
        "      FROM view_dol dol\n",
        "      WHERE CAST(order_dt AS DATE) BETWEEN last_send_dt AND DATE_ADD(last_send_dt,7)\n",
        "      GROUP BY 1,2,3\n",
        "    )\n",
        "\n",
        "    -- counting the first order demand after each send\n",
        "    , reset_per_campaign_sent AS (\n",
        "    SELECT a.*\n",
        "      ,CONCAT(a.upm_id, ' ', a.campaign_send_dt ) AS touchpoint_per_member\n",
        "      FROM test_control_table a -- should it be created based on buyers/\n",
        "      WHERE upm_id IN (SELECT DISTINCT upm_id_ab_buyer FROM ab_purchase )\n",
        "      )\n",
        "\n",
        "    , aud_order_join AS    (\n",
        "      SELECT a.*\n",
        "      , b.*\n",
        "      ,round(((bigint(to_timestamp(a.campaign_send_ts)))-(bigint(to_timestamp(b.order_ts))))/(60),3) AS order_send_diff_min\n",
        "      ,RANK() OVER(PARTITION BY  a.touchpoint_per_member  ORDER BY (((bigint(to_timestamp(a.campaign_send_ts)))-(bigint(to_timestamp(b.order_ts))))/(60)) DESC, a.campaign_send_dt ) AS nearest_order\n",
        "      FROM reset_per_campaign_sent a\n",
        "      LEFT JOIN ab_purchase b\n",
        "      ON (a.upm_id = b.upm_id_ab_buyer AND b.order_ts >= a.campaign_send_ts AND b.order_ts <= DATE_ADD(CAST(a.`campaign_send_dt` AS DATE), CAST(LEAST(7, COALESCE(a.`difference_next_send_days`, 7)) AS INT)))\n",
        "      ORDER BY a.upm_id, a.campaign_send_dt ASC\n",
        "    )\n",
        "\n",
        "    -- Aggregate each distinct send, attributing only the nearest order.\n",
        "    -- This prevents double counting of orders when a members receives multiple sends within the measurement window.\n",
        "    , aud_order_agg as (\n",
        "    SELECT upm_id AS upm_id_ab_buyer\n",
        "      ,SUM(demand) AS demand\n",
        "      ,SUM(base_FW_demand) AS base_FW_demand\n",
        "      ,SUM(base_AP_demand) AS base_AP_demand\n",
        "      ,SUM(base_EQ_demand) AS base_EQ_demand\n",
        "      ,SUM(clr_FW_demand) AS clr_FW_demand\n",
        "      ,SUM(clr_AP_demand) AS clr_AP_demand\n",
        "      ,SUM(clr_EQ_demand) AS clr_EQ_demand\n",
        "      ,SUM(launch_FW_demand) AS launch_FW_demand\n",
        "      ,SUM(launch_AP_demand) AS launch_AP_demand\n",
        "      ,SUM(launch_EQ_demand) AS launch_EQ_demand\n",
        "      ,SUM(rest_demand) AS rest_demand\n",
        "      ,SUM(orders) AS orders\n",
        "      ,SUM(units) AS units\n",
        "    FROM aud_order_join a\n",
        "    WHERE nearest_order = 1\n",
        "    AND demand IS NOT NULL -- additional filter for extraneous records\n",
        "    GROUP BY 1\n",
        "    ORDER BY 1\n",
        "    )\n",
        "\n",
        "    , historical_purchase AS (\n",
        "      SELECT aoa.upm_id_ab_buyer as upm_id\n",
        "      , COUNT(DISTINCT order_nbr) as histOrders\n",
        "      FROM aud_order_agg aoa  -- eligible audience\n",
        "      INNER JOIN view_dol dol on aoa.upm_id_ab_buyer = dol.upm_id\n",
        "      WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,730) AND DATE_SUB(first_send_dt,1)\n",
        "      GROUP BY 1\n",
        "    )\n",
        "\n",
        "    -- returns all the instances of the campaign codes for each member along with the action types and revenue if applicable\n",
        "    , email_engage as (\n",
        "      SELECT DISTINCT fer.upm_id\n",
        "      , LEFT(event_dttm,10) AS engage_dt\n",
        "      , action_type\n",
        "      , revenue_dollar\n",
        "      , sent_ind\n",
        "      , open_ind\n",
        "      , click_ind\n",
        "      , bounce_ind\n",
        "      , optout_ind\n",
        "      , purchase_ind\n",
        "      FROM awight.na_email_userengagement_snapshot_1 fer\n",
        "      INNER JOIN test_control_table tct ON fer.upm_id = tct.upm_id\n",
        "      WHERE record_dt BETWEEN ${hivevar:msmt_start_dt} AND DATE_ADD(${hivevar:msmt_end_dt},7)\n",
        "      AND action_type IN ('open','click','send','bounce','optout')\n",
        "      AND comm_id IN (${hivevar:email_commID_str})\n",
        "      AND CAST(LEFT(event_dttm,10) AS DATE) BETWEEN last_send_dt AND DATE_ADD(last_send_dt,7)\n",
        "      AND fer.upm_id IS NOT NULL\n",
        "      GROUP BY 1,2,3,4,5,6,7,8,9,10\n",
        "      )\n",
        "\n",
        "    -- calculates the the total number of opens, clicks, purchases and last touch attributed demand for each member for the campaign\n",
        "    -- responses are only considered if they occur between when the message was sent and 7 days later.\n",
        "    , email_calc AS (\n",
        "        SELECT fer.upm_id\n",
        "        , MAX(sent_ind) AS email_sends\n",
        "        , MAX(open_ind) AS email_opens\n",
        "        , MAX(click_ind) AS email_clicks\n",
        "        , MAX(bounce_ind) AS email_bounces\n",
        "        , MAX(optout_ind) AS email_optouts\n",
        "        , MAX(purchase_ind) AS email_purch\n",
        "        , SUM(CASE WHEN action_type = 'ecommerce-purchase' THEN revenue_dollar ELSE 0 END) AS email_demand\n",
        "        FROM email_engage fer\n",
        "        INNER JOIN test_control_table tct ON fer.upm_id = tct.upm_id\n",
        "        GROUP BY 1\n",
        "    )\n",
        "\n",
        "    , email_engage_agg as (\n",
        "      SELECT upm_id\n",
        "      , SUM(email_sends) AS email_sends\n",
        "      , SUM(email_opens) AS email_opens\n",
        "      , SUM(email_clicks) AS email_clicks\n",
        "      , SUM(email_bounces) AS email_bounces\n",
        "      , SUM(email_optouts) AS email_optouts\n",
        "      , SUM(email_purch) AS email_purch\n",
        "      , SUM(email_demand) AS email_demand\n",
        "      FROM email_calc\n",
        "      GROUP BY 1\n",
        "    )\n",
        "\n",
        ", push_engagement AS (\n",
        "    SELECT fpr.upm_id\n",
        "      , fpr.nuid\n",
        "      , COUNT(DISTINCT CASE WHEN event_dttm IS NOT NULL AND action_type = 'send' THEN cp_cd END) AS pushes\n",
        "      , COUNT(DISTINCT CASE WHEN event_dttm IS NOT NULL AND action_type = 'open' OR action_type = 'inferred_open' THEN cp_cd END) AS open_pushes\n",
        "    FROM awight.na_push_userengagement_snapshot_1 AS fpr\n",
        "    INNER JOIN test_control_table tc ON tc.nuid = fpr.nuid\n",
        "    WHERE CAST(event_dttm AS DATE) BETWEEN tc.last_send_dt AND DATE_ADD(tc.last_send_dt,7)\n",
        "    AND comm_id IN (${hivevar:push_commID_str1}, ${hivevar:push_commID_str2})\n",
        "    AND action_type IN ('send','open','inferred_open')\n",
        "    AND retail_geo = 'NA' -- optional\n",
        "    GROUP BY fpr.upm_id, fpr.nuid\n",
        ")\n",
        "\n",
        "/*\n",
        "    , app_metrics_feed AS (\n",
        "        SELECT upm_id\n",
        "      , SUM(tot_editorial_hub_card_shown_count\n",
        "                    + tot_stream_card_shown_count\n",
        "                    + tot_editorial_card_shown_count\n",
        "                    + tot_mhome_product_marketing_card_shown_count\n",
        "                    + tot_mhome_recommended_product_card_shown_count\n",
        "            ) as total_card_views\n",
        "      , \tSUM(tot_editorial_hub_card_clicked_count\n",
        "                    + tot_stream_card_clicked_count\n",
        "                    + tot_editorial_card_clicked_count\n",
        "                    + tot_mhome_product_marketing_card_clicked_count\n",
        "                    + tot_mhome_recommended_product_card_clicked_count\n",
        "            ) as total_card_taps\n",
        "        FROM content.agg_user_session_content_activity_daily\n",
        "        WHERE thread_id IN (${hivevar:feedID_str})\n",
        "          AND CAST(session_start_date AS DATE) BETWEEN ${hivevar:msmt_start_dt} AND DATE_ADD(${hivevar:msmt_end_dt},7)\n",
        "          AND app_version IS NOT NULL\n",
        "          AND card_key != 'UNKNOWN'\n",
        "        GROUP BY 1\n",
        "    )\n",
        "\n",
        "    , app_metrics_thread AS (\n",
        "      SELECT upm_id\n",
        "      ,SUM(tot_thread_clicks) as thread_taps\n",
        "      ,SUM(tot_thread_viewed_count) as thread_views\n",
        "      ,SUM(tot_thread_comment_added_count) as thread_comment_add\n",
        "      FROM content.agg_user_session_content_activity_daily\n",
        "      WHERE thread_id IN (${hivevar:threadID_str})\n",
        "        AND CAST(session_start_date AS DATE) BETWEEN ${hivevar:msmt_start_dt} AND DATE_ADD(${hivevar:msmt_end_dt},7)\n",
        "        AND app_version IS NOT NULL\n",
        "        AND card_key != 'UNKNOWN'\n",
        "      GROUP BY 1\n",
        "    )\n",
        "*/\n",
        "\n",
        ", combined_engagement AS (\n",
        "  SELECT upm_id\n",
        "      , SUM(email_sends) AS email_sends\n",
        "      , SUM(email_opens) AS email_opens\n",
        "      , SUM(email_clicks) AS email_clicks\n",
        "      , SUM(email_purch) AS email_purch\n",
        "      , SUM(email_demand) AS email_demand\n",
        "      , SUM(pushes) AS push_sent\n",
        "      , SUM(open_pushes) AS push_opens\n",
        "  FROM (\n",
        "    SELECT upm_id, email_sends, email_opens, email_clicks, email_purch, email_demand, 0 AS pushes, 0 AS open_pushes\n",
        "    FROM email_engage_agg\n",
        "    UNION ALL\n",
        "    SELECT pe.upm_id, 0 AS email_sends, 0 AS email_opens, 0 AS email_clicks, 0 AS email_purch, 0 AS email_demand, pushes, open_pushes\n",
        "    FROM push_engagement pe\n",
        "  ) combined\n",
        "  GROUP BY upm_id\n",
        ")\n",
        "\n",
        "---------------------\n",
        "-- MAIN QUERY\n",
        "----------------------\n",
        "SELECT  z.upm_id\n",
        "      , z.test_control\n",
        "\n",
        "      -- primary metrics ON incremental revenue: conversion rate, average demand per purchaser\n",
        "      , SUM(abp.demand) AS demand\n",
        "\n",
        "      -- secondary metrics ON order double click\n",
        "      , SUM(abp.orders) AS orders\n",
        "      , SUM(abp.units) AS units\n",
        "\n",
        "      -- secondary metrics ON emails\n",
        "      , SUM(combined_engagement.email_sends) AS email_sends\n",
        "      , SUM(combined_engagement.email_opens) AS email_opens\n",
        "      , SUM(combined_engagement.email_clicks) AS email_clicks\n",
        "      , SUM(combined_engagement.email_purch) AS email_purch\n",
        "      , SUM(combined_engagement.email_demand) AS email_demand\n",
        "\n",
        "      -- secondary metrics ON app push/engagement\n",
        "      , SUM(combined_engagement.push_sent) AS push_sends\n",
        "      , SUM(combined_engagement.push_opens) AS push_opens\n",
        "      , (CASE WHEN SUM(combined_engagement.push_sent) = 0 THEN 0 ELSE SUM(combined_engagement.push_opens) / SUM(combined_engagement.push_sent) END) AS push_open_rate\n",
        "\n",
        "      -- secondary metrics ON site/app engagement\n",
        "      , SUM(a.visits) AS site_app_visits\n",
        "      , SUM(a.PDP_FAVORITE_COUNT) AS PDP_FAVORITE_COUNT\n",
        "      , SUM(a.ADD_TO_CART_COUNT) AS ADD_TO_CART_COUNT\n",
        "      , SUM(a.physical_activity) AS physical_activity\n",
        "\n",
        "      -- secondary metrics ON margin\n",
        "      , SUM(abp.base_FW_demand) AS base_FW_demand\n",
        "      , SUM(abp.base_AP_demand) AS base_AP_demand\n",
        "      , SUM(abp.base_EQ_demand) AS base_EQ_demand\n",
        "      , SUM(abp.clr_FW_demand) AS clr_FW_demand\n",
        "      , SUM(abp.clr_AP_demand) AS clr_AP_demand\n",
        "      , SUM(abp.clr_EQ_demand) AS clr_EQ_demand\n",
        "      , SUM(abp.launch_FW_demand) AS launch_FW_demand\n",
        "      , SUM(abp.launch_AP_demand) AS launch_AP_demand\n",
        "      , SUM(abp.launch_EQ_demand) AS launch_EQ_demand\n",
        "      , SUM(abp.rest_demand) AS rest_demand\n",
        "\n",
        "      -- secondary metrics ON order history\n",
        "      , SUM(hp.histOrders) AS histOrders\n",
        "      , SUM(CASE WHEN hp.histOrders = 1 THEN 1 ELSE 0 END) AS 1_priorOrder\n",
        "      , SUM(CASE WHEN hp.histOrders = 2 THEN 1 ELSE 0 END) AS 2_priorOrder\n",
        "\n",
        "FROM test_control_table z\n",
        "INNER JOIN balanced_aud_aa b ON z.upm_id = b.upm_id  --inner join our balanced audience to remove members that were distorting the AA period balance\n",
        "INNER JOIN mhub mh ON z.upm_id = mh.upm_id\n",
        "LEFT JOIN member_activities a ON mh.member_id = a.member_id\n",
        "LEFT JOIN aud_order_agg abp ON z.upm_id = abp.upm_id_ab_buyer\n",
        "LEFT JOIN historical_purchase hp ON z.upm_id = hp.upm_id\n",
        "LEFT JOIN combined_engagement ON z.upm_id = combined_engagement.upm_id\n",
        "\n",
        "WHERE (abp.demand < 600 OR abp.demand IS NULL)\n",
        "AND test_control != 'na'\n",
        "\n",
        "GROUP BY z.upm_id, z.test_control\n",
        "ORDER BY z.upm_id DESC;\n",
        "\n"
      ],
      "metadata": {
        "id": "q40pKXmONeBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Whatt\n",
        "\n",
        "SELECT upm_id\n",
        "    ,  total_demand\n",
        "FROM(\n",
        "      SELECT upm_id\n",
        "      , SUM (demand) AS total_demand\n",
        "      FROM ${hivevar:user_db}.${hivevar:to_tbl}_AB_full\n",
        "      GROUP BY upm_id\n",
        ") as poo\n",
        "HAVING COUNT(upm_id) > 1"
      ],
      "metadata": {
        "id": "UsQz6S1Jt48U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " WITH\n",
        "-- Including the relevant sub-query to identify duplicate order_nbr\n",
        ", view_dol AS (\n",
        "  SELECT dol.upm_id\n",
        "  , order_nbr\n",
        "  , order_dt\n",
        "  , origl_ordered_qty\n",
        "  , grd_amt_excl_tax_usd\n",
        "  , line_of_business_desc\n",
        "  , univ_div_desc\n",
        "  , first_send_dt\n",
        "  , last_send_dt\n",
        "  FROM awight.na_digital_order_line_snapshot dol\n",
        "  INNER JOIN tc_first_last_send fls ON dol.upm_id = fls.upm_id\n",
        "  WHERE rec_excl_ind = 0\n",
        "    AND ttl_demand_ind = 1\n",
        "    AND univ_cat_desc <> 'Converse'\n",
        "    AND dol.upm_id IS NOT NULL\n",
        "    AND order_dt BETWEEN DATE_SUB(first_send_dt,730) AND DATE_ADD(last_send_dt,7)\n",
        "    AND (rtn_qty = 0 or rtn_qty IS NULL)\n",
        ")\n",
        "\n",
        "-- Identifies duplicate order numbers\n",
        "\n",
        "  SELECT order_nbr\n",
        "  , COUNT(*) AS duplicate_count\n",
        "  FROM view_dol\n",
        "  GROUP BY order_nbr\n",
        "  HAVING COUNT(*) > 1\n",
        "\n",
        "\n",
        "-- get details of duplicate orders\n",
        "SELECT vd.upm_id\n",
        "  , vd.order_nbr\n",
        "  , vd.order_dt\n",
        "  , vd.grd_amt_excl_tax_usd\n",
        "  , vd.line_of_business_desc\n",
        "  , vd.univ_div_desc\n",
        "FROM view_dol vd\n",
        "INNER JOIN duplicate_orders do ON vd.order_nbr = do.order_nbr\n",
        "ORDER BY vd.order_nbr;\n",
        "\n",
        "-- Alternatively, if you only need the counts:\n",
        "SELECT order_nbr, duplicate_count\n",
        "FROM duplicate_orders\n",
        "ORDER BY duplicate_count DESC;"
      ],
      "metadata": {
        "id": "7FfIqg0lLf00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adding Holdouts back in"
      ],
      "metadata": {
        "id": "dyFVmAuts95o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "-----------------------------------------------------------------------------------------------------------------------------------\n",
        "-- AB Period\n",
        "-----------------------------------------------------------------------------------------------------------------------------------\n",
        "WITH\n",
        "\n",
        "-- use audience file to determine send dates when holdout audience is used\n",
        "tc_app_sends AS (\n",
        "    SELECT upm_id\n",
        "    , CASE WHEN final_exposure_or_holdout like ${hivevar:node_tst_str} THEN 'test'\n",
        "        WHEN final_exposure_or_holdout like ${hivevar:node_ctl_str_2} THEN 'holdout'\n",
        "        WHEN final_exposure_or_holdout like ${hivevar:node_ctl_str} THEN 'control'\n",
        "      ELSE 'na' END AS test_control\n",
        "    , LEFT(timestamp,10) AS campaign_send_dt\n",
        "    , CAST(LEFT(timestamp,19) AS TIMESTAMP) AS campaign_send_ts\n",
        "    FROM ${hivevar:aud_table} aud\n",
        "    ANTI JOIN contam_base c ON aud.upm_id = c.upm_id\n",
        "    ANTI JOIN gwan13.bot_master_nike_com bot ON aud.upm_id = bot.upm_id\n",
        "    ANTI JOIN aud_select_workspace.reseller_new slr ON aud.upm_id = slr.upm_user_id\n",
        "    WHERE CAST(LEFT(timestamp,10) AS DATE) BETWEEN ${hivevar:msmt_start_dt} AND ${hivevar:msmt_end_dt}\n",
        ")\n",
        "\n",
        ", tc_first_last_send AS (\n",
        "  SELECT upm_id\n",
        "  , MIN(campaign_send_dt) as first_send_dt\n",
        "  , MAX(campaign_send_dt) as last_send_dt\n",
        "  FROM tc_app_sends\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", test_control_table AS (\n",
        "    SELECT aud.*\n",
        "    , fls.first_send_dt\n",
        "    , fls.last_send_dt\n",
        "    , LEAD(campaign_send_dt) OVER (PARTITION BY aud.upm_id ORDER BY campaign_send_dt) AS next_send_dt\n",
        "    , LAG(campaign_send_dt) OVER (PARTITION BY aud.upm_id ORDER BY campaign_send_dt) AS previous_send_dt\n",
        "    , (int(to_timestamp(campaign_send_dt)) - int(to_timestamp(LEAD(campaign_send_dt) OVER (PARTITION BY aud.upm_id ORDER BY campaign_send_dt))))/86400 AS difference_next_send_days\n",
        "    , (int(to_timestamp(campaign_send_dt)) - int(to_timestamp(LAG(campaign_send_dt) OVER (PARTITION BY aud.upm_id ORDER BY campaign_send_dt))))/86400 AS difference_previous_send_days\n",
        "    FROM tc_app_sends aud\n",
        "    INNER JOIN tc_first_last_send fls ON aud.upm_id = fls.upm_id\n",
        ")\n",
        "\n",
        ", view_dol AS (\n",
        "  SELECT dol.upm_id\n",
        "  , order_nbr\n",
        "  , order_dt\n",
        "  , origl_ordered_qty\n",
        "  , grd_amt_excl_tax_usd\n",
        "  , line_of_business_desc\n",
        "  , univ_div_desc\n",
        "  , first_send_dt\n",
        "  , last_send_dt\n",
        "  FROM awight.na_digital_order_line_snapshot dol\n",
        "  INNER JOIN tc_first_last_send fls ON dol.upm_id = fls.upm_id\n",
        "  WHERE rec_excl_ind = 0\n",
        "    AND ttl_demand_ind = 1\n",
        "    AND univ_cat_desc <> 'Converse'\n",
        "    AND dol.upm_id IS NOT NULL\n",
        "    AND order_dt BETWEEN DATE_SUB(first_send_dt,730) AND DATE_ADD(last_send_dt,7)\n",
        "    AND (rtn_qty = 0 or rtn_qty IS NULL)\n",
        ")\n",
        "\n",
        "-- AA period purchase. dynamically based on when the member receives their first communication. Only returns demand for members in the\n",
        ", aa_purchase AS (\n",
        "  SELECT dol.upm_id\n",
        "    ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "    ,COUNT(DISTINCT order_nbr) AS orders\n",
        "    ,SUM(origl_ordered_qty) AS units\n",
        "  FROM view_dol dol\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,31) AND DATE_SUB(first_send_dt,1)\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        ", demand_bucket AS (\n",
        "  SELECT tct.upm_id\n",
        "  , tct.test_control\n",
        "  , aa.upm_id as upm_id_aa_buyer\n",
        "  , aa.demand\n",
        "  , aa.orders\n",
        "  , aa.units\n",
        "  , CASE\n",
        "    WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "    WHEN aa.demand < 50 THEN '000-050'\n",
        "    WHEN aa.demand < 100 THEN '050-100'\n",
        "    WHEN aa.demand < 200 THEN '100-200'\n",
        "    WHEN aa.demand < 500 THEN '200-500'\n",
        "    WHEN aa.demand < 1000 THEN '500-1000'\n",
        "    WHEN aa.demand >=1000 THEN '1000+'\n",
        "    END AS demand_per_buyer_bucket\n",
        "  , PERCENT_RANK() OVER (\n",
        "    PARTITION BY test_control\n",
        "    , CASE\n",
        "      WHEN aa.demand = 0 OR aa.demand IS NULL THEN 'non-buyer'\n",
        "      WHEN aa.demand < 50 THEN '000-050'\n",
        "      WHEN aa.demand < 100 THEN '050-100'\n",
        "      WHEN aa.demand < 200 THEN '100-200'\n",
        "      WHEN aa.demand < 500 THEN '200-500'\n",
        "      WHEN aa.demand < 1000 THEN '500-1000'\n",
        "      WHEN aa.demand >=1000 THEN '1000+'\n",
        "      END ORDER BY RANDOM(0)\n",
        "    ) AS percent_rank\n",
        "  FROM test_control_table tct\n",
        "  LEFT JOIN aa_purchase aa ON tct.upm_id = aa.upm_id\n",
        ")\n",
        "\n",
        ", balanced_aud_aa as (\n",
        "  SELECT DISTINCT upm_id\n",
        "  FROM demand_bucket db\n",
        "  -- remove outliers\n",
        "  WHERE (demand < 2400 OR demand IS NULL)\n",
        "-- To rebalance, paste text block from template in place of defaults\n",
        "AND ((test_control = 'control' and (demand > 0 and demand <= 50) and PERCENT_RANK <= 0.97632 )\n",
        "OR (test_control = 'control' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 1 )\n",
        "OR (test_control = 'control' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 0.99509 )\n",
        "OR (test_control = 'control' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 1 )\n",
        "OR (test_control = 'control' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 0.90741 )\n",
        "OR (test_control = 'control' and (demand > 1000) and PERCENT_RANK <= 0.98161 )\n",
        "OR (test_control = 'control' and (demand = 0 or demand is null) and PERCENT_RANK <= 0.99998 )\n",
        "OR (test_control = 'test' and (demand >0 and demand <= 50) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 0.98934 )\n",
        "OR (test_control = 'test' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 0.98953 )\n",
        "OR (test_control = 'test' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand > 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'test' and (demand = 0 or demand is null) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'holdout' and (demand > 0 and demand <= 50) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'holdout' and (demand > 50 and demand <= 100) and PERCENT_RANK <= 1 )\n",
        "OR (test_control = 'holdout' and (demand > 100 and demand <= 200) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'holdout' and (demand > 200 and demand <= 500) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'holdout' and (demand > 500 and demand <= 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'holdout' and (demand > 1000) and PERCENT_RANK <= 1)\n",
        "OR (test_control = 'holdout' and (demand = 0 or demand is null) and PERCENT_RANK <= 1)\n",
        "))\n",
        "\n",
        ", mhub as (\n",
        " SELECT source_id as upm_id\n",
        " , nuid\n",
        " , member_id\n",
        " , preferred_gender\n",
        " FROM MEMBER.MEMBER_HUB\n",
        " WHERE ctry_2_cd = 'US'\n",
        " AND lower(dpa_status) IN ('activate','reactivate')\n",
        ")\n",
        "\n",
        "-- calculate a dynamic member activity window for each communication received by a member\n",
        "-- activity is calculated on a 7 day window, unless the member receives a communication within that timeframe\n",
        "-- When multiple messages are received, activity is only calculated up to the next message_send_dt to avoid double counting\n",
        ", member_activities as (\n",
        "  SELECT mgd.member_id\n",
        "  ,SUM(logged_in_visits_count) AS visits\n",
        "  ,SUM(pdp_favorite_count) AS pdp_favorite_count\n",
        "  ,SUM(add_to_cart_count) AS add_to_cart_count\n",
        "  ,SUM(physical_activity_count) AS physical_activity\n",
        "  ,MAX(CASE WHEN new_user_experience_flag = 'Y' THEN 1 ELSE 0 END) AS new_user_flag\n",
        "  ,SUM(eod_chat_count) AS NEOD_Session\n",
        "  FROM member_agd mgd --aud_select_workspace.member_agg_member_growth_daily mgd\n",
        "  INNER JOIN mhub mh ON mgd.member_id = mh.member_id\n",
        "  INNER JOIN test_control_table tct ON mh.upm_id = tct.upm_id\n",
        "  WHERE mgd.activity_dt >= tct.campaign_send_dt\n",
        "    AND mgd.activity_dt < DATE_ADD(campaign_send_dt, INT(LEAST(7,COALESCE(difference_next_send_days,7))))\n",
        "    AND mgd.preferred_retail_geo = 'NA'\n",
        "    AND mgd.experience_name != 'agnostic'\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        "-- for each user, aggregate each order placed within our measurement window. create labels to categorize the type of demand\n",
        ", ab_purchase as (\n",
        "  SELECT dol.upm_id as upm_id_ab_buyer\n",
        "  , order_dt as order_ts\n",
        "  , order_nbr\n",
        "  ,SUM(grd_amt_excl_tax_usd) AS demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Base Inline' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS base_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Clearance' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS clr_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Footwear' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_FW_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND univ_div_desc = 'Apparel' THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_AP_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc = 'Launch' AND (univ_div_desc = 'Equipment' OR univ_div_desc IS NULL) THEN grd_amt_excl_tax_usd ELSE 0 END) AS launch_EQ_demand\n",
        "  ,SUM(CASE WHEN line_of_business_desc NOT IN ('Base Inline', 'Clearance', 'Launch') THEN grd_amt_excl_tax_usd ELSE 0 END) AS rest_demand\n",
        "  ,COUNT(DISTINCT order_nbr) AS orders\n",
        "  ,SUM(origl_ordered_qty) AS units\n",
        "  FROM view_dol dol\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN last_send_dt AND DATE_ADD(last_send_dt,7)\n",
        "  GROUP BY 1,2,3\n",
        ")\n",
        "\n",
        "-- counting the first order demand after each send\n",
        ", reset_per_campaign_sent AS (\n",
        " SELECT a.*\n",
        "  ,CONCAT(a.upm_id, ' ', a.campaign_send_dt ) AS touchpoint_per_member\n",
        "  FROM test_control_table a -- should it be created based on buyers/\n",
        "  WHERE upm_id IN (SELECT DISTINCT upm_id_ab_buyer FROM ab_purchase )\n",
        "  )\n",
        "\n",
        "\n",
        ", aud_order_join AS    (\n",
        "  SELECT a.*\n",
        "  , b.*\n",
        "  ,round(((bigint(to_timestamp(a.campaign_send_ts)))-(bigint(to_timestamp(b.order_ts))))/(60),3) AS order_send_diff_min\n",
        "  ,RANK() OVER(PARTITION BY  a.touchpoint_per_member  ORDER BY (((bigint(to_timestamp(a.campaign_send_ts)))-(bigint(to_timestamp(b.order_ts))))/(60)) DESC, a.campaign_send_dt ) AS nearest_order\n",
        "  FROM reset_per_campaign_sent a\n",
        "  LEFT JOIN ab_purchase b\n",
        "  ON (a.upm_id = b.upm_id_ab_buyer AND b.order_ts >= a.campaign_send_ts AND b.order_ts <= DATE_ADD(CAST(a.`campaign_send_dt` AS DATE), CAST(LEAST(7, COALESCE(a.`difference_next_send_days`, 7)) AS INT)))\n",
        "  ORDER BY a.upm_id, a.campaign_send_dt ASC\n",
        ")\n",
        "\n",
        "\n",
        "-- Aggregate each distinct send, attributing only the nearest order.\n",
        "-- This prevent double counting of orders when a members receives multiple sends within the measurement window.\n",
        ", aud_order_agg as (\n",
        "SELECT upm_id AS upm_id_ab_buyer\n",
        "  ,SUM(demand) AS demand\n",
        "  ,SUM(base_FW_demand) AS base_FW_demand\n",
        "  ,SUM(base_AP_demand) AS base_AP_demand\n",
        "  ,SUM(base_EQ_demand) AS base_EQ_demand\n",
        "  ,SUM(clr_FW_demand) AS clr_FW_demand\n",
        "  ,SUM(clr_AP_demand) AS clr_AP_demand\n",
        "  ,SUM(clr_EQ_demand) AS clr_EQ_demand\n",
        "  ,SUM(launch_FW_demand) AS launch_FW_demand\n",
        "  ,SUM(launch_AP_demand) AS launch_AP_demand\n",
        "  ,SUM(launch_EQ_demand) AS launch_EQ_demand\n",
        "  ,SUM(rest_demand) AS rest_demand\n",
        "  ,SUM(orders) AS orders\n",
        "  ,SUM(units) AS units\n",
        "FROM aud_order_join a\n",
        "WHERE nearest_order = 1\n",
        "AND demand IS NOT NULL -- additional filter for extraneous records\n",
        "GROUP BY 1\n",
        "ORDER BY 1\n",
        ")\n",
        "\n",
        ", historical_purchase AS (\n",
        "  SELECT aoa.upm_id_ab_buyer as upm_id\n",
        "  , COUNT(DISTINCT order_nbr) as histOrders\n",
        "  FROM aud_order_agg aoa  -- eligible audience\n",
        "  INNER JOIN view_dol dol on aoa.upm_id_ab_buyer = dol.upm_id\n",
        "  WHERE CAST(order_dt AS DATE) BETWEEN DATE_SUB(first_send_dt,730) AND DATE_SUB(first_send_dt,1)\n",
        "  GROUP BY 1\n",
        ")\n",
        "\n",
        "---------------------\n",
        "-- MAIN QUERY\n",
        "----------------------\n",
        "SELECT z.test_control\n",
        "  -- primary metrics ON incremental revenue: conversion rate, average demand per purchaser\n",
        "  ,COUNT(DISTINCT z.upm_id) AS total_members\n",
        "  ,COUNT(DISTINCT abp.upm_id_ab_buyer) AS buying_members\n",
        "  ,COUNT(DISTINCT abp.upm_id_ab_buyer) / COUNT(DISTINCT z.upm_id) AS conversion_rate\n",
        "  ,AVG(demand) AS avg_demand\n",
        "  ,stddev(demand) AS std_demand\n",
        "  -- secondary metrics ON order double click\n",
        "  ,SUM(demand) / SUM(orders) AS AOV\n",
        "  ,SUM(demand) / SUM(units) AS AUR\n",
        "  ,SUM(units) / SUM(orders) AS UPT\n",
        "\n",
        "\n",
        "  -- secondary metrics ON site/app engagement\n",
        "\n",
        "  ,AVG(visits) AS avg_site_app_visits\n",
        "  ,AVG(PDP_FAVORITE_COUNT) AS avg_PDP_FAVORITE_COUNT\n",
        "  ,AVG(ADD_TO_CART_COUNT) AS avg_ADD_TO_CART_COUNT\n",
        "  ,AVG(physical_activity) AS avg_physical_activity\n",
        "\n",
        "  -- secondary metrics ON margin\n",
        "  ,(AVG(base_FW_demand) * 0.38 + AVG(base_AP_demand) * 0.28 + AVG(base_EQ_demand) * 0.32\n",
        "    + AVG(clr_FW_demand) * 0.23 + AVG(base_AP_demand) * 0.17 + AVG(clr_EQ_demand) * 0.12\n",
        "    + AVG(launch_FW_demand) * 0.25 + AVG(launch_AP_demand) * 0.08 + AVG(launch_EQ_demand) * 0.19\n",
        "    + AVG(rest_demand) * 0.3\n",
        "  ) AS avg_margin\n",
        "  ,AVG(base_FW_demand) AS base_FW_demand\n",
        "  ,AVG(base_AP_demand) AS base_AP_demand\n",
        "  ,AVG(base_EQ_demand) AS base_EQ_demand\n",
        "  ,AVG(clr_FW_demand) AS clr_FW_demand\n",
        "  ,AVG(clr_AP_demand) AS clr_AP_demand\n",
        "  ,AVG(clr_EQ_demand) AS clr_EQ_demand\n",
        "  ,AVG(launch_FW_demand) AS launch_FW_demand\n",
        "  ,AVG(launch_AP_demand) AS launch_AP_demand\n",
        "  ,AVG(launch_EQ_demand) AS launch_EQ_demand\n",
        "  ,AVG(rest_demand) AS rest_demand\n",
        "    -- secondary metrics ON order history\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END) AS 1_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END)/COUNT(DISTINCT z.upm_id) AS pctBuyers_1_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 1 THEN demand ELSE 0 END)/SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END) AS DPB_1_prior\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END) AS 2_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END)/COUNT(DISTINCT z.upm_id) AS pctBuyers_2_priorOrder\n",
        "  ,SUM(CASE WHEN histOrders = 2 THEN demand ELSE 0 END)/SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END) AS DPB_2_prior\n",
        "\n",
        "FROM test_control_table z\n",
        "INNER JOIN balanced_aud_aa b ON z.upm_id = b.upm_id  --inner join our balanced audience to remove members that were distorting the AA period balance\n",
        "\n",
        "INNER JOIN mhub mh ON z.upm_id = mh.upm_id\n",
        "LEFT JOIN member_activities a ON mh.member_id = a.member_id\n",
        "\n",
        "LEFT JOIN aud_order_agg abp ON z.upm_id = abp.upm_id_ab_buyer\n",
        "LEFT JOIN historical_purchase hp ON z.upm_id = hp.upm_id\n",
        "\n",
        "--LEFT JOIN platform_usage plt ON z.upm_id = plt.upm_id\n",
        "\n",
        "WHERE (demand < 600 OR demand IS NULL)\n",
        "AND test_control != 'na'\n",
        "\n",
        "GROUP BY 1\n",
        "ORDER BY 1 DESC;\n"
      ],
      "metadata": {
        "id": "DityJar1tAVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pairwise Chi-Square for Conversion"
      ],
      "metadata": {
        "id": "AhhbmVZhqcxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Here is the structure of my data, where the test z.test_control groupings are: test, control, holdout\n",
        "\n",
        "SELECT z.upm_id\n",
        "          , z.test_control\n",
        "\n",
        "          -- primary metrics ON incremental revenue: conversion rate, average demand per purchaser\n",
        "          , SUM(demand) AS demand\n",
        "\n",
        "          -- secondary metrics ON order double click\n",
        "          , SUM(orders) AS orders\n",
        "          , SUM(units) AS units\n",
        "\n",
        "          -- secondary metrics ON site/app engagement\n",
        "          , SUM(visits) AS site_app_visits\n",
        "          , SUM(PDP_FAVORITE_COUNT) AS PDP_FAVORITE_COUNT\n",
        "          , SUM(ADD_TO_CART_COUNT) AS ADD_TO_CART_COUNT\n",
        "          , SUM(physical_activity) AS physical_activity\n",
        "\n",
        "          -- secondary metrics ON margin\n",
        "          , SUM(base_FW_demand) AS base_FW_demand\n",
        "          , SUM(base_AP_demand) AS base_AP_demand\n",
        "          , SUM(base_EQ_demand) AS base_EQ_demand\n",
        "          , SUM(clr_FW_demand) AS clr_FW_demand\n",
        "          , SUM(clr_AP_demand) AS clr_AP_demand\n",
        "          , SUM(clr_EQ_demand) AS clr_EQ_demand\n",
        "          , SUM(launch_FW_demand) AS launch_FW_demand\n",
        "          , SUM(launch_AP_demand) AS launch_AP_demand\n",
        "          , SUM(launch_EQ_demand) AS launch_EQ_demand\n",
        "          , SUM(rest_demand) AS rest_demand\n",
        "\n",
        "          -- secondary metrics ON order history\n",
        "          ,SUM(histOrders) AS histOrders\n",
        "          ,SUM(CASE WHEN histOrders = 1 THEN 1 ELSE 0 END) AS 1_priorOrder\n",
        "          ,SUM(CASE WHEN histOrders = 2 THEN 1 ELSE 0 END) AS 2_priorOrder\n",
        "\n",
        "        FROM test_control_table z\n",
        "        INNER JOIN balanced_aud_aa b ON z.upm_id = b.upm_id  --inner join our balanced audience to remove members that were distorting the AA period balance\n",
        "\n",
        "        INNER JOIN mhub mh ON z.upm_id = mh.upm_id\n",
        "        LEFT JOIN member_activities a ON mh.member_id = a.member_id\n",
        "\n",
        "        LEFT JOIN aud_order_agg abp ON z.upm_id = abp.upm_id_ab_buyer\n",
        "        LEFT JOIN historical_purchase hp ON z.upm_id = hp.upm_id\n",
        "\n",
        "        WHERE (demand < 600 OR demand IS NULL)\n",
        "\n",
        "        GROUP BY 1,2\n",
        "        ORDER BY 1 DESC\n"
      ],
      "metadata": {
        "id": "Z7LYaoaMqu10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AB Scorecard"
      ],
      "metadata": {
        "id": "Oy3dKhvdK71K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### For Two Variants"
      ],
      "metadata": {
        "id": "qsbwEkK4WGk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Assuming 'df' is your DataFrame with necessary columns\n",
        "\n",
        "# Step 1: Add 'buyer' and 'prior_order' indicator columns\n",
        "df_ab['buyer'] = (df_ab['orders'] > 0).astype(int)\n",
        "df_ab['demand_1_prior'] = np.where(df_ab['1_priorOrder'] == 1, df_ab['demand'], np.nan)\n",
        "df_ab['demand_2_prior'] = np.where(df_ab['2_priorOrder'] == 1, df_ab['demand'], np.nan)\n",
        "df_ab['member_1_prior'] = np.where(df_ab['1_priorOrder'] == 1, 1, 0)\n",
        "df_ab['member_2_prior'] = np.where(df_ab['2_priorOrder'] == 1, 1, 0)\n",
        "\n",
        "# Step 2: Perform temporary calculations for DPB_1_prior and DPB_2_prior\n",
        "temp_calculations = df_ab.groupby('test_control').agg({\n",
        "    'demand_1_prior': 'sum',\n",
        "    'demand_2_prior': 'sum',\n",
        "    'member_1_prior': 'sum',\n",
        "    'member_2_prior': 'sum',\n",
        "}).rename(columns={\n",
        "    'demand_1_prior': 'Total Demand 1 Prior',\n",
        "    'demand_2_prior': 'Total Demand 2 Prior',\n",
        "    'member_1_prior': 'Members 1 Prior',\n",
        "    'member_2_prior': 'Members 2 Prior',\n",
        "})\n",
        "\n",
        "# Step 3: Calculate DPB_1_prior and DPB_2_prior using temporary calculations\n",
        "dpb_1_prior = temp_calculations['Total Demand 1 Prior'] / temp_calculations['Members 1 Prior']\n",
        "dpb_2_prior = temp_calculations['Total Demand 2 Prior'] / temp_calculations['Members 2 Prior']\n",
        "\n",
        "# Step 4: Aggregate metrics\n",
        "aggregated_metrics = df_ab.groupby('test_control').agg({\n",
        "    'upm_id': 'nunique',\n",
        "    'buyer': 'sum',\n",
        "    'orders': 'sum',\n",
        "    'units': 'sum',\n",
        "    'demand': ['sum', 'mean'],\n",
        "    '1_priorOrder': 'sum',\n",
        "    '2_priorOrder': 'sum',\n",
        "    'demand_1_prior': 'sum',\n",
        "    'demand_2_prior': 'sum',\n",
        "    'member_1_prior': 'sum',\n",
        "    'member_2_prior': 'sum',\n",
        "    # 'email_opens': 'sum',\n",
        "    # 'email_sends': 'sum',\n",
        "    # 'email_clicks': 'sum',\n",
        "    # 'push_sends': 'sum',\n",
        "    # 'push_opens': 'sum',\n",
        "    'site_app_visits': 'mean',\n",
        "    'PDP_FAVORITE_COUNT': 'mean',\n",
        "    'ADD_TO_CART_COUNT': 'mean',\n",
        "    'physical_activity': 'mean',\n",
        "    'base_FW_demand': 'mean',\n",
        "    'clr_FW_demand': 'mean',\n",
        "    'launch_FW_demand': 'mean',\n",
        "    'base_AP_demand': 'mean',\n",
        "    'clr_AP_demand': 'mean',\n",
        "    'launch_AP_demand': 'mean',\n",
        "    'base_EQ_demand': 'mean',\n",
        "    'clr_EQ_demand': 'mean',\n",
        "    'launch_EQ_demand': 'mean',\n",
        "})\n",
        "\n",
        "# Step 5: Correct the MultiIndex handling by flattening and renaming\n",
        "aggregated_metrics.columns = ['_'.join(col).strip() for col in aggregated_metrics.columns.values]\n",
        "aggregated_metrics = aggregated_metrics.rename(columns={\n",
        "    'upm_id_nunique': 'Total Members',\n",
        "    'buyer_sum': 'Buying Members',\n",
        "    'demand_mean': 'Demand per Buyer',\n",
        "    'demand_sum': 'Total Demand',\n",
        "    'units_sum': 'Total Units',\n",
        "    'orders_sum': 'Total Orders',\n",
        "    # Secondary KPIs - Repeat buyers\n",
        "    '1_priorOrder_sum': 'Members with 1 previous order',\n",
        "    '2_priorOrder_sum':'Members with 2 previous orders',\n",
        "    # Secondary KPIs - Site/app Engagement metrics\n",
        "    'site_app_visits_mean':'Visits per known member',\n",
        "    'PDP_FAVORITE_COUNT_mean':'PDP favorite per known member',\n",
        "    'ADD_TO_CART_COUNT_mean': 'Add to cart per known member',\n",
        "    'physical_activity_mean': 'Workouts per known member',\n",
        "})\n",
        "\n",
        "# Step 6: Calculate DPB_1_prior and DPB_2_prior based on the temporary calculations and integrate directly into the DataFrame\n",
        "# aggregated_metrics['DPB_1_prior'] = aggregated_metrics['demand_1_prior'] / aggregated_metrics['member_1_prior']\n",
        "# aggregated_metrics['DPB_2_prior'] = aggregated_metrics['demand_2_prior'] / aggregated_metrics['member_2_prior']\n",
        "\n",
        "# Step 7: Calculate additional metrics\n",
        "\n",
        "aggregated_metrics['Conversion Rate'] = aggregated_metrics['Buying Members'] / aggregated_metrics['Total Members']\n",
        "aggregated_metrics['Demand per Send'] = aggregated_metrics['Demand per Buyer'] * aggregated_metrics['Conversion Rate']\n",
        "# Secondary KPIs - order deep dive\n",
        "aggregated_metrics['AOV'] = aggregated_metrics['Total Demand'] / aggregated_metrics['Total Orders']\n",
        "aggregated_metrics['AUR'] = aggregated_metrics['Total Demand'] / aggregated_metrics['Total Units']\n",
        "aggregated_metrics['UPT'] = aggregated_metrics['Total Units'] / aggregated_metrics['Total Orders']\n",
        "aggregated_metrics['# Txn per Buyer'] = aggregated_metrics['Demand per Buyer']/aggregated_metrics['AOV']\n",
        "# Secondary KPIs - Repeat buyers\n",
        "aggregated_metrics['Conversion rate (1x buyers/email receivers)'] = aggregated_metrics['Members with 1 previous order'] / aggregated_metrics['Total Members']\n",
        "aggregated_metrics['Conversion rate (2x buyers/email receivers)'] = aggregated_metrics['Members with 2 previous orders'] / aggregated_metrics['Total Members']\n",
        "aggregated_metrics['Demand per Buyer (1 previous order)'] = aggregated_metrics['demand_1_prior_sum'] / aggregated_metrics.get('member_1_prior_sum', 1)\n",
        "aggregated_metrics['Demand per Buyer (2 previous order)'] = aggregated_metrics['demand_2_prior_sum'] / aggregated_metrics.get('member_2_prior_sum', 1)\n",
        "\n",
        "# Secondary KPIs - Email/Push Engagement metrics (all email/push received during measurement windows)\n",
        "# aggregated_metrics['Email Open Rate (against sends)'] = aggregated_metrics['email_opens_sum'] / aggregated_metrics['email_sends_sum']\n",
        "# aggregated_metrics['Email Click Rate (against sends)'] = aggregated_metrics['email_clicks_sum'] / aggregated_metrics['email_sends_sum']\n",
        "# aggregated_metrics['Push Open Rate (against sends)'] = aggregated_metrics['push_opens_sum'] / aggregated_metrics['push_sends_sum']\n",
        "\n",
        "# Secondary KPIs - product mix\n",
        "aggregated_metrics['Footwear Demand per Member'] = ( aggregated_metrics['base_FW_demand_mean'] + aggregated_metrics['clr_FW_demand_mean'] + aggregated_metrics['launch_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "aggregated_metrics['Apparel Demand per Member'] = ( aggregated_metrics['base_AP_demand_mean'] + aggregated_metrics['clr_AP_demand_mean'] + aggregated_metrics['launch_AP_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "aggregated_metrics['Equipment Demand per Member'] = ( aggregated_metrics['base_EQ_demand_mean'] + aggregated_metrics['clr_EQ_demand_mean'] + aggregated_metrics['launch_EQ_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "# Secondary KPIs - LOB\n",
        "aggregated_metrics['Regular Product Demand per Member'] = ( aggregated_metrics['base_EQ_demand_mean'] + aggregated_metrics['base_AP_demand_mean'] + aggregated_metrics['base_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "\n",
        "aggregated_metrics['Clearance Demand per Member'] = ( aggregated_metrics['clr_EQ_demand_mean'] + aggregated_metrics['clr_AP_demand_mean'] + aggregated_metrics['clr_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "\n",
        "aggregated_metrics['Launch Demand per Member'] = ( aggregated_metrics['launch_EQ_demand_mean'] + aggregated_metrics['launch_AP_demand_mean'] + aggregated_metrics['launch_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "\n",
        "\n",
        "# Step 7: Remove temporary calculation columns as they're not needed in final DataFrame\n",
        "aggregated_metrics.drop(columns=['demand_1_prior_sum', 'demand_2_prior_sum', 'member_1_prior_sum', 'member_2_prior_sum',  'base_FW_demand_mean',\n",
        "  # 'email_clicks_sum','email_opens_sum','email_sends_sum',\n",
        " 'clr_FW_demand_mean', 'launch_FW_demand_mean', 'base_AP_demand_mean', 'clr_AP_demand_mean', 'launch_AP_demand_mean', 'base_EQ_demand_mean', 'clr_EQ_demand_mean','launch_EQ_demand_mean'], inplace=True)\n",
        "\n",
        "# Step 8: Perform t-test for 'Conversion Rate' and 'Demand per Buyer'\n",
        "\n",
        "# Separate the test and control groups to perform the t-tests correctly.\n",
        "test_group_ab = df_ab[df_ab['test_control'] == 'test']\n",
        "control_group_ab = df_ab[df_ab['test_control'] == 'control']\n",
        "\n",
        "# Conversion Rate Chi-square\n",
        "\n",
        "# Create a contingency table\n",
        "conv_contingency_table = pd.crosstab(df_ab['test_control'], df_ab['buyer']) # Count num of buyers and non-buyers in each group\n",
        "\n",
        "# Perform the Chi-square test\n",
        "chi2_stat_conversion, p_value_conversion, dof_conversion, expected_cnt_conversion = chi2_contingency(conv_contingency_table)\n",
        "\n",
        "# Demand per Buyer Wilcox (Mann-Whitney U) test\n",
        "mw_u_statistic_ab, mw_p_value_ab = stats.mannwhitneyu(test_group_ab['demand'].dropna(), control_group_ab['demand'].dropna(), alternative='two-sided')\n",
        "\n",
        "\n",
        "# Demand per Buyer T-Test\n",
        "\"\"\"\n",
        "# 'nan' values imputed with 0\n",
        "t_stat_demand, p_value_demand = stats.ttest_ind(test_group_ab['demand'].fillna(0), control_group_ab['demand'].fillna(0), equal_var=False)\n",
        "\"\"\"\n",
        "# 'nan' values omitted\n",
        "t_stat_demand, p_value_demand = stats.ttest_ind(test_group_ab['demand'], control_group_ab['demand'], equal_var=False, nan_policy='omit')\n",
        "# Note, results for demand ttest are the same with 'nan' values omitted or imputed to 0\n",
        "\n",
        "# Step 9: Transpose the table\n",
        "pd.options.display.float_format = '{:.8f}'.format\n",
        "aggregated_metrics = aggregated_metrics.T\n",
        "\n",
        "# Step 10: Calculate lift for all metrics except 'Conversion Rate' and 'Demand per Buyer'\n",
        "scorecard = pd.DataFrame(index=aggregated_metrics.index, columns=['Metric', 'Test', 'Control', 'Lift','P-Value','Stat'])\n",
        "\n",
        "# Populate Test and Control values from aggregated_metrics\n",
        "scorecard['Test'] = aggregated_metrics['test']\n",
        "scorecard['Control'] = aggregated_metrics['control']\n",
        "\n",
        "# Calculate Lift\n",
        "for metric in aggregated_metrics.index:\n",
        "    if metric not in ['Conversion Rate', 'Demand per Buyer']: # excluding Conversion_Rate and Demand_per_Buyer\n",
        "        test_val = scorecard.at[metric, 'Test']\n",
        "        control_val = scorecard.at[metric, 'Control']\n",
        "        lift = ((test_val - control_val) / control_val) * 100 if control_val != 0 else np.nan\n",
        "        scorecard.at[metric, 'Lift'] = lift\n",
        "\n",
        "# Step 11: Add t_stat and p_value for 'Conversion Rate' and 'Demand per Buyer' to dataframe\n",
        "scorecard.at['Conversion Rate', 'Stat'] = chi2_stat_conversion\n",
        "scorecard.at['Conversion Rate', 'P-Value'] = p_value_conversion\n",
        "scorecard.at['Demand per Buyer', 'Stat'] = mw_u_statistic_ab\n",
        "scorecard.at['Demand per Buyer', 'P-Value'] = mw_p_value_ab\n",
        "\n",
        "# Step 12: Ensure 'Metric' column is correctly populated\n",
        "# A byproduct of transposing the dataframe with some calculations only being applied selectively\n",
        "scorecard['Metric'] = scorecard.index\n",
        "scorecard.reset_index(drop=False, inplace=True)\n",
        "scorecard.drop(columns=['index'], inplace=True)\n",
        "\n",
        "# Step 13: Exclude 'Conversion Rate' and 'Demand per Buyer' from lift calculation\n",
        "scorecard.loc[scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 'Lift'] = ''\n",
        "scorecard.loc[~scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 'Stat'] = ''\n",
        "scorecard.loc[~scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 'P-Value'] = ''\n",
        "\n",
        "# Step 14: Display the scorecard\n",
        "scorecard\n"
      ],
      "metadata": {
        "id": "3mQmna4mKy0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### For Three Variants"
      ],
      "metadata": {
        "id": "zMV-WbOBWKdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from scipy.stats import chi2_contingency\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "\n",
        "\n",
        "# Step 1: Add 'buyer' and 'prior_order' indicator columns\n",
        "df_ab['buyer'] = (df_ab['orders'] > 0).astype(int)\n",
        "df_ab['demand_1_prior'] = np.where(df_ab['1_priorOrder'] == 1, df_ab['demand'], np.nan)\n",
        "df_ab['demand_2_prior'] = np.where(df_ab['2_priorOrder'] == 1, df_ab['demand'], np.nan)\n",
        "df_ab['member_1_prior'] = np.where(df_ab['1_priorOrder'] == 1, 1, 0)\n",
        "df_ab['member_2_prior'] = np.where(df_ab['2_priorOrder'] == 1, 1, 0)\n",
        "\n",
        "# Step 2: Perform temporary calculations for DPB_1_prior and DPB_2_prior\n",
        "temp_calculations = df_ab.groupby('test_control').agg({\n",
        "    'demand_1_prior': 'sum',\n",
        "    'demand_2_prior': 'sum',\n",
        "    'member_1_prior': 'sum',\n",
        "    'member_2_prior': 'sum',\n",
        "}).rename(columns={\n",
        "    'demand_1_prior': 'Total Demand 1 Prior',\n",
        "    'demand_2_prior': 'Total Demand 2 Prior',\n",
        "    'member_1_prior': 'Members 1 Prior',\n",
        "    'member_2_prior': 'Members 2 Prior',\n",
        "})\n",
        "\n",
        "# Step 3: Calculate DPB_1_prior and DPB_2_prior using temporary calculations\n",
        "dpb_1_prior = temp_calculations['Total Demand 1 Prior'] / temp_calculations['Members 1 Prior']\n",
        "dpb_2_prior = temp_calculations['Total Demand 2 Prior'] / temp_calculations['Members 2 Prior']\n",
        "\n",
        "# Step 4: Aggregate metrics\n",
        "aggregated_metrics = df_ab.groupby('test_control').agg({\n",
        "    'upm_id': 'nunique',\n",
        "    'buyer': 'sum',\n",
        "    'orders': 'sum',\n",
        "    'units': 'sum',\n",
        "    'demand': ['sum', 'mean'],\n",
        "    '1_priorOrder': 'sum',\n",
        "    '2_priorOrder': 'sum',\n",
        "    'demand_1_prior': 'sum',\n",
        "    'demand_2_prior': 'sum',\n",
        "    'member_1_prior': 'sum',\n",
        "    'member_2_prior': 'sum',\n",
        "    # 'email_opens': 'sum',\n",
        "    # 'email_sends': 'sum',\n",
        "    # 'email_clicks': 'sum',\n",
        "    # 'push_sends': 'sum',\n",
        "    # 'push_opens': 'sum',\n",
        "    'site_app_visits': 'mean',\n",
        "    'PDP_FAVORITE_COUNT': 'mean',\n",
        "    'ADD_TO_CART_COUNT': 'mean',\n",
        "    'physical_activity': 'mean',\n",
        "    'base_FW_demand': 'mean',\n",
        "    'clr_FW_demand': 'mean',\n",
        "    'launch_FW_demand': 'mean',\n",
        "    'base_AP_demand': 'mean',\n",
        "    'clr_AP_demand': 'mean',\n",
        "    'launch_AP_demand': 'mean',\n",
        "    'base_EQ_demand': 'mean',\n",
        "    'clr_EQ_demand': 'mean',\n",
        "    'launch_EQ_demand': 'mean',\n",
        "})\n",
        "\n",
        "# Step 5: Correct the MultiIndex handling by flattening and renaming\n",
        "aggregated_metrics.columns = ['_'.join(col).strip() for col in aggregated_metrics.columns.values]\n",
        "aggregated_metrics = aggregated_metrics.rename(columns={\n",
        "    'upm_id_nunique': 'Total Members',\n",
        "    'buyer_sum': 'Buying Members',\n",
        "    'demand_mean': 'Demand per Buyer',\n",
        "    'demand_sum': 'Total Demand',\n",
        "    'units_sum': 'Total Units',\n",
        "    'orders_sum': 'Total Orders',\n",
        "    # Secondary KPIs - Repeat buyers\n",
        "    '1_priorOrder_sum': 'Members with 1 previous order',\n",
        "    '2_priorOrder_sum':'Members with 2 previous orders',\n",
        "    # Secondary KPIs - Site/app Engagement metrics\n",
        "    'site_app_visits_mean':'Visits per known member',\n",
        "    'PDP_FAVORITE_COUNT_mean':'PDP favorite per known member',\n",
        "    'ADD_TO_CART_COUNT_mean': 'Add to cart per known member',\n",
        "    'physical_activity_mean': 'Workouts per known member',\n",
        "})\n",
        "\n",
        "# Step 6: Calculate DPB_1_prior and DPB_2_prior based on the temporary calculations and integrate directly into the DataFrame\n",
        "# aggregated_metrics['DPB_1_prior'] = aggregated_metrics['demand_1_prior'] / aggregated_metrics['member_1_prior']\n",
        "# aggregated_metrics['DPB_2_prior'] = aggregated_metrics['demand_2_prior'] / aggregated_metrics['member_2_prior']\n",
        "\n",
        "# Step 7: Calculate additional metrics\n",
        "\n",
        "aggregated_metrics['Conversion Rate'] = aggregated_metrics['Buying Members'] / aggregated_metrics['Total Members']\n",
        "aggregated_metrics['Demand per Send'] = aggregated_metrics['Demand per Buyer'] * aggregated_metrics['Conversion Rate']\n",
        "# Secondary KPIs - order deep dive\n",
        "aggregated_metrics['AOV'] = aggregated_metrics['Total Demand'] / aggregated_metrics['Total Orders']\n",
        "aggregated_metrics['AUR'] = aggregated_metrics['Total Demand'] / aggregated_metrics['Total Units']\n",
        "aggregated_metrics['UPT'] = aggregated_metrics['Total Units'] / aggregated_metrics['Total Orders']\n",
        "aggregated_metrics['# Txn per Buyer'] = aggregated_metrics['Demand per Buyer']/aggregated_metrics['AOV']\n",
        "# Secondary KPIs - Repeat buyers\n",
        "aggregated_metrics['Conversion rate (1x buyers/email receivers)'] = aggregated_metrics['Members with 1 previous order'] / aggregated_metrics['Total Members']\n",
        "aggregated_metrics['Conversion rate (2x buyers/email receivers)'] = aggregated_metrics['Members with 2 previous orders'] / aggregated_metrics['Total Members']\n",
        "aggregated_metrics['Demand per Buyer (1 previous order)'] = aggregated_metrics['demand_1_prior_sum'] / aggregated_metrics.get('member_1_prior_sum', 1)\n",
        "aggregated_metrics['Demand per Buyer (2 previous order)'] = aggregated_metrics['demand_2_prior_sum'] / aggregated_metrics.get('member_2_prior_sum', 1)\n",
        "\n",
        "# Secondary KPIs - Email/Push Engagement metrics (all email/push received during measurement windows)\n",
        "# aggregated_metrics['Email Open Rate (against sends)'] = aggregated_metrics['email_opens_sum'] / aggregated_metrics['email_sends_sum']\n",
        "# aggregated_metrics['Email Click Rate (against sends)'] = aggregated_metrics['email_clicks_sum'] / aggregated_metrics['email_sends_sum']\n",
        "# aggregated_metrics['Push Open Rate (against sends)'] = aggregated_metrics['push_opens_sum'] / aggregated_metrics['push_sends_sum']\n",
        "\n",
        "# Secondary KPIs - product mix\n",
        "aggregated_metrics['Footwear Demand per Member'] = ( aggregated_metrics['base_FW_demand_mean'] + aggregated_metrics['clr_FW_demand_mean'] + aggregated_metrics['launch_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "aggregated_metrics['Apparel Demand per Member'] = ( aggregated_metrics['base_AP_demand_mean'] + aggregated_metrics['clr_AP_demand_mean'] + aggregated_metrics['launch_AP_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "aggregated_metrics['Equipment Demand per Member'] = ( aggregated_metrics['base_EQ_demand_mean'] + aggregated_metrics['clr_EQ_demand_mean'] + aggregated_metrics['launch_EQ_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "# Secondary KPIs - LOB\n",
        "aggregated_metrics['Regular Product Demand per Member'] = ( aggregated_metrics['base_EQ_demand_mean'] + aggregated_metrics['base_AP_demand_mean'] + aggregated_metrics['base_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "\n",
        "aggregated_metrics['Clearance Demand per Member'] = ( aggregated_metrics['clr_EQ_demand_mean'] + aggregated_metrics['clr_AP_demand_mean'] + aggregated_metrics['clr_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "\n",
        "aggregated_metrics['Launch Demand per Member'] = ( aggregated_metrics['launch_EQ_demand_mean'] + aggregated_metrics['launch_AP_demand_mean'] + aggregated_metrics['launch_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "\n",
        "\n",
        "# Step 7: Remove temporary calculation columns as they're not needed in final DataFrame\n",
        "aggregated_metrics.drop(columns=['demand_1_prior_sum', 'demand_2_prior_sum', 'member_1_prior_sum', 'member_2_prior_sum',  'base_FW_demand_mean',\n",
        " # 'email_clicks_sum','email_opens_sum','email_sends_sum',  'push_sends_sum', 'push_opens_sum',\n",
        " 'clr_FW_demand_mean', 'launch_FW_demand_mean', 'base_AP_demand_mean', 'clr_AP_demand_mean', 'launch_AP_demand_mean', 'base_EQ_demand_mean', 'clr_EQ_demand_mean','launch_EQ_demand_mean'], inplace=True)\n",
        "\n",
        "# Step 8: Perform Chi-Square and Mann-Whitney U tests for 'Conversion Rate' and 'Demand per Buyer'\n",
        "\n",
        "# Separate the groups for pairwise tests\n",
        "test_group_ab = df_ab[df_ab['test_control'] == 'test']\n",
        "control_group_ab = df_ab[df_ab['test_control'] == 'control']\n",
        "holdout_group_ab = df_ab[df_ab['test_control'] == 'holdout']\n",
        "\n",
        "# Pairwise Chi-Square tests for Conversion Rate\n",
        "conv_contingency_table_test_control = pd.crosstab(df_ab[df_ab['test_control'].isin(['test', 'control'])]['test_control'], df_ab[df_ab['test_control'].isin(['test', 'control'])]['buyer'])\n",
        "conv_contingency_table_test_holdout = pd.crosstab(df_ab[df_ab['test_control'].isin(['test', 'holdout'])]['test_control'], df_ab[df_ab['test_control'].isin(['test', 'holdout'])]['buyer'])\n",
        "conv_contingency_table_control_holdout = pd.crosstab(df_ab[df_ab['test_control'].isin(['control', 'holdout'])]['test_control'], df_ab[df_ab['test_control'].isin(['control', 'holdout'])]['buyer'])\n",
        "\n",
        "chi2_test_control, p_test_control, _, _ = chi2_contingency(conv_contingency_table_test_control)\n",
        "chi2_test_holdout, p_test_holdout, _, _ = chi2_contingency(conv_contingency_table_test_holdout)\n",
        "chi2_control_holdout, p_control_holdout, _, _ = chi2_contingency(conv_contingency_table_control_holdout)\n",
        "\n",
        "# Adjust p-values for multiple comparisons\n",
        "p_values_chi2 = [p_test_control, p_test_holdout, p_control_holdout]\n",
        "corrected_p_values_chi2 = multipletests(p_values_chi2, method='bonferroni')[1]\n",
        "\n",
        "# Pairwise Mann-Whitney U tests for Demand per Buyer\n",
        "u_test_control, p_test_control_mwu = stats.mannwhitneyu(test_group_ab['demand'].dropna(), control_group_ab['demand'].dropna(), alternative='two-sided')\n",
        "u_test_holdout, p_test_holdout_mwu = stats.mannwhitneyu(test_group_ab['demand'].dropna(), holdout_group_ab['demand'].dropna(), alternative='two-sided')\n",
        "u_control_holdout, p_control_holdout_mwu = stats.mannwhitneyu(control_group_ab['demand'].dropna(), holdout_group_ab['demand'].dropna(), alternative='two-sided')\n",
        "\n",
        "# Adjust p-values for multiple comparisons\n",
        "p_values_mwu = [p_test_control_mwu, p_test_holdout_mwu, p_control_holdout_mwu]\n",
        "corrected_p_values_mwu = multipletests(p_values_mwu, method='bonferroni')[1]\n",
        "\n",
        "# Step 9: Transpose the table\n",
        "pd.options.display.float_format = '{:.8f}'.format\n",
        "aggregated_metrics = aggregated_metrics.T\n",
        "\n",
        "# Step 10: Calculate lift for all metrics except 'Conversion Rate' and 'Demand per Buyer'\n",
        "scorecard = pd.DataFrame(index=aggregated_metrics.index, columns=['Metric', 'Test', 'Control', 'Holdout', 'Lift Test vs Control', 'Lift Test vs Holdout', 'Lift Control vs Holdout', 'P-Value Chi2', 'Stat Chi2', 'P-Value MWU', 'Stat MWU'])\n",
        "\n",
        "# Populate Test, Control, and Holdout values from aggregated_metrics\n",
        "scorecard['Test'] = aggregated_metrics['test']\n",
        "scorecard['Control'] = aggregated_metrics['control']\n",
        "scorecard['Holdout'] = aggregated_metrics['holdout']\n",
        "\n",
        "# Calculate Lift\n",
        "for metric in aggregated_metrics.index:\n",
        "    if metric not in ['Conversion Rate', 'Demand per Buyer']: # excluding Conversion_Rate and Demand_per_Buyer\n",
        "        test_val = scorecard.at[metric, 'Test']\n",
        "        control_val = scorecard.at[metric, 'Control']\n",
        "        holdout_val = scorecard.at[metric, 'Holdout']\n",
        "        lift_test_control = ((test_val - control_val) / control_val) * 100 if control_val != 0 else np.nan\n",
        "        lift_test_holdout = ((test_val - holdout_val) / holdout_val) * 100 if holdout_val != 0 else np.nan\n",
        "        lift_control_holdout = ((control_val - holdout_val) / holdout_val) * 100 if holdout_val != 0 else np.nan\n",
        "        scorecard.at[metric, 'Lift Test vs Control'] = lift_test_control\n",
        "        scorecard.at[metric, 'Lift Test vs Holdout'] = lift_test_holdout\n",
        "        scorecard.at[metric, 'Lift Control vs Holdout'] = lift_control_holdout\n",
        "\n",
        "# Step 11: Add Chi2 and MWU test results for 'Conversion Rate' and 'Demand per Buyer' to dataframe\n",
        "scorecard.at['Conversion Rate', 'Stat Chi2'] = chi2_test_control\n",
        "scorecard.at['Conversion Rate', 'P-Value Chi2'] = corrected_p_values_chi2[0]\n",
        "scorecard.at['Demand per Buyer', 'Stat MWU'] = u_test_control\n",
        "scorecard.at['Demand per Buyer', 'P-Value MWU'] = corrected_p_values_mwu[0]\n",
        "\n",
        "# Step 12: Ensure 'Metric' column is correctly populated\n",
        "# A byproduct of transposing the dataframe with some calculations only being applied selectively\n",
        "scorecard['Metric'] = scorecard.index\n",
        "scorecard.reset_index(drop=False, inplace=True)\n",
        "scorecard.drop(columns=['index'], inplace=True)\n",
        "\n",
        "# Step 13: Exclude 'Conversion Rate' and 'Demand per Buyer' from lift calculation\n",
        "scorecard.loc[scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 'Lift Test vs Control'] = ''\n",
        "scorecard.loc[scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 'Lift Test vs Holdout'] = ''\n",
        "scorecard.loc[scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 'Lift Control vs Holdout'] = ''\n",
        "scorecard.loc[~scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 'Stat Chi2'] = ''\n",
        "scorecard.loc[~scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 'P-Value Chi2'] = ''\n",
        "scorecard.loc[~scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 'Stat MWU'] = ''\n",
        "scorecard.loc[~scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 'P-Value MWU'] = ''\n",
        "\n",
        "# Step 14: Display the scorecard\n",
        "scorecard\n"
      ],
      "metadata": {
        "id": "UXrRhMKOWMzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Updated to reflect fewer p-value and u stat columns\n",
        "\n"
      ],
      "metadata": {
        "id": "lGQHSDTWhws8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Step 1: Add 'buyer' and 'prior_order' indicator columns\n",
        "df_ab['buyer'] = (df_ab['orders'] > 0).astype(int)\n",
        "df_ab['demand_1_prior'] = np.where(df_ab['1_priorOrder'] == 1, df_ab['demand'], np.nan)\n",
        "df_ab['demand_2_prior'] = np.where(df_ab['2_priorOrder'] == 1, df_ab['demand'], np.nan)\n",
        "df_ab['member_1_prior'] = np.where(df_ab['1_priorOrder'] == 1, 1, 0)\n",
        "df_ab['member_2_prior'] = np.where(df_ab['2_priorOrder'] == 1, 1, 0)\n",
        "\n",
        "# Step 2: Perform temporary calculations for DPB_1_prior and DPB_2_prior\n",
        "temp_calculations = df_ab.groupby('test_control').agg({\n",
        "    'demand_1_prior': 'sum',\n",
        "    'demand_2_prior': 'sum',\n",
        "    'member_1_prior': 'sum',\n",
        "    'member_2_prior': 'sum',\n",
        "}).rename(columns={\n",
        "    'demand_1_prior': 'Total Demand 1 Prior',\n",
        "    'demand_2_prior': 'Total Demand 2 Prior',\n",
        "    'member_1_prior': 'Members 1 Prior',\n",
        "    'member_2_prior': 'Members 2 Prior',\n",
        "})\n",
        "\n",
        "# Step 3: Calculate DPB_1_prior and DPB_2_prior using temporary calculations\n",
        "dpb_1_prior = temp_calculations['Total Demand 1 Prior'] / temp_calculations['Members 1 Prior']\n",
        "dpb_2_prior = temp_calculations['Total Demand 2 Prior'] / temp_calculations['Members 2 Prior']\n",
        "\n",
        "# Step 4: Aggregate metrics\n",
        "aggregated_metrics = df_ab.groupby('test_control').agg({\n",
        "    'upm_id': 'nunique',\n",
        "    'buyer': 'sum',\n",
        "    'orders': 'sum',\n",
        "    'units': 'sum',\n",
        "    'demand': ['sum', 'mean'],\n",
        "    '1_priorOrder': 'sum',\n",
        "    '2_priorOrder': 'sum',\n",
        "    'demand_1_prior': 'sum',\n",
        "    'demand_2_prior': 'sum',\n",
        "    'member_1_prior': 'sum',\n",
        "    'member_2_prior': 'sum',\n",
        "    # 'email_opens': 'sum',\n",
        "    # 'email_sends': 'sum',\n",
        "    # 'email_clicks': 'sum',\n",
        "    # 'push_sends': 'sum',\n",
        "    # 'push_opens': 'sum',\n",
        "    'site_app_visits': 'mean',\n",
        "    'PDP_FAVORITE_COUNT': 'mean',\n",
        "    'ADD_TO_CART_COUNT': 'mean',\n",
        "    'physical_activity': 'mean',\n",
        "    'base_FW_demand': 'mean',\n",
        "    'clr_FW_demand': 'mean',\n",
        "    'launch_FW_demand': 'mean',\n",
        "    'base_AP_demand': 'mean',\n",
        "    'clr_AP_demand': 'mean',\n",
        "    'launch_AP_demand': 'mean',\n",
        "    'base_EQ_demand': 'mean',\n",
        "    'clr_EQ_demand': 'mean',\n",
        "    'launch_EQ_demand': 'mean',\n",
        "})\n",
        "\n",
        "# Step 5: Correct the MultiIndex handling by flattening and renaming\n",
        "aggregated_metrics.columns = ['_'.join(col).strip() for col in aggregated_metrics.columns.values]\n",
        "aggregated_metrics = aggregated_metrics.rename(columns={\n",
        "    'upm_id_nunique': 'Total Members',\n",
        "    'buyer_sum': 'Buying Members',\n",
        "    'demand_mean': 'Demand per Buyer',\n",
        "    'demand_sum': 'Total Demand',\n",
        "    'units_sum': 'Total Units',\n",
        "    'orders_sum': 'Total Orders',\n",
        "    # Secondary KPIs - Repeat buyers\n",
        "    '1_priorOrder_sum': 'Members with 1 previous order',\n",
        "    '2_priorOrder_sum':'Members with 2 previous orders',\n",
        "    # Secondary KPIs - Site/app Engagement metrics\n",
        "    'site_app_visits_mean':'Visits per known member',\n",
        "    'PDP_FAVORITE_COUNT_mean':'PDP favorite per known member',\n",
        "    'ADD_TO_CART_COUNT_mean': 'Add to cart per known member',\n",
        "    'physical_activity_mean': 'Workouts per known member',\n",
        "})\n",
        "\n",
        "# Step 6: Calculate DPB_1_prior and DPB_2_prior based on the temporary calculations and integrate directly into the DataFrame\n",
        "# aggregated_metrics['DPB_1_prior'] = aggregated_metrics['demand_1_prior'] / aggregated_metrics['member_1_prior']\n",
        "# aggregated_metrics['DPB_2_prior'] = aggregated_metrics['demand_2_prior'] / aggregated_metrics['member_2_prior']\n",
        "\n",
        "# Step 7: Calculate additional metrics\n",
        "\n",
        "aggregated_metrics['Conversion Rate'] = aggregated_metrics['Buying Members'] / aggregated_metrics['Total Members']\n",
        "aggregated_metrics['Demand per Send'] = aggregated_metrics['Demand per Buyer'] * aggregated_metrics['Conversion Rate']\n",
        "# Secondary KPIs - order deep dive\n",
        "aggregated_metrics['AOV'] = aggregated_metrics['Total Demand'] / aggregated_metrics['Total Orders']\n",
        "aggregated_metrics['AUR'] = aggregated_metrics['Total Demand'] / aggregated_metrics['Total Units']\n",
        "aggregated_metrics['UPT'] = aggregated_metrics['Total Units'] / aggregated_metrics['Total Orders']\n",
        "aggregated_metrics['# Txn per Buyer'] = aggregated_metrics['Demand per Buyer']/aggregated_metrics['AOV']\n",
        "# Secondary KPIs - Repeat buyers\n",
        "aggregated_metrics['Conversion rate (1x buyers/email receivers)'] = aggregated_metrics['Members with 1 previous order'] / aggregated_metrics['Total Members']\n",
        "aggregated_metrics['Conversion rate (2x buyers/email receivers)'] = aggregated_metrics['Members with 2 previous orders'] / aggregated_metrics['Total Members']\n",
        "aggregated_metrics['Demand per Buyer (1 previous order)'] = aggregated_metrics['demand_1_prior_sum'] / aggregated_metrics.get('member_1_prior_sum', 1)\n",
        "aggregated_metrics['Demand per Buyer (2 previous order)'] = aggregated_metrics['demand_2_prior_sum'] / aggregated_metrics.get('member_2_prior_sum', 1)\n",
        "\n",
        "# Secondary KPIs - Email/Push Engagement metrics (all email/push received during measurement windows)\n",
        "# aggregated_metrics['Email Open Rate (against sends)'] = aggregated_metrics['email_opens_sum'] / aggregated_metrics['email_sends_sum']\n",
        "# aggregated_metrics['Email Click Rate (against sends)'] = aggregated_metrics['email_clicks_sum'] / aggregated_metrics['email_sends_sum']\n",
        "# aggregated_metrics['Push Open Rate (against sends)'] = aggregated_metrics['push_opens_sum'] / aggregated_metrics['push_sends_sum']\n",
        "\n",
        "# Secondary KPIs - product mix\n",
        "aggregated_metrics['Footwear Demand per Member'] = ( aggregated_metrics['base_FW_demand_mean'] + aggregated_metrics['clr_FW_demand_mean'] + aggregated_metrics['launch_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "aggregated_metrics['Apparel Demand per Member'] = ( aggregated_metrics['base_AP_demand_mean'] + aggregated_metrics['clr_AP_demand_mean'] + aggregated_metrics['launch_AP_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "aggregated_metrics['Equipment Demand per Member'] = ( aggregated_metrics['base_EQ_demand_mean'] + aggregated_metrics['clr_EQ_demand_mean'] + aggregated_metrics['launch_EQ_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "# Secondary KPIs - LOB\n",
        "aggregated_metrics['Regular Product Demand per Member'] = ( aggregated_metrics['base_EQ_demand_mean'] + aggregated_metrics['base_AP_demand_mean'] + aggregated_metrics['base_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "\n",
        "aggregated_metrics['Clearance Demand per Member'] = ( aggregated_metrics['clr_EQ_demand_mean'] + aggregated_metrics['clr_AP_demand_mean'] + aggregated_metrics['clr_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "\n",
        "aggregated_metrics['Launch Demand per Member'] = ( aggregated_metrics['launch_EQ_demand_mean'] + aggregated_metrics['launch_AP_demand_mean'] + aggregated_metrics['launch_FW_demand_mean'] ) * aggregated_metrics['Conversion Rate']\n",
        "\n",
        "# Step 7: Remove temporary calculation columns as they're not needed in final DataFrame\n",
        "aggregated_metrics.drop(columns=['demand_1_prior_sum', 'demand_2_prior_sum', 'member_1_prior_sum', 'member_2_prior_sum',  'base_FW_demand_mean',\n",
        "   # 'email_clicks_sum','email_opens_sum','email_sends_sum',  'push_sends_sum', 'push_opens_sum',\n",
        " 'clr_FW_demand_mean', 'launch_FW_demand_mean', 'base_AP_demand_mean', 'clr_AP_demand_mean', 'launch_AP_demand_mean', 'base_EQ_demand_mean', 'clr_EQ_demand_mean','launch_EQ_demand_mean'], inplace=True)\n",
        "\n",
        "# Step 8: Perform Chi-Square and Mann-Whitney U tests for 'Conversion Rate' and 'Demand per Buyer'\n",
        "\n",
        "# Separate the groups for pairwise tests\n",
        "test_group_ab = df_ab[df_ab['test_control'] == 'test']\n",
        "control_group_ab = df_ab[df_ab['test_control'] == 'control']\n",
        "holdout_group_ab = df_ab[df_ab['test_control'] == 'holdout']\n",
        "\n",
        "# Pairwise Chi-Square tests for Conversion Rate\n",
        "conv_contingency_table_test_control = pd.crosstab(df_ab[df_ab['test_control'].isin(['test', 'control'])]['test_control'], df_ab[df_ab['test_control'].isin(['test', 'control'])]['buyer'])\n",
        "conv_contingency_table_test_holdout = pd.crosstab(df_ab[df_ab['test_control'].isin(['test', 'holdout'])]['test_control'], df_ab[df_ab['test_control'].isin(['test', 'holdout'])]['buyer'])\n",
        "conv_contingency_table_control_holdout = pd.crosstab(df_ab[df_ab['test_control'].isin(['control', 'holdout'])]['test_control'], df_ab[df_ab['test_control'].isin(['control', 'holdout'])]['buyer'])\n",
        "\n",
        "chi2_test_control, p_test_control, _, _ = chi2_contingency(conv_contingency_table_test_control)\n",
        "chi2_test_holdout, p_test_holdout, _, _ = chi2_contingency(conv_contingency_table_test_holdout)\n",
        "chi2_control_holdout, p_control_holdout, _, _ = chi2_contingency(conv_contingency_table_control_holdout)\n",
        "\n",
        "# Pairwise Mann-Whitney U tests for Demand per Buyer\n",
        "u_test_control, p_test_control_mwu = stats.mannwhitneyu(test_group_ab['demand'].dropna(), control_group_ab['demand'].dropna(), alternative='two-sided')\n",
        "u_test_holdout, p_test_holdout_mwu = stats.mannwhitneyu(test_group_ab['demand'].dropna(), holdout_group_ab['demand'].dropna(), alternative='two-sided')\n",
        "u_control_holdout, p_control_holdout_mwu = stats.mannwhitneyu(control_group_ab['demand'].dropna(), holdout_group_ab['demand'].dropna(), alternative='two-sided')\n",
        "\n",
        "# Step 9: Transpose the table\n",
        "pd.options.display.float_format = '{:.8f}'.format\n",
        "aggregated_metrics = aggregated_metrics.T\n",
        "\n",
        "# Step 10: Calculate lift for all metrics except 'Conversion Rate' and 'Demand per Buyer'\n",
        "scorecard = pd.DataFrame(index=aggregated_metrics.index, columns=['Metric', 'Test', 'Control', 'Holdout', 'Lift Test vs Control', 'Lift Test vs Holdout', 'Lift Control vs Holdout', 'Stat Test vs Control', 'P-Value Test vs Control', 'Stat Test vs Holdout', 'P-Value Test vs Holdout', 'Stat Control vs Holdout', 'P-Value Control vs Holdout'])\n",
        "\n",
        "# Populate Test, Control, and Holdout values from aggregated_metrics\n",
        "scorecard['Test'] = aggregated_metrics['test']\n",
        "scorecard['Control'] = aggregated_metrics['control']\n",
        "scorecard['Holdout'] = aggregated_metrics['holdout']\n",
        "\n",
        "# Calculate Lift\n",
        "for metric in aggregated_metrics.index:\n",
        "    if metric not in ['Conversion Rate', 'Demand per Buyer']: # excluding Conversion_Rate and Demand_per_Buyer\n",
        "        test_val = scorecard.at[metric, 'Test']\n",
        "        control_val = scorecard.at[metric, 'Control']\n",
        "        holdout_val = scorecard.at[metric, 'Holdout']\n",
        "        lift_test_control = ((test_val - control_val) / control_val) * 100 if control_val != 0 else np.nan\n",
        "        lift_test_holdout = ((test_val - holdout_val) / holdout_val) * 100 if holdout_val != 0 else np.nan\n",
        "        lift_control_holdout = ((control_val - holdout_val) / holdout_val) * 100 if holdout_val != 0 else np.nan\n",
        "        scorecard.at[metric, 'Lift Test vs Control'] = lift_test_control\n",
        "        scorecard.at[metric, 'Lift Test vs Holdout'] = lift_test_holdout\n",
        "        scorecard.at[metric, 'Lift Control vs Holdout'] = lift_control_holdout\n",
        "\n",
        "# Step 11: Add test results for 'Conversion Rate' and 'Demand per Buyer' to the dataframe\n",
        "# Conversion Rate - Chi2 Test results\n",
        "scorecard.at['Conversion Rate', 'Stat Test vs Control'] = chi2_test_control\n",
        "scorecard.at['Conversion Rate', 'P-Value Test vs Control'] = p_test_control\n",
        "scorecard.at['Conversion Rate', 'Stat Test vs Holdout'] = chi2_test_holdout\n",
        "scorecard.at['Conversion Rate', 'P-Value Test vs Holdout'] = p_test_holdout\n",
        "scorecard.at['Conversion Rate', 'Stat Control vs Holdout'] = chi2_control_holdout\n",
        "scorecard.at['Conversion Rate', 'P-Value Control vs Holdout'] = p_control_holdout\n",
        "\n",
        "# Demand per Buyer - MWU Test results\n",
        "scorecard.at['Demand per Buyer', 'Stat Test vs Control'] = u_test_control\n",
        "scorecard.at['Demand per Buyer', 'P-Value Test vs Control'] = p_test_control_mwu\n",
        "scorecard.at['Demand per Buyer', 'Stat Test vs Holdout'] = u_test_holdout\n",
        "scorecard.at['Demand per Buyer', 'P-Value Test vs Holdout'] = p_test_holdout_mwu\n",
        "scorecard.at['Demand per Buyer', 'Stat Control vs Holdout'] = u_control_holdout\n",
        "scorecard.at['Demand per Buyer', 'P-Value Control vs Holdout'] = p_control_holdout_mwu\n",
        "\n",
        "# Step 12: Ensure 'Metric' column is correctly populated\n",
        "# A byproduct of transposing the dataframe with some calculations only being applied selectively\n",
        "scorecard['Metric'] = scorecard.index\n",
        "scorecard.reset_index(drop=False, inplace=True)\n",
        "scorecard.drop(columns=['index'], inplace=True)\n",
        "\n",
        "# Step 13: Exclude 'Conversion Rate' and 'Demand per Buyer' from lift calculation\n",
        "scorecard.loc[scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 'Lift Test vs Control'] = ''\n",
        "scorecard.loc[scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 'Lift Test vs Holdout'] = ''\n",
        "scorecard.loc[scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 'Lift Control vs Holdout'] = ''\n",
        "scorecard.loc[~scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 'Stat Test vs Control'] = ''\n",
        "scorecard.loc[~scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 'P-Value Test vs Control'] = ''\n",
        "scorecard.loc[~scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 'Stat Test vs Holdout'] = ''\n",
        "scorecard.loc[~scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 'P-Value Test vs Holdout'] = ''\n",
        "scorecard.loc[~scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 'Stat Control vs Holdout'] = ''\n",
        "scorecard.loc[~scorecard['Metric'].isin(['Conversion Rate', 'Demand per Buyer']), 'P-Value Control vs Holdout'] = ''\n",
        "\n",
        "# Step 14: Display the scorecard\n",
        "scorecard\n"
      ],
      "metadata": {
        "id": "dPOh8KknifSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chi Square for Conversion"
      ],
      "metadata": {
        "id": "CYSlBii9I4fS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### For Two Variants - test & control"
      ],
      "metadata": {
        "id": "rSM3YoaXJAUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Two Variants - test and control\n",
        "\n",
        "%python\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# 'df' contains 'test_control' column indicating group membership\n",
        "# 'buyer' column indicating conversion (1) or non-conversion (0)\n",
        "\n",
        "# set alpha\n",
        "alpha = 0.1\n",
        "\n",
        "# Creating a contingency table\n",
        "# Count the number of conversions and non-conversions in each group\n",
        "conv_contingency_table_ab = pd.crosstab(df_ab['test_control'], df_ab['buyer'])\n",
        "\n",
        "print(\"Contingency Table for Conversion:\\n\")\n",
        "print(conv_contingency_table_ab)\n",
        "\n",
        "# Perform Chi-square test\n",
        "chi2_stat_conversion_ab, p_value_conversion_ab, dof_conversion_ab, expected_cnt_conversion_ab = chi2_contingency(conv_contingency_table_ab)\n",
        "\n",
        "print(f\"\\nChi2 Stat: {chi2_stat_conversion_ab}\")\n",
        "print(f\"\\nP-value: {p_value_conversion_ab}\")\n",
        "print(f\"\\nDegrees of Freedom: {dof_conversion_ab}\")\n",
        "print(f\"\\nExpected Counts: {expected_cnt_conversion_ab}\")\n",
        "\n",
        "# Interpretation\n",
        "if p_value_conversion_ab < alpha:\n",
        "    print(\"\\nSignificant difference in conversion rates between groups.\")\n",
        "    print(f\"\\nTherefore, we reject the null hypothesis (that conversion rates are the same between groups).\")\n",
        "else:\n",
        "    print(\"\\nNo significant difference in conversion rates between groups.\")\n",
        "    print(f\"\\nTherefore, we do not reject the null hypothesis (that conversion rates are the same between groups).\")\n"
      ],
      "metadata": {
        "id": "3N1mR2ezI6xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### For Three Variants - test & control & holdout"
      ],
      "metadata": {
        "id": "mG2ck231Jv5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Pairwise Chi-Square Tests: Conduct chi-square tests for each pair of groups separately.\n",
        "\n",
        "%python\n",
        "import pandas as pd\n",
        "from scipy.stats import chi2_contingency\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "\n",
        "\n",
        "# Set alpha\n",
        "alpha = 0.1\n",
        "\n",
        "# Create contingency tables for each pair of groups\n",
        "\n",
        "# Test vs Control\n",
        "conv_contingency_table_test_control = pd.crosstab(df_ab[df_ab['test_control'].isin(['test', 'control'])]['test_control'],\n",
        "                                                  df_ab[df_ab['test_control'].isin(['test', 'control'])]['buyer'])\n",
        "\n",
        "# Test vs Holdout\n",
        "conv_contingency_table_test_holdout = pd.crosstab(df_ab[df_ab['test_control'].isin(['test', 'holdout'])]['test_control'],\n",
        "                                                  df_ab[df_ab['test_control'].isin(['test', 'holdout'])]['buyer'])\n",
        "\n",
        "# Control vs Holdout\n",
        "conv_contingency_table_control_holdout = pd.crosstab(df_ab[df_ab['test_control'].isin(['control', 'holdout'])]['test_control'],\n",
        "                                                     df_ab[df_ab['test_control'].isin(['control', 'holdout'])]['buyer'])\n",
        "\n",
        "# Perform Chi-Square tests\n",
        "chi2_test_control, p_test_control, dof_test_control, expected_test_control = chi2_contingency(conv_contingency_table_test_control)\n",
        "chi2_test_holdout, p_test_holdout, dof_test_holdout, expected_test_holdout = chi2_contingency(conv_contingency_table_test_holdout)\n",
        "chi2_control_holdout, p_control_holdout, dof_control_holdout, expected_control_holdout = chi2_contingency(conv_contingency_table_control_holdout)\n",
        "\n",
        "# Adjust for multiple comparisons using the Bonferroni correction\n",
        "p_values = [p_test_control, p_test_holdout, p_control_holdout]\n",
        "corrected_p_values = multipletests(p_values, method='bonferroni')[1]\n",
        "\n",
        "p_test_control_corrected = corrected_p_values[0]\n",
        "p_test_holdout_corrected = corrected_p_values[1]\n",
        "p_control_holdout_corrected = corrected_p_values[2]\n",
        "\n",
        "# Print results\n",
        "print(\"Corrected p-value for Test vs Control:\", p_test_control_corrected)\n",
        "print(\"Corrected p-value for Test vs Holdout:\", p_test_holdout_corrected)\n",
        "print(\"Corrected p-value for Control vs Holdout:\", p_control_holdout_corrected)\n",
        "\n",
        "# Interpretation\n",
        "print(\"\\nInterpretation after correction:\")\n",
        "if p_test_control_corrected < alpha:\n",
        "    print(\"Significant difference between Test and Control.\")\n",
        "else:\n",
        "    print(\"No significant difference between Test and Control.\")\n",
        "\n",
        "if p_test_holdout_corrected < alpha:\n",
        "    print(\"Significant difference between Test and Holdout.\")\n",
        "else:\n",
        "    print(\"No significant difference between Test and Holdout.\")\n",
        "\n",
        "if p_control_holdout_corrected < alpha:\n",
        "    print(\"Significant difference between Control and Holdout.\")\n",
        "else:\n",
        "    print(\"No significant difference between Control and Holdout.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "auOnXO8-JzF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **chi2_contingency function** from SciPy returns four values:\n",
        "\n",
        "1. The chi-squared test statistic (`chi2`).\n",
        "2. The p-value of the test (`p`).\n",
        "3. The degrees of freedom (`dof`).\n",
        "4. The expected frequencies in each category (`expected`).\n",
        "\n",
        "We are not using degrees of freedom and expected frequenciesin this particular analysis."
      ],
      "metadata": {
        "id": "-iPLtlvgUBlh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pairwise Mann Whitney U for Demand"
      ],
      "metadata": {
        "id": "BbeDPsWXV1oQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Perform Pairwise Mann Whitney U Tests: Conduct Mann Whitney U tests for each pair of groups separately.\n",
        "\n",
        "%python\n",
        "from scipy.stats import mannwhitneyu\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "\n",
        "# Separate the groups\n",
        "test_group = df_ab[df_ab['test_control'] == 'test']['demand'].dropna()\n",
        "control_group = df_ab[df_ab['test_control'] == 'control']['demand'].dropna()\n",
        "holdout_group = df_ab[df_ab['test_control'] == 'holdout']['demand'].dropna()\n",
        "\n",
        "# Perform Mann-Whitney U tests for each pair\n",
        "u_test_control, p_test_control = mannwhitneyu(test_group, control_group, alternative='two-sided')\n",
        "u_test_holdout, p_test_holdout = mannwhitneyu(test_group, holdout_group, alternative='two-sided')\n",
        "u_control_holdout, p_control_holdout = mannwhitneyu(control_group, holdout_group, alternative='two-sided')\n",
        "\n",
        "# Print raw p-values\n",
        "print(\"Raw p-values for Mann-Whitney U test:\")\n",
        "print(f\"Test vs Control: {p_test_control}\")\n",
        "print(f\"Test vs Holdout: {p_test_holdout}\")\n",
        "print(f\"Control vs Holdout: {p_control_holdout}\")\n",
        "\n",
        "# Adjust for multiple comparisons using the Bonferroni correction\n",
        "p_values = [p_test_control, p_test_holdout, p_control_holdout]\n",
        "corrected_p_values = multipletests(p_values, method='bonferroni')[1]\n",
        "\n",
        "# Assign corrected p-values to variables\n",
        "p_test_control_corrected = corrected_p_values[0]\n",
        "p_test_holdout_corrected = corrected_p_values[1]\n",
        "p_control_holdout_corrected = corrected_p_values[2]\n",
        "\n",
        "# Print corrected p-values\n",
        "print(\"\\nCorrected p-values using Bonferroni correction:\")\n",
        "print(f\"Corrected p-value for Test vs Control: {p_test_control_corrected}\")\n",
        "print(f\"Corrected p-value for Test vs Holdout: {p_test_holdout_corrected}\")\n",
        "print(f\"Corrected p-value for Control vs Holdout: {p_control_holdout_corrected}\")\n",
        "\n",
        "# Interpretation based on corrected p-values\n",
        "alpha = 0.1 / 3  # Adjusted alpha for Bonferroni correction\n",
        "\n",
        "print(\"\\nInterpretation after correction:\")\n",
        "if p_test_control_corrected < alpha:\n",
        "    print(\"Significant difference between Test and Control.\")\n",
        "else:\n",
        "    print(\"No significant difference between Test and Control.\")\n",
        "\n",
        "if p_test_holdout_corrected < alpha:\n",
        "    print(\"Significant difference between Test and Holdout.\")\n",
        "else:\n",
        "    print(\"No significant difference between Test and Holdout.\")\n",
        "\n",
        "if p_control_holdout_corrected < alpha:\n",
        "    print(\"Significant difference between Control and Holdout.\")\n",
        "else:\n",
        "    print(\"No significant difference between Control and Holdout.\")\n"
      ],
      "metadata": {
        "id": "JtZLT9-7V41h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}